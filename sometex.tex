
%-------------------------------------------------------------------
% Author        : Mahbub Alam
% File          : sometex.tex
% Created       : 07/03/19 02:01:34 IST
% Description   : Random Tex
%-------------------------------------------------------------------

\documentclass[12pt,reqno]{book}%{T{E{X

% Usepackages%{T{E{X
%-------------------------------------------------------------------

\usepackage{afterpage}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbding}
\usepackage{bbm}
\usepackage{bm}
\usepackage{caption}
\usepackage{contour}%for writing sc bf
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{esvect}
\usepackage{eufrak}
\usepackage{float}
\usepackage[top=3cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{graphicx}
% \usepackage[linktoc=all]{hyperref}
\usepackage{lscape}
\usepackage{mathdots}
\usepackage{mathrsfs}
\usepackage{mathtools}
% \usepackage{MnSymbol}
\usepackage{palatino}
\usepackage{physics}
\usepackage{relsize}
\usepackage{slantsc}
\usepackage{stmaryrd}
\usepackage{tfrupee}
\usepackage{thmtools}
\usepackage{tikz}
% \usepackage{titlesec}
% \usepackage{titling}
\usepackage[normalem]{ulem}%for writing a underlined text
\usepackage{xcolor}
% \usepackage[dvipsnames]{xcolor}
\usepackage{yfonts}

\graphicspath{{Images/}}

%}T}E}X
% Environments%{T{E{X
%-------------------------------------------------------------------

% To write names of theorems in bold
\makeatletter
\def\th@plain{%
    \thm@notefont{}% same as heading font
    \itshape% body font
}
\def\th@definition{%
    \thm@notefont{}% same as heading font
    \normalfont% body font
}
\makeatother



\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}{Example}[chapter]
\newtheorem{exercise}{Exercise}[chapter]

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Unnumbered Environments
%-------------------------------------------------------------------

\theoremstyle{theorem}
\newtheorem*{utheorem}{Theorem}
\newtheorem*{ulemma}{Lemma}
\newtheorem*{uproposition}{Proposition}
\newtheorem*{ucorollary}{Corollary}

\theoremstyle{remark}
\newtheorem*{uremark}{Remark}
\newtheorem*{unotation}{Notation}

%}T}E}X
% Newcommands%{T{E{X
%-------------------------------------------------------------------

\newcommand{\mychi}{\raisebox{2pt}{$\chi$}}

\newcommand{\fl}[1]{\lfloor#1 \rfloor}
\newcommand{\bigfl}[1]{\big\lfloor#1 \big\rfloor}
\newcommand{\Bigfl}[1]{\Big\lfloor#1 \Big\rfloor}
\newcommand{\biggfl}[1]{\bigg\lfloor#1 \bigg\rfloor}
\newcommand{\Biggfl}[1]{\Bigg\lfloor#1 \Bigg\rfloor}
\newcommand{\lrfl}[1]{\left\lfloor#1 \right\rfloor}

\newcommand{\ce}[1]{\lceil#1 \rceil}
\newcommand{\bigce}[1]{\big\lceil#1 \big\rceil}
\newcommand{\Bigce}[1]{\Big\lceil#1 \Big\rceil}
\newcommand{\biggce}[1]{\bigg\lceil#1 \bigg\rceil}
\newcommand{\Biggce}[1]{\Bigg\lceil#1 \Bigg\rceil}
\newcommand{\lrce}[1]{\left\lceil#1 \right\rceil}

\newcommand{\M}{\mathrm{M}}
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\SL}{\mathrm{SL}}
\newcommand{\PSL}{\mathrm{PSL}}
\newcommand{\SO}{\mathrm{SO}}
\newcommand{\U}{\mathrm{U}}
\newcommand{\SU}{\mathrm{SU}}

\renewcommand{\O}{\mathrm{O}}

\renewcommand{\d}{\dd}

\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Lie}{Lie}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\Ad}{Ad}

% Numbering of equations
%-------------------------------------------------------------------

% \numberwithin{equation}{section}

% QED symbol
%-------------------------------------------------------------------

\renewcommand{\qedsymbol}{$\blacksquare$}

%}T}E}X

%}T}E}X

\begin{document}

\pagestyle{plain}

\title{\bf \Huge{Lie Groups}}

\author{Mahbub Alam}

\date{}

\maketitle

\thispagestyle{empty}

% Part
\part{Compact Groups}\label{}%{T{E{X
% Chapter
\chapter{Haar Measure}\label{}%{T{E{X
\vspace{5em}

If $G$ is a locally compact group, there is, up to a constant multiple, a unique regular Borel measure $\mu_L$ that is invariant under left translation.
Here \emph{left translation invariance} means that $\mu(X) = \mu(g^{-1}X)$ for all measurable sets $X$.
\emph{Regularity} means that
\[
    \mu(X) = \inf \{\mu(U) : U \supseteq X, U \ \text{open}\} = \sup \{\mu(K) : K \subseteq X, K \ \text{compact}\}.
\]
Such a measure is called a \emph{left Haar measure}.
It has the properties that any compact set has finite measure and any nonempty open set has measure $> 0$.
\newline

We will prove the existence and uniqueness of the Haar measure.
See for example Halmos, Hewitt and Ross, Chap. IV, or Loomis for a proof of this.
Left-invariance of the measure amounts to left-invariance of the corresponding integral,
% Equation
\begin{equation}\label{eqlefthaarint}
    \int_{G} f(\gamma g) \, \d \mu_L(g) = \int_{G} f(g) \, \d \mu_L(g),
\end{equation}
for any Haar integrable function $f$ on $G$.

There is also a right-invariant measure, $\mu_R$, unique up to constant multiple, called a \emph{right Haar measure}.
Left and right Haar measures may or may not coincide.
For example, if
\[
    G = {\left\{ \begin{pmatrix}
                y & x  \\
                0 & 1
            \end{pmatrix}
            : x, y \in \mathbb{R}, y > 0\right\}},
\]
then it is easy to see that the left- and right-invariant measures are, respectively,
\[
    \d\mu_L = y^{-2} \, \d x \, \d y, \qquad \d\mu_R = y^{-1} \, \d x \, \d y.
\]
They are not the same.
However, there are many cases where they do coincide, and if the left Haar measure is also right-invariant, we call $G$ \emph{unimodular}.

Conjugation is an automorphism of $G$, and so it takes a left Haar measure to another left Haar measure, which must be a constant multiple of the first.
Thus, if $g \in G$, there exists a constant $\delta(g) > 0$ such that
\[
    \int_{G} f(g^{-1}hg) \, \d\mu_L(h) = \delta(g) \int_{G} f(h) \, \d\mu_L(h).
\]
If $G$ is a topological group, a \emph{quasicharacter} is continuous homomorphism $\mychi : G \to \mathbb{C}^{\times}$.
If $|\mychi(g)| = 1$ for all $g \in G$, then $\mychi$ is called a (linear) \emph{character} or \emph{unitary quasicharacter}.

% Proposition
\begin{proposition}\label{}%{T{E{X
    The function $\delta : G \to \mathbb{R}_{>0}$ is a quasicharacter.
    The measure $\delta(h) \mu_L(h)$ is right-invariant.
\end{proposition}%}T}E}X

The measure $\delta(h) \mu_L(h)$ is a right Haar measure, and we may write $\mu_R(h) = \delta(h) \mu_L(h)$.
The quasicharacter $\delta$ is called the \emph{modular quasicharacter}.
% Proof
\begin{proof}%{T{E{X
    Conjugation by first $g_1$ and then $g_2$ is the same as conjugation by $g_1g_2$ in one step.
    Thus $\delta(g_1g_2) = \delta(g_1) \delta(g_2)$, so $\delta$ is a quasicharacter.
    Using~\eqref{eqlefthaarint},
    \[
        \delta(g) \int_{G} f(h) \, \d\mu_L(h) = \int_{G} f(g \cdot g^{-1}hg) \, \d\mu_L(h) = \int_{G} f(hg) \, \d\mu_L(h).
    \]
    Replace $f$ by $f\delta$ in this identity and then divide both sides by $\delta(g)$ to find that
    \[
        \int_{G} f(h) \delta(h) \, \d\mu_L(h) = \int_{G} f(hg) \delta(h) \, \d\mu_L(h).
    \]
    Thus, the measure $\delta(h) \, \d\mu_L(h)$ is right-invariant.
\end{proof}%}T}E}X

% Proposition
\begin{proposition}\label{}%{T{E{X
    If $G$ is compact, then $G$ is unimodular and $\mu_L(G) < \infty$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Since $\delta$ is a homomorphism, the image of $\delta$ is a subgroup of $\mathbb{R}_{>0}$.
    Since $G$ is compact, $\delta(G)$ is also compact, and the only compact subgroup of $\mathbb{R}_{>0}$ is just $\{1\}$.
    Thus $\delta$ is trivial, so a left Haar measure is right-invariant.
    We have mentioned as an assumed fact that the Haar volume of any compact subset of a locally compact group is finite, so if $G$ is compact, its Haar volume is finite.
\end{proof}%}T}E}X

If $G$ is compact, then it is natural to normalize the Haar measure so that $G$ has volume 1.

To simplify our notation, we will denote $\displaystyle\int_{G} f(g) \, \d\mu(g)$ by $\displaystyle\int_{G} f(g) \, \d g$.

% Proposition
\begin{proposition}\label{}%{T{E{X
    If $G$ is unimodular, then the map $g \mapsto g^{-1}$ is an isometry.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    It is easy to see that $g \mapsto g^{-1}$ turns a left Haar measure into a right Haar measure.
    If left and right Haar measures agree, then $g \mapsto g^{-1}$ multiplies the left Haar measure by a positive constant, which must be 1 since the map has order 2.
\end{proof}%}T}E}X

% UnnumberedSection
\section*{EXERCISES}\label{}%{T{E{X
% Exercise
\begin{exercise}\label{}
    Let $\d_{\bm{a}}X$ denote the Lebesgue measure on $\mathrm{M}_{n}(\mathbb{R})$.
    It is of course a Haar measure for the additive group $\mathrm{M}_{n}(\mathbb{R})$.
    Show that $|\det(X)|^{-n} \, \d_{\bm{a}}X$ is both a left and a right Haar measure on $\mathrm{GL}_{n}(\mathbb{R})$.
\end{exercise}

% Exercise
\begin{exercise}\label{}
    Let $P$ be the subgroup of $\mathrm{GL}_{r+s}(\mathbb{R})$ consisting of matrices of the form
    \[
        p = \begin{pmatrix}
            g_1 & X  \\
            \ & g_2
        \end{pmatrix}
        , \qquad g_1 \in \mathrm{GL}_{r}(\mathbb{R}), g_2 \in \mathrm{GL}_{s}(\mathbb{R}), \qquad X \in \mathrm{M}_{r \times s}(\mathbb{R}).
    \]
    Let $\d g_1$ and $\d g_2$ denote Haar measures on $\mathrm{GL}_{r}(\mathbb{R})$ and $\mathrm{GL}_{s}(\mathbb{R})$, and let $\d_{\bm{a}}X$ denote an additive Haar measure on $\mathrm{M}_{r \times s}(\mathbb{R})$.
    Show that
    \[
        \d_L p = |\det(g_1)|^{-s} \, \d g_1 \, \d g_2 \, \d_{\bm{a}} X, \qquad \d_R p = |\det(g_2)|^{-r} \, \d g_1 \, \d g_2
    \]
    are (respectively) left and right Haar measures on $P$, and conclude that the modular quasicharacter of $P$ is
    \[
        \delta(p) = |\det(g_1)|^{-s} |\det(g_2)|^{-r}.
    \]
\end{exercise}

%}T}E}X

%}T}E}X
% Chapter
\chapter{Schur Orthogonality}\label{}%{T{E{X
\vspace{5em}

In this chapter and the next two, we will consider the representation theory of compact groups.
Let us begin with a few observations about this theory and its relationship to some related theories.

If $V$ is finite-dimensional complex vector space, or more generally a Banach space, and $\pi : G \to \mathrm{GL}(V)$ a continuous homomorphism, then $(\pi, V)$ is called a \emph{representation}.
Assuming $\dim(V) < \infty$, the function $\mychi_\pi(g) = \tr \, \pi(g)$ is called the \emph{character} of $\pi$.
Also assuming $\dim(V) < \infty$, the representation $(\pi, V)$ is called \emph{irreducible} if $V$ has no proper nonzero invariant subspaces, and a character is called \emph{irreducible} if it is a character of a irreducible representation.

[If $V$ is an infinite-dimensional topological vector space, then $(\pi, V)$ is called irreducible if it has no proper nonzero invariant \emph{closed} subspaces.]
A quasicharacter $\mychi$ is a character in this sense since we can take $V = \mathbb{C}$ and $\pi(g)v = \mychi(g)v$ to obtain a representation whose character is $\mychi$.

The archetypal compact Abelian group $\mathbb{T} = \{z \in \mathbb{C}^{\times} : |z| = 1\}$.
We normalize the Haar measure on $\mathbb{T}$ so that it has volume 1.
Its characters are the functions $\mychi_n : \mathbb{T} \to \mathbb{C}^{\times}, \mychi_n(z) = z^n$.
The important properties of the $\mychi_n$ are that they form an orthonormal system and (deeper) an orthonormal basis of $L^2(\mathbb{T})$.

More generally, if $G$ is compact Abelian group, the characters of $G$ form an orthonormal basis of $L^2(G)$.
If $f \in L^2(G)$, we have a Fourier expansion,
% Equation
\begin{equation}\label{eqfourierforG}
    f(g) = \sum_{\chi} a_{\chi} \mychi(g), \qquad a_{\chi} = \int_{G} f(g) \overline{\mychi(g)} \, \d g,
\end{equation}
and the Plancherel formula is the identity:
% Equation
\begin{equation}\label{eqplancherel}
    \int_{G} |f(g)|^2 \, \d g = \sum_{\chi} |a_{\chi}|^2.
\end{equation}
These facts can be directly generalized in two ways.
First, Fourier analysis on locally compact Abelian groups, including Pontriagin duality, Fourier inversion, the Plancherel formula, etc.\ is an important and complete theory due to Weil and discussed, for example, in Rudin or Loomis.
The most important difference from the compact case is that the characters can vary continuously.
The characters themselves form a group, the \emph{dual group} $\widehat{G}$, whose topology is that of uniform convergence on compact sets.
The Fourier expansion~\eqref{eqfourierforG} is replaced by the \emph{Fourier inversion formula}
\[
    f(g) = \int_{\widehat{G}} \widehat{f}(\mychi) \mychi(g) \, \d\mychi, \qquad \widehat{f}(\mychi) = \int_{G} f(g) \overline{\mychi(g)} \, \d g.
\]
The symmetry between $G$ and $\widehat{G}$ is now evident.
Similarly in Plancherel formula~\eqref{eqplancherel} the sum on the right is replaced by an integral.

The second generalization, to arbitrary \emph{compact} group, is the subject of this chapter and the next two.
In summary, group representation theory given a orthonormal basis of $L^2(G)$ in the matrix coefficients of irreducible representations of $G$ and a (more important and very canonical) orthonormal basis of the subspace of $L^2(G)$ consisting of class functions in terms of the representations are all finite-dimensional.
The orthonormality of these sets is Schur orthogonality; the completeness is the Peter-Weyl theorem.

These two directions of generalization can be unified.
Harmonic analysis on locally compact groups agrees with representation theory.
The Fourier inversion formula and the Plancherel formula now involve the matrix coefficients of the irreducible unitary representations, which may occur in continuous families and are usually infinite-dimensional.
This field of mathematics, largely created by Harish-Chandra, is fundamental but beyond the scope of this book.
See Knapp for an extended introduction, and Gelfand, Graev and Piatetski-Shapiro and Varadarajan for the Plancherel formula for $\mathrm{SL}_{2}(\mathbb{R})$.

Although \emph{infinite-dimensional} representations are thus essential in harmonic analysis on a noncompact group such as $\mathrm{SL}_{n}(\mathbb{R})$, noncompact Lie groups also have irreducible \emph{finite-dimensional} representations, which are important in their own right.
They are seldom unitary and hence not relevant to the Plancherel formula.
The scope of this book includes finite-dimensional representations of Lie groups but not infinite-dimensional ones.

In this chapter and the next two, we will e mainly concerned with compact groups.
In this chapter, all representations will be complex and finite-dimensional except when explicitly noted otherwise.

By an \emph{inner product} on a complex vector space, we mean a positive definite Hermitian form, denoted ${\left\langle \, , \, \right\rangle}$.
Thus, ${\left\langle v, w\right\rangle}$ is linear in $v$, conjugate linear in $w$, satisfies ${\left\langle w, v\right\rangle} = \overline{\left\langle v, w\right\rangle}$, and ${\left\langle v, v\right\rangle} > 0$ if $v \neq 0$.
We will also use the term \emph{inner product} for real vector spaces --- an inner product on a real vector space is a positive definite symmetric bilinear form.
Given a group $G$ and a real or complex representation $\pi : G \to \mathrm{GL}(V)$, we say the inner product ${\left\langle \, , \, \right\rangle}$ on $V$ is \emph{invariant} or $G$-\emph{equivariant} if it satisfies the identity
\[
    {\left\langle \pi(g) v, \pi(g) w\right\rangle} = {\left\langle v, w\right\rangle}.
\]

% Proposition
\begin{proposition}\label{}%{T{E{X
    If $G$ is compact and $(\pi, V)$ is any finite-dimensional complex representation, then $V$ admits a $G$-equivariant inner product.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Start with an arbitrary inner product ${\left\langle \, , \, \right\rangle}'$.
    Averaging it gives another inner product,
    \[
        {\left\langle v, w\right\rangle} = \int_{G} {\left\langle \pi(g) v, \pi(g) w\right\rangle}' \, \d g,
    \]
    for it is easy to see that this inner product is Hermitian and positive definite.
    It is $G$-invariant by construction.
\end{proof}%}T}E}X

% Proposition
\begin{proposition}\label{}%{T{E{X
    If $G$ is compact, then each finite-dimensional representation if the direct sum of irreducible representations.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Let $(\pi, V)$ be given.
    Let $V_1$ be a nonzero invariant subspace of minimal dimension.
    It is clearly irreducible.
    Let $V_1^\perp$ be the orthogonal complement of $V_1$ with respect to a $G$-invariant inner product.
    It is easily checked to be invariant and is of lower dimension than $V$.
    By induction $V_1^\perp = V_2 \oplus \cdots \oplus V_n$ is a direct sum of invariant subspaces and so $V = V_1 \oplus \cdots \oplus V_n$ is also.
\end{proof}%}T}E}X

A function of the form $\varphi(g) = L(\pi(g)v)$, where $(\pi, V)$ is a finite-dimensional representation of $G$, $v \in V$ and $L : V \to \mathbb{C}$ is a linear functional, is called a \emph{matrix coefficient} on $G$.
This terminology in natural, because if we choose a basis $e_1, \ldots, e_n$, of $V$, we can identify $V$ with $\mathbb{C}^{n}$ and represent $g$ by matrices:
\[
    \pi(g)v = % Pmatrix
    \begin{pmatrix}
        \pi_{11}(g) & \cdots & \pi_{1n}(g) \\
        \vdots      & \      & \vdots \\
        \pi_{n1}(g) & \cdots & \pi_{nn}(g)
    \end{pmatrix}
    \begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    , \qquad v = \begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    = \sum_{j=1}^{n} v_j e_j.
\]
Then each of the $n^2$ functions $\pi_{ij}$ is a matrix coefficient.
Indeed
\[
    \pi_{ij}(g) = L_i(\pi(g)e_j),
\]
where $L_i(\sum_{j} v_j e_j) = v_i$.

% Proposition
\begin{proposition}\label{propmatcoeffs}%{T{E{X
    The matrix coefficients of $G$ are continuous functions.
    The pointwise sum or product of two matrix coefficients is a matrix coefficient, so they form a ring.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    If $v \in V$, then $g \mapsto \pi(g)v$ is continuous since by definition of representation $\pi : G \to \mathrm{GL}(V)$ is continuous and so a matrix coefficient $L(\pi(g)v)$ is continuous.

    If $(\pi_1, V_1)$ and $(\pi_2, V_2)$ are representations, $v_i \in V_i$ are vectors and $L_i : V_i \to \mathbb{C}$ are linear functionals, then we have representations $\pi_1 \oplus \pi_2$ and $\pi_1 \otimes \pi_2$ on $V_1 \oplus V_2$ and $V_1 \otimes V_2$, respectively.
    Given vectors $v_i \in V_i$ and functionals $L_i \in V_i^*$, then $L_i(\pi_1(g)v_1) \pm L_2(\pi_2(g)v_2)$ can be expressed as $L((\pi_1 \oplus \pi_2)(g)(v_1, v_2))$ where $L : V_1 \oplus V_2 \to \mathbb{C}$ is $L(x_1, x_2) = L_1(x_1) \pm L_2(x_2)$, so the matrix coefficients are closed under addition and subtraction.

    Similarly, we have a linear functional $L_1 \otimes L_2$ on $V_1 \otimes V_2$ satisfying
    \[
        (L_1 \otimes L_2)(x_1 \otimes x_2) = L_1(x_1) L_2(x_2)
    \]
    and
    \[
        (L_1 \otimes L_2)((\pi_1 \otimes \pi_2)(g)(v_1 \otimes v_2)) = L_1(\pi_1(g)v_1)L_2(\pi_2(g)v_2),
    \]
    proving that the product of two matrix coefficients is a matrix coefficient.
\end{proof}%}T}E}X

If $(\pi, V)$ is a representation, let $V^*$ be the dual space of $V$.
To emphasize the symmetry between $V$ and $V^*$, let us write the dual pairing $V \times V^* \to \mathbb{C}$ in the symmetrical form $L(v) = \llbracket v, L\rrbracket$.
We have a representation $(\widehat{\pi}, V^*)$, called the \emph{contragredient} of $\pi$, defined by
% Equation
\begin{equation}\label{eqcontragredientofpi}
    \llbracket v, \pi(g)L \rrbracket = \llbracket \pi(g^{-1})v, L \rrbracket.
\end{equation}
Note that the inverse is needed here sot that $\widehat{\pi}(g_1g_2) = \widehat{\pi}(g_1) \widehat{\pi}(g_2)$.

If $(\pi, V)$ is a representation, then by Proposition~\ref{propmatcoeffs} any linear combination of functions of the form $L(\pi(g)v)$ with $v \in V, L \in V^*$ is a matrix coefficient, though it may be a function $L'(\pi'(g)v')$ where $(\pi', V')$ is not $(\pi, V)$, but a larger representation.
Nevertheless, we call any linear combination of functions of the form $L(\pi(g)v)$ a \emph{matrix coefficient of the representation} $(\pi, V)$.
Thus the matrix coefficients of $\pi$ form a vector space, which we will denote by $\mathscr{M}$.
Clearly, $\dim(\mathscr{M}_\pi) \leq {\dim(V)}^2$.

% Proposition
\begin{proposition}\label{propmatcoeffofpiandpihat}%{T{E{X
    If $f$ is a matrix coefficient of $(\pi, V)$, then $\check{f}(g) = f(g^{-1})$ is a matrix coefficient of $(\widehat{\pi}, V^*)$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    This is clear from~\eqref{eqcontragredientofpi}, regarding $v$ as linear functional on $V^*$.
\end{proof}%}T}E}X

We have action of $G$ on the space of functions on $G$ by left and right translation.
Thus if $f$ is function and $g \in G$, the left and right translates are
\[
    (\lambda(g)f)(x) = f(g^{-1}x), \qquad (\rho(g)f)(x) = f(xg).
\]

% Theorem
\begin{theorem}\label{thmleftrightregularrepandmatcoeff}%{T{E{X
    Let $f$ be a function on $G$.
    The following are equivalent.
    % Enumerate
    % \renewcommand{\labelenumi}{\emph{\Roman{enumi}.}}
    \begin{enumerate}[label= (\roman*),font=\normalfont,before=\normalfont]
        \item The functions $\lambda(g)f$ span a finite-dimensional vector space.\label{itemleftrightregularrepandmatcoeffi}
        \item The functions $\rho(g)f$ span a finite-dimensional vector space.\label{itemleftrightregularrepandmatcoeffii}
        \item The function $f$ is a matrix coefficient of a finite-dimensional representation.\label{itemleftrightregularrepandmatcoeffiii}
    \end{enumerate}
\end{theorem}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    It is easy to check that if $f$ is matrix coefficient of a particular representation $V$, then so are $\lambda(g)f$ and $\rho(g)f$ for any $g \in G$.
    Since $V$ is finite-dimensional, its matrix coefficients span a finite-dimensional vector space; in fact, a space of dimension at most ${\dim(V)}^{2}$.
    Thus~\ref{itemleftrightregularrepandmatcoeffiii} implies~\ref{itemleftrightregularrepandmatcoeffi} and~\ref{itemleftrightregularrepandmatcoeffii}.

    Suppose that the functions $\rho(gf)$ span a finite-dim vector space $V$.
    Then $(\rho, V)$ is a finite-dim representation of $G$, and we claim that $f$ is a matrix coefficient.
    Indeed, define a functional $L : V \to \mathbb{C}$ by $L(\varphi) = \varphi(1)$.
    Clearly, $L(\rho(g)f) = f(g)$, so $f$ is a matrix coefficient, as required.
    Thus~\ref{itemleftrightregularrepandmatcoeffii} implies~\ref{itemleftrightregularrepandmatcoeffiii}.

    Finally, if the functions $\lambda(g)f$ span a finite-dim space, composing these functions with $g \mapsto g^{-1}$ gives another finite-dim space which is closed under right translation, and $\check{f}$ defined as in Proposition~\ref{propmatcoeffofpiandpihat} is an element of this space; hence $\check{f}$ is a matrix coefficient, so~\ref{itemleftrightregularrepandmatcoeffi} implies~\ref{itemleftrightregularrepandmatcoeffiii}.
\end{proof}%}T}E}X

If $(\pi_1, V_1)$ and $(\pi_1, V_1)$ are representations, an \emph{intertwining operator}, also known as a $G$-\emph{equivariant map} $T : V_1 \to V_2$ or (since $V_1$ and $V_2$ are sometimes called $G$-\emph{modules}) a $G$-\emph{module homomorphism}, is a linear transformation $T : V_1 \to V_2$ such that
\[
    T \circ \pi_1(g) = \pi_2(g) \circ T
\]
for $g \in G$.
We will denote by $\Hom_{\mathbb{C}}(V_1, V_2)$ the space of all linear transformations $V_1 \to V_2$ and by $\Hom_{G}(V_1, V_2)$ the subspace of those that are intertwining maps.
It $T$ is a bijective intertwining map, then $T^{-1} : V_2 \to V_1$ is also an intertwining map, so $T$ is an isomorphism.

For the remainder of this chapter, unless otherwise stated, $G$ will denote a compact group.

% Theorem
\begin{theorem}[Schur's Lemma]\label{thmschurslem}%{T{E{X
    % Enumerate
    \begin{enumerate}[label= (\roman*),font=\normalfont,before=\normalfont]
        \item Let $(\pi_1, V_1)$ and $(\pi_2, V_2)$ be irreducible representations, and let $T : V_1 \to V_2$ be an intertwining operator.
            Then either $T$ is zero or it is an isomorphism.\label{thmschuri}
        \item Suppose that $(\pi, V)$ is and irreducible representation of $G$ and $T : V \to V$ is an intertwining operator.
            Then there exists a scalar $\lambda \in \mathbb{C}$ such that $T(v) = \lambda v$ for all $v \in V$.\label{thmschurii}
    \end{enumerate}
\end{theorem}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    For~\ref{thmschuri}, the kernel of $T$ is an invariant subspace of $V_1$, which is assumed irreducible, so it $T$ is not zero, $\ker(T) = 0$.
    Thus $T$ is injective.
    Also, the image of $T$ is an invariant subspace of $V_2$.
    Since $V_2$ is irreducible, if $T$ is not zero, then $\Im(T) = V_2$.
    Therefore $T$ is bijective.

    For~\ref{thmschurii}, let $\lambda$ be any eigenvalue of $T$.
    Let $I : V \to V$ denote the identity map.
    The linear transformation $T - \lambda I$ is an intertwining operator that is not an isomorphism, so it is the zero map by~\ref{thmschuri}.
\end{proof}%}T}E}X

We are assuming that $G$ is compact.
The Haar volume of $G$ is therefore finite, and we normalize the Haar measure so that the volume of $G$ is 1.

We will consider the space $L^2(G)$ of functions on $G$ that are square-integrable with respect to the Haar measure.
This is a Hilbert space with the inner product
\[
    {\left\langle f_1, f_2\right\rangle}_{L^2} = \int_{G} f_1(g)\overline{f_2(g)} \, \d g.
\]
Schur orthogonality will give us an orthonormal basis for this space.

If $(\pi, V)$ is a representation, and ${\left\langle \, , \,\right\rangle}$ is an invariant inner product on $V$, then every linear functional is of the form $x \to {\left\langle x, v\right\rangle}$ for some $v \in V$.
Thus a matrix coefficient may be written in the form $g \to {\left\langle \pi(g) w, v\right\rangle}$, and such a representation will be useful to us in our discussion of Schur orthogonality.

% Lemma
\begin{lemma}\label{lemequivariantcxrep}%{T{E{X
    Suppose that $(\pi_1, V_1)$ and $(\pi_2, V_2)$ are complex representations of the compact group $G$.
    Let ${\left\langle \, , \,\right\rangle}$ be any inner product on $V_1$.
    If $v_i, w_i \in V_i$, then the map $T : V_1 \to V_2$ given by
    % Equation
    \begin{equation}\label{eqlemequivariantcxrep}
        T(w) = \int_{G} {\left\langle \pi_1(g)w, v_1\right\rangle} \pi_2(g^{-1})v_2 \, \d g
    \end{equation}
    is $G$-equivariant.
\end{lemma}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    We have
    \[
        T(\pi(h)w) = \int_{G} {\left\langle \pi_1(gh)w, v_1\right\rangle} \pi_2(g^{-1})v_2 \, \d g.
    \]
    The variable change $g \to gh^{-1}$ shows that this equals $\pi_2(h) T(w)$, as required.
\end{proof}%}T}E}X

% Theorem
\begin{theorem}[Schur orthogonality]\label{thmschurortho}%{T{E{X
    Suppose that $(\pi, V_1)$ and $(\pi_2, V_2)$ irreducible representations of the compact group $G$.
    Either every matrix coefficient of $\pi_1$ is orthogonal in $L^2(G)$ to every matrix coefficient of $\pi_2$, or the representations are isomorphic.
\end{theorem}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    We must show that if there exist matrix coefficients $f_i : G \to \mathbb{C}$ of $\pi_i$ that are \emph{not} orthogonal, then there is an isomorphism $T : V_1 \to V_2$.
    We may assume that the $f_i$ have the form $f_i(g) = {\left\langle \pi_i(g)w_i, v_i\right\rangle}$ since functions of that form span the spaces of matrix coefficients of matrix coefficients of the representations $\pi_i$.
    Here we use the notation ${\left\langle \, , \,\right\rangle}$ to denote invariant bilinear forms on both $V_1$ and $V_2$, and $v_i, w_i \in V_i$.
    Then our assumption is that
    \[
        \int_{G} {\left\langle \pi_1(g)w_1, v_1\right\rangle} {\left\langle \pi_2(g^{-1})v_2, w_2\right\rangle} \, \d g = \int_{G} {\left\langle \pi_1(g)w_1, v_1\right\rangle} \overline{{\left\langle \pi_2(g)w_2, v_2\right\rangle}} \, \d g \neq 0.
    \]
    Define $T : V_1 \to V_2$ by~\eqref{lemequivariantcxrep}.
    The map is nonzero since the last inequality can be written ${\left\langle w_2, T(w_1)\right\rangle} \neq 0$.
    It is an isomorphism by Schur's Lemma.
\end{proof}%}T}E}X

This gives orthogonality for matrix coefficients coming from \emph{nonisomorphic} irreducible representations.
But what about matrix coefficients from the same representation?
(If the representations are isomorphic, we may as well assume they are equal.)
The following result gives us an answer to this question.

% Theorem
\begin{theorem}[Schur orthognality]\label{thmschurorthosamerep}%{T{E{X
    Let $(\pi, V)$ be an irreducible representation of the compact group $G$, with invariant inner product ${\left\langle \, , \, \right\rangle}$.
    Then there exists a constant $d > 0$ such that
    % Equation
    \begin{equation}\label{eqschurorthosamerep}
        \int_{G} {\left\langle \pi(g)w_1, v_1\right\rangle} \overline{{\left\langle \pi(g)w_2, v_2\right\rangle}} \, \d g = d^{-1} {\left\langle w_1, w_2\right\rangle} {\left\langle v_2, v_1\right\rangle}.
    \end{equation}
\end{theorem}%}T}E}X

Later in Proposition \textcolor{red}{(ref here)}, we will show that $d = \dim(V)$.

% Proof
\begin{proof}%{T{E{X
    Fixing $v_1$ and $v_2$, $T$ given by~\eqref{eqlemequivariantcxrep} is $G$-equivariant, so by Schur's Lemma it is a scalar.
    Thus, there is a constant $c = c(v_1, v_2)$ depending only on $v_1$ and $v_2$ such that $T(w) = cw$.
    In particular, $T(w_1) = cw_1$, and so
    % UnnumberedAlign
    \begin{align*}
        c(v_1, v_2) {\left\langle w_1, w_2\right\rangle} &= {\left\langle T(w_1), w_2\right\rangle} \\
        &= \int_{G} {\left\langle \pi(g)w_1, v_1\right\rangle} {\left\langle \pi(g^{-1})v_2, w_2\right\rangle} \, \d g \\
        &= \int_{G} {\left\langle \pi(g)w_1, v_1\right\rangle} \overline{{\left\langle \pi(g)w_2, v_2\right\rangle}} \, \d g.
    \end{align*}
    On the other hand, the variable change $g \mapsto g^{-1}$ and the properties oft he inner product give us
    \[
        \int_{G} {\left\langle \pi(g)w_1, v_1\right\rangle} \overline{{\left\langle \pi(g)w_2, v_2\right\rangle}} \, \d g = \int_{G} {\left\langle \pi(g)v_2, w_2\right\rangle}\overline{{\left\langle \pi(g)v_1, w_1\right\rangle}} \, \d g,
    \]
    so the same argument shows that there exists another constant $c'(w_1, w_2)$ such that for all $v_1$ and $v_2$ we have
    \[
        \int_{G} {\left\langle \pi(g)w_1, v_1\right\rangle} \overline{{\left\langle \pi(g)w_2, v_2\right\rangle}} \, \d g = c'(w_1, w_2) {\left\langle v_2, v_1\right\rangle}.
    \]
    Putting these two facts together, we get~\eqref{eqschurorthosamerep}.
    We will compute $d$ later in Proposition \textcolor{red}{(ref here)}, but for now we simply note that it is positive since, taking $w_1 = w_2$ and $v_1 = v_2$, both the left-hand side of~\eqref{eqschurorthosamerep} and the two inner products on the right-hand side are positive.
\end{proof}%}T}E}X

Before we turn to the evaluation of the constant $d$, we will prove a different orthogonality for the characters of irreducible representations (Theorem~\ref{thmleftrightregularrepandmatcoeff}).
This will require some preparations.

% Proposition
\begin{proposition}\label{propcharismatcoeff}%{T{E{X
    The character $\mychi$ of a representation $(\pi, V)$ is a matrix coefficient of $V$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    If $v_1, \ldots, v_n$ is a matrix of $V$, and $L_1, \ldots, L_n$ is the dual basis of $V^*$, then $\mychi(g) = \sum_{i=1}^{n} L_i(\pi(g)v_i)$.
\end{proof}%}T}E}X

% Proposition
\begin{proposition}\label{propcharofcontragredientrepn}%{T{E{X
    Suppose that $(\pi, V)$ is a representation of $G$ and $(\pi^*, V^*)$ is its contragredient.
    Then the character of $\pi^*$ is the complex conjugate $\overline{\mychi}$ of the character $\mychi$ of $G$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Referring to~\eqref{eqcontragredientofpi}, $\pi^*(g)$ is the adjoint of ${\pi(g)}^{-1}$ with respect to the dual pairing $\llbracket \, , \, \rrbracket$, so its trace equals the trace of $\pi(g)^{-1}$.
    Since $\pi(g)$ is unitary with respect to an invariant inner product ${\left\langle \, , \, \right\rangle}$, its eigenvalues $t_1, \ldots, t_n$ all have absolute value 1, and so
    \[
        \tr \pi(g)^{-1} = \sum_{i} t_i^{-1} = \sum_{i} \overline{t_i} = \overline{\mychi(g)}.
    \]
\end{proof}%}T}E}X

The \emph{trivial representation} of any group $G$ is the representation on a one-dimensional vector space $V$ with $\pi(g)v = v$ being the trivial action.

% Proposition
\begin{proposition}\label{propintofchar}%{T{E{X
    If $(\pi, V)$ is an irreducible representation and $\mychi$ its character, then
    % DcaseDefinition
    \[
        \int_{G} \mychi(g) \, \d g =
        \begin{dcases}
            1 & \text{if} \ \pi \ \text{is the trivial representation;} \\
            0 & \text{otherwise.} \ 
        \end{dcases}
    \]
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    The character of the trivial representation is just the constant function 1, and since we normalized the Haar measure so that $G$ has volume 1, this integral is 1 if $\pi$ is trivial.
    In general, we may regard $\int_{G} \mychi(g) \, \d g$ as the inner product of $\mychi$ with the character 1 of the trivial representation, and if $\pi$ is nontrivial, these are matrix coefficients of different irreducible representations and hence orthogonal by Theorem~\ref{thmschurortho}.
\end{proof}%}T}E}X

If $(\pi, V)$ is representation, let $V^G$ be the subspace of $G$-\emph{invariants}, that is,
\[
    V^G = \{v \in V : \pi(g)v = v \ \text{for all} \ g \in G\}.
\]

% Proposition
\begin{proposition}\label{propintofcharanddimoffixedpts}%{T{E{X
    If $(\pi, V)$ is a representation of $G$ and $\mychi$ its character, then
    \[
        \int_{G} \mychi(g) \, \d g = \dim(V^G).
    \]
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Decompose $V = \otimes_{i} V_i$ into a direct sum of irreducible invariant subspaces, and let $\mychi_i$ be the character of the restriction $\pi_i$ of $\pi$ to $V_i$.
    By Proposition~\ref{propintofchar}, $\int_{G} \mychi_i(g) \, \d g = 1$ if and only if $\pi_i$ is trivial.
    Hence $\int_{G} \mychi(g) \, \d g$ is the number of trivial $\pi_i$.
    The direct sum of the $V_i$ with $\pi_i$ trivial is $V^G$, and the statement follows.
\end{proof}%}T}E}X

Suppose that $(\pi_1, V_1)$ and $(\pi_2, V_2)$ are representations of $G$.
We define a representation $\varPi$ of $G \times G$ on the space $\Hom_\mathbb{C}(V_1, V_2)$ of all linear transformations $T : V_1 \to V_2$ by
% Equation
\begin{equation}\label{eqprodofrepnonG}
    \varPi(g, h)T = \pi_2(g) \circ T \circ \pi_1(h^{-1}).
\end{equation}
We recall that $V_1^*$ is a module for the contragredient representation $\widehat{\pi}_1$.
We will compare this to the representation $\pi_2 \otimes \widehat{\pi}_1 : G \times G \to \mathrm{GL}(V_2 \otimes V_1^*)$ defined by $(\pi_2 \otimes \widehat{\pi}_1)(g, h) = \pi_2(g) \otimes \widehat{\pi}_1(g)$.
We denote by $\llbracket \, , \, \rrbracket$ the dual pairing $V_1 \times V_1^* \to \mathbb{C}$.

% Proposition
\begin{proposition}\label{propequivoftworepnsonGtimesGi}%{T{E{X
    Let $(\pi_1, V_1)$ and $(\pi_2, V_2)$ be representations of $G$.
    Then the representation~\eqref{eqprodofrepnonG} of $G \times G$ on $\Hom_\mathbb{C}(V_1, V_2)$ is equivalent to the representation $\pi \otimes \widehat{\pi}_1$ of $G \times G$ on $V_2 \otimes V_1^*$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Define a bilinear map $V_2 \times V_1^* \to \Hom_\mathbb{C}(V_1, V_2)$ by mapping $(v_2, L) \in V_2 \times V_1^*$ to the linear transformation $v_1 \mapsto \llbracket v_1, L \rrbracket v_2$.
    By the universal property of tensor product, there is induced a linear map $\theta : V_2 \otimes V_1^* \to \Hom_\mathbb{C}(V_1, V_2)$ such that $\theta(v_2 \otimes L) v_1 = \llbracket v_1, L \rrbracket v_2$.
    It is easy to see that $\theta$ is an isomorphism.
    We must show that it is $G \times G$-equivariant, that is,
    % Equation
    \begin{equation}\label{eqGtimesGequivarianceofth}
        \theta \circ (\pi_2(g) \otimes \widehat{\pi}_1(g)) = \varPi(g, h) \circ \theta.
    \end{equation}
    We have, for $v_i \in V_i$ and $L \in V_1^*$,
    % UnnumberedAlign
    \begin{align*}
        \theta \circ (\pi_2(g) \otimes \widehat{\pi}_1(g))(v_2 \otimes L)(v_1) &= \theta(\pi_2(g)v_2 \otimes \widehat{\pi}_1(h)L)(v_1) \\
        &= {\llbracket v_1, \widehat{\pi}_1(h)L \rrbracket}\pi_2(g)v_2 \\
        &= {\llbracket \pi_1(h^{-1})v_1, L\rrbracket} \pi_2(g)v_2 \\
        &= \pi_2(g) ({\llbracket \pi_1(h^{-1})v_1, L\rrbracket}v_2) \\
        &= \pi_2(g)\theta(v_2 \otimes L) \pi_1(h^{-1})v_1 \\
        &= (\varPi(g, h) \circ \theta)(v_2 \otimes L)v_1.
    \end{align*}
    This proves~\eqref{eqGtimesGequivarianceofth}.
\end{proof}%}T}E}X

If $(\pi, V)$ is an irreducible representation, we also have an action of $G \times G$ on the space $\mathscr{M}_\pi$ of matrix coefficients of $\pi$.
If $(g, h) \in G \times G$ and $f \in \mathscr{M}_\pi$, define $\mu(g, h)f : G \to \mathbb{C}$ by $(\mu(g, h)f)(x) = f(h^{-1}xg)$.

% Proposition
\begin{proposition}\label{propequivtworepnonGtimesGii}%{T{E{X
    If $(\pi, V)$ is an irreducible complex representation of the compact group $G$, and $f \in \mathscr{M}_\pi$, then $\mu(g, h)f \in \mathscr{M}_\pi$.
    Thus $\mu : G \times G \to \mathrm{GL}(\mathscr{M}_\pi)$ is a representation.
    It is equivalent to the representation of $G \times G$ on $V \otimes V^* \cong \End(V)$ obtained by taking $(\pi_1, V_1) = (\pi_2, V_2) = (\pi, V)$ to be the same representation in Proposition~\ref{propequivoftworepnsonGtimesGi}.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    If $v \in V, L \in V^*$, let $f_{v, L}(g) = {\llbracket \pi(g)f, L\rrbracket}$.
    The bilinear map $(v, L) \to f_{v, L}$ induces an isomorphism $V \otimes V^* \to \mathscr{M}_\pi$, which we claim is an isomorphism.
    This map is surjective by the definition of $\mathscr{M}_\pi$, and Schur orthogonality (Theorem~\ref{thmschurorthosamerep}) guarantees that $\mathscr{M}_\pi$ contains $\dim(V)^2$ orthogonal and hence linearly independent vectors, so the map must also be injective.
    We check easily that $\mu(g, h)f_{v, L} = f_{\pi(g)v, \widehat{\pi}(h)L}$, so this map is $G$-equivariant, and we conclude that $\mathscr{M}_\pi \cong V \otimes V^*$ as $G$-modules.
    The result now follows from Proposition~\ref{propequivoftworepnsonGtimesGi}.
\end{proof}%}T}E}X

If $(\pi_1, V_1)$ and $(\pi_2, V_2)$ are irreducible representations, and $\mychi_1$ and $\mychi_2$ are their characters, we have already noted in proving Proposition~\ref{propmatcoeffs} that we may form representations $\pi_1 \oplus \pi_2$ and $\pi_1 \otimes \pi_2$ on $V_1 \oplus V_2$ and $V_1 \otimes V_2$.
It is easy to see that $\mychi_{\pi_1 \oplus \pi_2} = \mychi_{\pi_1} + \mychi_{\pi_2}$ and $\mychi_{\pi_1 \otimes \pi_2} = \mychi_{\pi_1} \mychi_{\pi_2}$.
It is not quite true that the characters form a ring.
Certainly the negative of a matrix coefficient is a matrix coefficient, yet the negative of a character is not a character.
The set of characters is closed under addition and multiplication but not subtraction.
We define a \emph{generalized} (or \emph{virtual}) \emph{character} to be a function of the form $\mychi_1 - \mychi_2$, where $\mychi_1$ and $\mychi_2$ are characters.
It is now clear that the generalized characters form a ring.

The character of a representation satisfies
\[
    \mychi(ghg^{-1}) = \mychi(h)
\]
since $\mychi(ghg^{-1})$ is the trace of $\pi(g)\pi(h)\pi(g)^{-1}$, and the trace of a linear transformation is unchanged by conjugation.

% Theorem
\begin{theorem}[Schur orthogonality]\label{thmschurorthodiffrep}%{T{E{X
    Let $(\pi_1, V_1)$ and $(\pi_2, V_2)$ be representations of $G$ with characters $\mychi_1$ and $\mychi_2$.
    Then
    % Equation
    \begin{equation}\label{eqschurorthodiffrep}
        \int_{G} \mychi_1(g) \overline{\mychi_2(g)} \, \d g = \dim \Hom_G(V_1, V_2).
    \end{equation}
    If $\pi_1$ and $\pi_2$ are irreducible, then
    % CaseDefinition
    \[
        \int_{G} \mychi_1(g) \overline{\mychi_2(g)} \, \d g =
        \begin{cases}
            1 & \text{if} \ \pi_1 \cong \pi_2; \\
            0 & \text{otherwise.} \ 
        \end{cases}
    \]
\end{theorem}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    We embed $G \to G \times G$ along the diagonal.
    Then $\Hom_\mathbb{C}(V_1, V_2)$, which is a $G \times G$-module by virtue of the representation~\eqref{eqprodofrepnonG}, becomes a $G$-module, and it is clear that $\Hom_G(V_1, V_2)$ is just the space of $G$-invariants.
    By Proposition~\ref{propequivoftworepnsonGtimesGi}, this means that $\dim \Hom_G(V_1, V_2)$ is the same as the dimension of the space of $G$-invariants in $V_2 \times V_1^\ast$, and using Proposition~\ref{propcharofcontragredientrepn}, the character of $\pi_2 \otimes \widehat{\pi}_1$ is $\mychi_2 \overline{\mychi_1}$.
    The dimension of the space of $G$-invariants is $\int_{G} \mychi_2(g) \overline{\mychi_1(g)} \, \d g$ by Proposition~\ref{propintofcharanddimoffixedpts}.
    It is an integer, so we may take its complex conjugate to~\eqref{eqschurorthodiffrep}.

    The second statement follows from~\eqref{eqschurorthodiffrep} by Schur's Lemma, Theorem~\ref{thmschurslem}.
\end{proof}%}T}E}X

% Proposition
\begin{proposition}\label{prop2.11}%{T{E{X
    The constant $d$ in Theorem~\ref{thmschurorthosamerep} equals $\dim (V)$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Let $v_1, \ldots, v_n$ be an orthonormal basis of $V$, $n = \dim (V)$.
    We have
    \[
        \mychi(g) = \sum_{i} {\left\langle \pi_i(g) v_i, v_i\right\rangle}
    \]
    since ${\left\langle \pi(g)v_j, v_i\right\rangle}$ is the $i, j$ component of the matrix of $\pi(g)$ with respect to this basis.
    Now
    \[
        1 = \int_{G} |\mychi(g)|^2 \, \d g = \sum_{i, j} \int_{G} {\left\langle \pi(g) v_i, v_i\right\rangle} \overline{{\left\langle \pi(g)v_j, v_j\right\rangle}} \, \d g.
    \]
    There are $n^2$ terms on the right, but by~\eqref{eqschurorthosamerep} only the terms with $i = j$ are nonzero, and those equal $d^{-1}$.
    Thus $d = n$.
\end{proof}%}T}E}X

A function $f$ on $G$ is called a \emph{class function} is it is constant on conjugacy classes, that is, if it satisfies the equation $f(hgh^{-1}) = f(g)$.

% Proposition
\begin{proposition}\label{prop2.12}%{T{E{X
    If $f$ is the matrix coefficient of an irreducible representation $(\pi, V)$, and if $f$ is a class function, then $f$ is a constant multiple of $\mychi_\pi$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    By Schur's Lemma, there is a unique $G$-invariant vector in $\Hom_{\mathbb{C}}(V, V)$; hence, by Proposition~\ref{propequivtworepnonGtimesGii}, the same is true of $\mathcal{M}_\pi$ in the action of $G$ by conjugation.
    This matrix coefficient is of course $\mychi_\pi$.
\end{proof}%}T}E}X

% Theorem
\begin{theorem}\label{thm2.6}%{T{E{X
    If $f$ is a matrix coefficient and also a class function, then $f$ is a finite linear combination of characters of irreducible representations.
\end{theorem}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Write $f = \sum_{i=1}^{n} f_i$, where each $f_i$ is a class function of a distinct irreducible representation $(\pi_i, V_i)$.
    Since $f$ is conjugation-invariant, and since the $f_i$ live in spaces $\mathcal{M}_{\pi_i}$, which are conjugation-invariant and mutually orthogonal, each $f_i$ is itself a class function and hence a constant multiple of $\mychi_\pi$, by Proposition~\ref{prop2.12}.
\end{proof}%}T}E}X

% \begin{center}
%     \bf EXERCISES
% \end{center}

% UnnumberedSection
\section*{EXERCISES}%{T{E{X
% Exercise
\begin{exercise}\label{}
    Suppose that $G$ is compact Abelian group and $\pi : G \to \mathrm{GL}(n, \mathbb{C})$ an irreducible representation.
    Prove that $n=1$.
\end{exercise}

% Exercise
\begin{exercise}\label{}
    Suppose that $G$ is compact group and $f : G \to \mathbb{C}$ is the matrix coefficient of an irreducible representation.
    Show that $g \mapsto f(g^{-1})$ is a matrix coefficient of the same representation $\pi$.
\end{exercise}

% Exercise
\begin{exercise}\label{}
    Suppose that $G$ is compact group.
    Let $C(g)$ be the space of continuous functions on $G$.
    If $f_1$ and $f_2 \in C(G)$, define the convolution $f_1 * f_2$ of $f_1$ and $f_2$ by
    \[
        (f_1 * f_2)(g) = \int_{G} f_1(gh^{-1}) f_2(h) \, \d h = \int_{G} f_1(h) f_2(h^{-1}g) \, \d h.
    \]
    % Enumerate
    % \renewcommand{\labelenumi}{\emph{\Roman{enumi}.}}
    \begin{enumerate}[label= (\roman*),font=\normalfont,before=\normalfont]
        \item Use the variable change $h \to h^{-1}g$ to prove the identity of the last two term.
            Prove that this operation is associative, and so $C(G)$ is a ring (without unit) with respect to convolution.
        \item Let $\pi$ be an irreducible representation.
            Show that the space $\mathcal{\pi}$ of matrix coefficients of $\pi$ is a 2-sided ideal in $C(G)$, and explain how this fact implies Theorem~\ref{thmschurortho}.
    \end{enumerate}
\end{exercise}

%}T}E}X

%}T}E}X
% Chapter
\chapter{Compact Operators}\label{}%{T{E{X
If $\mathfrak{H}$ is a normed linear space, a linear operator $T : \mathfrak{H} \to \mathfrak{H}$ is called \emph{bounded} if there exists a constant $C$ such that $|Tx| \leq C|x|$ for all $x \in \mathfrak{H}$.
In this case, the smallest such $C$ is called the \emph{operator norm} of $T$, and is denoted $|T|$.
The boundedness of the operator $T$ is equivalent to its continuity.
If $\mathfrak{H}$ is a Hilbert space, then a bounded operator $T$ is \emph{self-adjoint} if
\[
    {\left\langle Tf, g\right\rangle} = {\left\langle f, Tg\right\rangle}
\]
for all $f, g \in \mathfrak{H}$.
As usual, we call $f$ an \emph{eigenvector} with \emph{eigenvalue} $\lambda$ is $f \neq 0$ and $Tf = \lambda f$.
Given $\lambda$, the set of eigenvectors with eigenvalue $\lambda$ is called the $\lambda$-\emph{eigenspace}.
It follows from elementary and usual arguments that if $T$ is a self-adjoint bounded operator, then its eigenvalues are real, and the eigenspaces corresponding to distinct eigenvalues are orthogonal.
Moreover, if $V \subseteq \mathfrak{H}$ is a subspace such that $T(V) \subseteq V$, it is easy to see that also $T(V^\perp) \subseteq V^\perp$.

A bounded operator $T : \mathfrak{H} \to \mathfrak{H}$ is \emph{compact} if whenever $\{x_1, x_2, x_3, \ldots\}$ is any sequence in $\mathfrak{H}$, the sequence $\{Tx_1, Tx_2, Tx_3, \ldots \}$ has a convergent subsequence.

% Theorem
\begin{theorem}[Spectral Theorem for compact operators]\label{thm3.1}%{T{E{X
    Let $T$ be a compact self-adjoint operator on a Hilbert space $\mathfrak{H}$.
    Let $\mathfrak{N}$ be the nullspace of $T$.
    Then the Hilbert space dimension of $\mathfrak{N}^\perp$ is at most countable.
    $\mathfrak{N}^\perp$ has an orthonormal basis $\varphi_i \, (i = 1, 2, 3, \ldots )$ of eigenvectors of $T$ so that $T\varphi_i = \lambda_i \varphi_i$.
    If $\mathfrak{N}^\perp$ is not finite-dimensional, the eigenvalues $\lambda_i \to 0$ as $i \to \infty$.
\end{theorem}%}T}E}X

Since the eigenvalues $\lambda_i \to 0$, if $\lambda$ is any nonzero eigenvalue, it follows from this statement that the $\lambda$-eigenspace is finite-dimensional.

% Proof
\begin{proof}%{T{E{X
    This depends upon the equality
    % Equation
    \begin{equation}\label{eqnormT}
        |T| = \sup_{0 \neq x \in \mathfrak{H}} \frac{|{\left\langle T x, x\right\rangle}|}{{\left\langle x, x\right\rangle}}.
    \end{equation}
    To prove this, let $B$ denote the right-hand side.
    If $0 \neq x \in \mathfrak{H}$,
    \[
        |{\left\langle T x, x\right\rangle}| \leq |T x| \cdot |x| \leq |T| \cdot |x|^2 =  |T| \cdot {\left\langle x, x\right\rangle},
    \]
    so $B \leq |T|$.
    We must prove the converse.
    Let $\lambda > 0$ be a constant, to be determined later.
    Using ${\left\langle T^2x, x\right\rangle} = {\left\langle T x, T x\right\rangle}$, we have
    % UnnumberedAlign
    \begin{align*}
        {\left\langle T x, T x\right\rangle} &= \frac{1}{4} |{\left\langle T(\lambda x + \lambda^{-1}T x), \lambda x + \lambda^{-1}T x\right\rangle} - {\left\langle T(\lambda x - \lambda^{-1}T x), \lambda x - \lambda^{-1}T x\right\rangle}| \\
        &\leq \frac{1}{4} |{\left\langle T(\lambda x + \lambda^{-1}T x), \lambda x + \lambda^{-1}T x\right\rangle}| + |{\left\langle T(\lambda x - \lambda^{-1}T x), \lambda x - \lambda^{-1}T x\right\rangle}| \\
        &\leq \frac{1}{4} [B{\left\langle (\lambda x + \lambda^{-1}T x), \lambda x + \lambda^{-1}T x\right\rangle} + B{\left\langle (\lambda x - \lambda^{-1}T x), \lambda x - \lambda^{-1}T x\right\rangle}] \\
        &= \frac{B}{2} [\lambda^2 {\left\langle x, x\right\rangle} + \lambda^{-2}{\left\langle T x, T x\right\rangle}].
    \end{align*}
    Now taking $\lambda = \sqrt{|T x|/|x|}$, we obtain
    \[
        |T x|^2 = {\left\langle T x, T x\right\rangle} \leq B|x||T x|,
    \]
    so $|T x| \leq B|x|$, which implies that $|T| \leq B$, whence~\eqref{eqnormT}.

    We now prove that $\mathfrak{N}^\perp$ has an orthonormal basis consisting of eigenvectors of $T$.
    It is a easy consequence of self-adjointness that $\mathfrak{N}^\perp$ is $T$-stable.
    Let $\Sigma$ be the set of all orthonormal subsets of $\mathfrak{N}^\perp$ whose elements are eigenvectors of $T$.
    Ordering $\Sigma$ by inclusion, Zorn's Lemma implies that it has a maximal element $S$.
    Let $V$ be the closure of the linear span of $S$.
    We must prove that $V = \mathfrak{N}^\perp$.
    Let $\mathfrak{H}_0 = V^\perp$.
    We wish to show $\mathfrak{H}_0 = \mathfrak{N}$.
    It is obvious that $\mathfrak{N} \subseteq \mathfrak{H}_0$.
    If $T$ has a nonzero eigenvector in $\mathfrak{H}_0$, this will contradict the maximality of $\Sigma$.
    It is therefore sufficient to show that \emph{a compact self-adjoint operator on a nonzero Hilbert space has an eigenvector}.

    Replacing $\mathfrak{H}$ by $\mathfrak{H}_0$, we are therefore reduced to the easier problem of showing that if $T \neq 0$, then $T$ has a nonzero eigenvector.
    By~\eqref{eqnormT}, there is a sequence $x_1, x_2, x_3, \ldots $ of unit vectors such that $|{\left\langle Tx_i, x_i\right\rangle}| \to |T|$.
    Observe that if $x \in \mathfrak{H}$, we have
    \[
        {\left\langle T x, x\right\rangle} = {\left\langle x, T x\right\rangle} = \overline{{\left\langle T x, x\right\rangle}}
    \]
    so the ${\left\langle Tx_i, x_i\right\rangle}$ are real; we may therefore replace the sequence by a subsequence such that ${\left\langle Tx_i, x_i\right\rangle} \to \lambda$, where $\lambda = \pm |T|$.
    Since $T \neq 0, \lambda \neq 0$.
    Since $T$ is compact, there exists a further subsequence $\{x_i\}$ such that $Tx_i$ converges to a vector $v$.
    We will show that $x_i \to \lambda^{-1}v$.

    Observe first that
    \[
        |{\left\langle Tx_i, x_i\right\rangle}| \leq |Tx_i| |x_i| = |Tx_i| \leq |T||x_i| = |\lambda|,
    \]
    and since ${\left\langle T x_i, x_i\right\rangle} \to \lambda$, it follows that $|Tx_i| \to |\lambda|$.
    Now
    \[
        |\lambda x_i - Tx_i|^2 = {\left\langle \lambda x_i - T x_i, \lambda x_i - T x_i\right\rangle} = \lambda^2 |x_i|^2 + |Tx_i|^2 - 2\lambda{\left\langle T x_i, x_i\right\rangle},
    \]
    and since $|x_i| = 1, |Tx_i| \to |\lambda|$, and ${\left\langle Tx_i, x_i\right\rangle} \to \lambda$, this converges to 0.
    Since $Tx_i \to v$, the sequence $\lambda x_i$ therefore also converges to $v$, and $x_i \to \lambda^{-1}v$.
    Now, by continuity, $Tx_i \to \lambda^{-1}Tv$, so $v = \lambda^{-1}Tv$.
    This proves that $v$ is an eigenvector with eigenvalue $\lambda$.
    This completes the proof that $\mathfrak{N}^\perp$ has an orthonormal basis consisting of eigenvectors.

    Now let $\{\varphi_i\}$ be this orthonormal basis and let $\lambda_i$ be the corresponding eigenvalues.
    If $\varepsilon > 0$ is given, only finitely many $|\lambda_i| > \varepsilon$ since otherwise we can find an infinite sequence of $\varphi_i$ with $|T\varphi_i| > \varepsilon$.
    Such a sequence will have no convergent subsequence, contradicting the compactness of $T$.
    Thus $\mathfrak{N}^\perp$ is countable-dimensional, and we may arrange the $\{\varphi_i\}$ in a sequence.
    If it is infinite, we see that $\lambda_i \to 0$.
\end{proof}%}T}E}X

% Proposition
\begin{proposition}\label{prop3.1}%{T{E{X
    Let $X$ and $Y$ be topological spaces with $Y$ a metric space with distance function $d$.
    Let $U$ be a set of continuous maps $X \to Y$ such that for every $x \in X$ and every $\varepsilon > 0$ there exists a neighborhood $N$ of $x$ such that $d(f(x), f(x')) < \varepsilon$ for all $x' \in N$ and for all $f \in U$.
    Then every sequence in $U$ has a uniformly convergent subsequence.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Let $S_0 = \{f_1, f_2, f_3, \ldots\}$ be a sequence in $U$.
    We will show that it has a convergent subsequence.
    We will construct a subsequence that is uniformly Cauchy and hence has a limit.
    For every $n > 1$, we will construct a subsequence $S_n = \{f_{n,1}, f_{n,2}, f_{n,3}, \ldots \}$ of $S_{n-1}$ such that $\sup_{x \in X} d(f_{n,i}(x), f_{n,j}(x)) \leq 1/n$.

    Assume that $S_{n-1}$ is contracted.
    For each $x \in X$, equicontinuity guarantees the existence of an open neighborhood $N_x$ of $x$ such that $d(f(y), f(x)) \leq \frac{1}{3n}$ for all $y \in N_x$ and all $f \in U$.
    Since $X$ is compact, we can cover $X$ by a finite number of these sets, say $N_{x_1}, \ldots, N_{x_m}$.
    Since the $f_{n-1, i}$ take values in the compact space $Y$, the $m$-tuples $(f_{n-1, i}(x_1), \ldots, f_{n-1, i}(x_m))$ have an accumulation point, and we may therefore select the subsequence $\{f_{n, i}\}$ such that $d(f_{n, i}(x_k), f_{, j}(x_k)) \leq \frac{1}{3n}$ for all $i, j$ and $1 \leq k \leq m$.
    Then for any $y$, there exists $x_k$ such that $y \in N_{x_k}$ and
    % UnnumberedAlign
    \begin{align*}
        d(f_{n, i}(y), f_{n, j}(y)) &\leq d(f_{n, i}(y), f_{n, i}(x_k)) + d(f_{n, i}(x_k), f_{n, j}(x_k)) + d(f_{n, j}(y), f_{n, j}(x_k)) \\
        &\leq \frac{1}{3n} + \frac{1}{3n} + \frac{1}{3n} = \frac{1}{n}.
    \end{align*}
    This completes the construction of the sequences $\{f_{n, i}\}$.

    The diagonal sequence $\{f_{11}, f_{22}, f_{33}, \ldots \}$ is uniformly Cauchy.
    Since $Y$ is a compact metric space, it is complete, and so this sequence is uniformly convergent.
\end{proof}%}T}E}X

We topologize $C(X)$ by giving it the $L^\infty$ norm $| \cdot |_\infty$ (sup norm).

% Proposition
\begin{proposition}[Arzela and Ascoli]\label{proparzelaascoli}%{T{E{X
    Suppose that $X$ is a compact space and that $U \subseteq C(X)$ is a bounded subset such that for every $x \in X$ and $\varepsilon > 0$ there is a neighborhood $N$ of $x$ such that $|f(x) - f(y)|_\infty \leq \varepsilon$ for all $y \in N$ and all $f \in U$.
    Then every sequence in $U$ has a uniformly convergent subsequence.
\end{proposition}%}T}E}X

Again, the hypotheses on $U$ is called \emph{equicontinuity}.

% Proof
\begin{proof}%{T{E{X
    Since $U$ is bounded, there is a compact interval $Y \subseteq \mathbb{R}$ such that all functions in $U$ take values in $Y$.
    The result follows from Proposition~\ref{prop3.1}.
\end{proof}%}T}E}X

% UnnumberedSection
\section*{EXERCISES}%{T{E{X
% Exercise
\begin{exercise}\label{}
    Suppose the $T$ is a bounded operator on the Hilbert space $\mathfrak{H}$, and suppose that for every $\varepsilon > 0$ there exists a compact operator $T_\varepsilon$ such that $|T - T_\varepsilon| < \varepsilon$.
    Show that $T$ is compact.
    (Use a diagonal argument like the proof of Proposition~\ref{prop3.1}.)
\end{exercise}

% Exercise
\begin{exercise}[Hilbert-Schmidt operators]\label{}
    Let $X$ be a locally compact Hausdorff space with a positive Borel measure $\mu$.
    Assume that $L^2(X)$ has a countable basis.
    Let $K \in L^2(X \times X)$.
    Consider the operator on $L^2(X)$ with kernel $K$ defined by
    \[
        T f(x) = \int_{X} K(x, y) f(y) \, \d y
    \]
    Let $\varphi_i$ be an orthonormal basis of $L^2(X)$.
    Expand $K$ in a Fourier expansion:
    \[
        K(x, y) = \sum\limits_{i=1}^{\infty} \psi_i(x) \overline{\varphi_i(y)}, \qquad \psi_i = T\varphi_i.
    \]
    Show that $\sum |\psi_i|^2 = \int \int |K(x, y)|^2 \, \d x \, \d y < \infty$.
    Consider the operator $T_N$ with kernel
    \[
        K_N(x, y) = \sum\limits_{i=1}^{N} \phi_i(x) \overline{\varphi_i(y)}.
    \]
    Show that $T_N$ is compact, and deduce that $T$ is compact.
\end{exercise}

%}T}E}X

%}T}E}X
% Chapter
\chapter{The Peter-Weyl Theorem}\label{}%{T{E{X
In this chapter, we assume that $G$ is a compact group.
Let $C(G)$ be the convolution ring of continuous functions on $G$.
It is a ring (without unit unless $G$ is finite) under the multiplication of \emph{convolution}:
\[
    (f_1 * f_2)(g) = \int_{G} f_1(gh^{-1}) f_2(h) \, \d h = \int_{G} f_1(h) f_2(h^{-1}g) \, \d h.
\]
(Use the variable change $h \to h^{-1}g$ to prove the identity of the last two terms. See Exercise 2.3.)
We will sometimes define $f_1*f_2$ by this formula even if $f_1$ and $f_2$ are not assumed continuous.
For example, we will make use of the convolution defined this way if $f_1 \in L^\infty(G)$ and $f_2 \in L^1(G)$, or vice versa.

Since $G$ has total volume 1, we have inequalities (where $| \cdot |_p$ denotes the $L^p$ norm, $1 \leq p \leq \infty$)
% Equation
\begin{equation}\label{eqpnormineq}
    |f|_1 \leq |f|_2 \leq |f|_\infty.
\end{equation}
The second inequality is trivial, and the first is Cauchy-Schwarz:
\[
    |f|_1 = {\left\langle |f|, 1\right\rangle} \leq |f|_2 \cdot |1|_2 = |f|_2.
\]
(Here $|f|$ means the function $|f|(x) = |f(x)|$.)

% Proposition
\begin{proposition}\label{prop4.1}%{T{E{X
    If $\varphi \in C(G)$, then convolution with $\varphi$ is a bounded operator $T_\varphi$ on $L^1(G)$.
    If $f \in L^1(G)$, then $T_\varphi f \in L^\infty(G)$ and
    % Equation
    \begin{equation}\label{eqprop4.1}
        |T_\varphi f|_\infty \leq |\varphi|_\infty |f|_1.
    \end{equation}
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    If $f \in L^1(G)$, then
    \[
        |T_\varphi f|_\infty = \sup_{g \in G} \bigg|\int_{G} \varphi(gh^{-1}) f(h) \, \d h \bigg| \leq |\varphi|_\infty \int_{G} |f(h)| \, \d h,
    \]
    proving~\eqref{eqprop4.1}.
    Using~\eqref{eqpnormineq}, it follows that the operator $T_\varphi$ is bounded.
    In fact,~\eqref{eqpnormineq} shows that it is bounded in each of the three metrics $| \cdot |_1, | \cdot |_2, | \cdot |_\infty$.
\end{proof}%}T}E}X

% Proposition
\begin{proposition}\label{prop4.2}%{T{E{X
    If $\varphi \in C(G)$, then convolution with $\varphi$ is a bounded operator $T_\varphi$ on $L^2(G)$ and $|T_\varphi| \leq |\varphi|_\infty$.
    The operator $T_\varphi$ is compact, and if $\varphi(g^{-1}) = \overline{\varphi(g)}$, it is self-adjoint.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Using~\eqref{eqprop4.1}, $L^\infty(G) \subseteq L^2(G) \subseteq L^1(G)$, and by~\eqref{eqprop4.1}, $|T_\varphi f|_2 \leq |T_\varphi f|_\infty \leq |\varphi|_\infty |f|_1 \leq |\varphi|_\infty |f|_2$, so the operator norm $|T_\varphi| \leq |\varphi|_2$.

    By~\eqref{eqprop4.1}, the unit ball in $L^2(G)$ is contained in the unit ball in $L^1(G)$, so it is sufficient to show that $\mathfrak{B} = \{T_\varphi f : f \in L^1(G), |f|_1 \leq 1\}$ is sequentially compact in $L^2(G)$.
    Also, by~\eqref{eqprop4.1}, it is sufficient to show that it is sequentially compact in $L^\infty(G)$, that is, in $C(G)$, whose topology is induced by the $L^\infty(G)$ norm.
    It follows from~\eqref{eqprop4.1} that $\mathfrak{B}$ is bounded.
    We show that it is equicontinuous.
    Since $\varphi$ is continuous and $G$ is compact, $\varphi$ is uniformly continuous.
    This means that given $\varepsilon > 0$ there is a neighborhood $N$ of the identity such that $|\varphi(kg) - \varphi(g)| < \varepsilon$ for all $g$ when $k \in N$.
    Now, if $f \in L^1(G)$ and $|f|_1 \leq 1$, we have, for all $g$,
    % UnnumberedAlign
    \begin{align*}
        |(\varphi * f)(kg) - (\varphi * f)(g)| &= \bigg|\int_{G} [\varphi(kgh^{-1}) - \varphi(gh^{-1})] f(h) \, \d h \bigg| \\
        &\leq \int_{G} |\varphi(kgh^{-1}) - \varphi(gh^{-1})| |f(h)| \, \d h \\
        &\leq \varepsilon|f|_1 \leq \varepsilon.
    \end{align*}
    This proves equicontinuity, and sequential compactness of $\mathfrak{B}$ now follows by the Arzela-Ascoli Lemma (Proposition~\ref{proparzelaascoli}).

    If $\varphi(g^{-1}) = \overline{\varphi(g)}$, then
    \[
        {\left\langle T_\varphi f_1, f_2\right\rangle} = \int_{G} \int_{G} \varphi(gh^{-1}) f_1(h) \overline{f_2(g)} \, \d g \, \d h
    \]
    while
    \[
        {\left\langle f_1, T_\varphi f_2\right\rangle} = \int_{G} \int_{G} \overline{\varphi(hg^{-1})}f_1(h) \overline{f_2(g)} \, \d g \, \d h.
    \]
    These are equal, so $T_\varphi$ is self-adjoint.
\end{proof}%}T}E}X

Recall that if $g \in G$, then $(\rho(g)f)(x) = f(xg)$ is the right translate of $f$ by $g$.

% Proposition
\begin{proposition}\label{prop4.3}%{T{E{X
    If $\varphi \in C(G)$, and $\lambda \in \mathbb{C}$, the $\lambda$-eigenspace
    \[
        V(\lambda) = \{f \in L^2(G) : T_\varphi f = \lambda f\}
    \]
    is invariant under $\rho(g)$ for all $g \in G$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Suppose $T_\varphi f = \lambda f$.
    Then
    \[
        (T_\varphi \rho(g) f)(x) = \int_{G} \varphi(xh^{-1}) f(hg) \, \d h.
    \]
    After the change of variables $h \to hg^{-1}$, this equals
    \[
        \int_{G} \varphi(xgh^{-1}) f(h) \, \d h = \rho(g)(T_\varphi f)(x) = \lambda\rho(g) f(x).
    \]
\end{proof}%}T}E}X

% Theorem
\begin{theorem}[Peter and Weyl]\label{thmpeterweyl}%{T{E{X
    The matrix coefficients of $G$ are dense in $C(G)$.
\end{theorem}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Let $f \in C(G)$.
    We will prove that there exists a matrix coefficient $f'$ such that $|f - f'|_\infty < \varepsilon$ for any given $\varepsilon > 0$.

    Since $G$ is compact, $f$ is uniformly continuous.
    This means that there exists an open neighborhood $U$ of the identity such that if $g \in U$, then $|\rho(g)f - f|_\infty < \varepsilon/2$.
    Let $\varphi$ be a nonnegative function supported in $U$ such that $\int_{G} \varphi(g) \, \d g = 1$.
    We may arrange that $\varphi(g) = \varphi(g^{-1})$ so that the operator $T_\varphi$ is self adjoint as well as compact.
    We claim that $|T_\varphi f - f|_\infty < \varepsilon/2$.
    Indeed, if $h \in G$,
    % UnnumberedAlign
    \begin{align*}
        |(\varphi * f)(h) - f(h)| &= \bigg|\int_{G} [\varphi(g) f(g^{-1}h) - \varphi(g) f(h)] \, \d g \bigg| \\
        &\leq \int_{U} \varphi(g) |f(g^{-1}h) - f(h)| \, \d g \\
        &\leq \int_{U} \varphi(g) |\rho(g)f - f|_\infty \, \d g \\
        &\leq \int_{U} \varphi(g) (\varphi/2) \, \d g = \frac{\varphi}{2}.
    \end{align*}
    By Proposition~\ref{prop4.1}, $T_\varphi$ is compact operator on $L^2(G)$.
    If $\lambda$ is an eigenvalue of $T_\varphi$, let $V(\lambda)$ be the $\lambda$-eigenspace.
    By the spectral theorem, the spaces $V(\lambda)$ are finite-dimensional (except perhaps $V(0)$), mutually orthogonal, and they span $L^2(G)$ as a Hilbert space.
    By Proposition~\ref{prop4.3} they are $T_\varphi$-invariant.
    Let $f_\lambda$ be the projection of $f$ on $V(\lambda)$.
    Orthogonality of the $f_\lambda$ implies that
    % Equation
    \begin{equation}\label{eqpeterweyl}
        \sum_{\lambda} |f_\lambda|_{2}^{2} = |f|_{2}^{2} < \infty.
    \end{equation}
    Let
    \[
        f' = T_\varphi(f''), \qquad f'' =  \sum_{|\lambda| > q} f_\lambda,
    \]
    where $q > 0$ remains to be chosen.
    We note that $f'$ and $f''$ are both contained in $\bigoplus_{|\lambda| > q} V(\lambda)$, which is a finite-dimensional vector space, and closed under right translation by Proposition~\ref{prop4.3}, and by Theorem~\ref{thmleftrightregularrepandmatcoeff}, it follows that they are matrix coefficients.

    By~\eqref{eqpeterweyl}, we may choose $q$ so that $\sum_{0 < q < |\lambda|} |f_\lambda|_{2}^{2}$ is as small as we like.
    Using~\eqref{eqprop4.1} may thus arrange that
    % Equation
    \begin{equation}\label{eqpeterweylii}
        \Bigg|\sum_{0 < |\lambda| < q} f_\lambda\Bigg|_1 \leq \Bigg|\sum_{0 < |\lambda| < q} f_\lambda \Bigg|_2 = \sqrt{\sum_{0 < |\lambda| < q} |f_\lambda|_{2}^{2}} < \frac{\varepsilon}{2|\varphi|_\infty}.
    \end{equation}
    We have
    \[
        T_\varphi(f - f'') = T_\varphi \Bigg(f_0 + \sum_{0 < |\lambda| < q} f_\lambda \Bigg) = T_\varphi \Bigg(\sum_{0 < |\lambda| < q} f_\lambda \Bigg).
    \]
    Using~\eqref{eqprop4.1} and~\eqref{eqpeterweylii} we have $|T_\varphi(f - f'')|_\infty \leq \varphi/2$.
    Now
    \[
        |f - f'|_\infty = |f - T_\varphi f + T_\varphi(f - f'')| \leq |f - T_\varphi f| + |T_\varphi f - T_\varphi f''| \leq \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon.
    \]
\end{proof}%}T}E}X

% Corollary
\begin{corollary}\label{cor4.1}%{T{E{X
    The matrix  coefficients of $G$ are dense in $L^2(G)$.
\end{corollary}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Since $C(G)$ is dense in $L^2(G)$, this follows from the Peter-Weyl Theorem and~\eqref{eqpnormineq}.
\end{proof}%}T}E}X

We say that a topological group $G$ has \emph{no small subgroups} if it has a neighborhood $U$ of the identity such that the only subgroup of $G$ contained in $U$ is just $\{1\}$.
For example, we will see that Lie groups have no small subgroups.
On the other hand, some groups, such as $\mathrm{GL}(n, \mathbb{Z}_p)$ where $\mathbb{Z}_p$ is the ring of $p$-adic integers, have a neighborhood basis at the identity consisting of open subgroups.
Such a group is called \emph{totally disconnected}, and for such a group the no small subgroups property fails very strongly.

A representation is called \emph{faithful} if its kernel is trivial.

% Theorem
\begin{theorem}\label{thm4.2}%{T{E{X
    Let $G$ be a compact group that has no small subgroups.
    Then $G$ has a faithful finite-dimensional representation.
\end{theorem}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Let $U$ be a neighborhood of the identity tat contains no subgroup but $\{1\}$.
    By Peter-Weyl Theorem, we can find a finite-dimensional representation $\pi$ and a matrix coefficient $f$ such that $f(1) = 0$ but $f(g) > 1$ when $g \notin U$.
    The function $f$ is constant on the kernel of $\pi$, so that kernel is contained in $U$.
    It follows that the kernel is trivial.
\end{proof}%}T}E}X

We will now prove a fact about infinite-dimensional representations of a compact group $G$.
The Peter-Weyl Theorem amounts to a ``completeness'' of the finite-dimensional representations from the point of view o harmonic analysis.
One aspect of this is the $L^2$ completeness asserted in Corollary~\ref{cor4.1}.
Another aspect, which we now prove, is that there are no irreducible unitary infinite-dimensional representations.
From the point of view of harmonic analysis, these two statements are closely related and in fact equivalent.
Representation theory and Fourier analysis on groups are essentially the same thing.

If $H$ is a Hilbert space, a representation $\pi : G \to \End(H)$ is called \emph{unitary} if ${\left\langle \pi(g)v, \pi(g)w\right\rangle} = {\left\langle v, w\right\rangle}$ for all $v, w \in H, g \in G$.
It is also assumed that the map $(g, v) \mapsto \pi(g)v$ from $G \times H \to H$ is continuous.

% Theorem
\begin{theorem}[Peter and Weyl]\label{thmpeterweylii}%{T{E{X
    Let $H$ be a Hilbert space and $G$ a compact group.
    Let $\pi : G \to \End(H)$ be a unitary representation.
Then $H$ is a direct sum of finite-dimensional irreducible representations.
\end{theorem}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    We first show that if $H$ is nonzero then it has an irreducible finite dimensional invariant subspace.
    We choose a nonzero vector $v \in H$.
    Let $N$ be a neighborhood of the identity of $G$ such that if $g \in N$ then $|\pi(g)v - v| \leq |v|/2$.
    We can find a nonnegative continuous function $\varphi$ on $G$ supported in $N$ such that $\int_{G} \varphi(g) \, \d g = 1$.

    We claim that $\int_{G} \varphi(g) \pi(g)v \, \d g \neq 0$.
    This can be proved by taking the inner product with $v$.
    Indeed
    % Equation
    \begin{equation}\label{eqthmpeterweylii}
        {\left\langle \int_{G} \varphi(g) \pi(g)v \, \d g, v\right\rangle} = {\left\langle v, v\right\rangle} - {\left\langle \int_{N} \varphi(g)(v - \pi(g)v) \, \d g, v\right\rangle}
    \end{equation}
    and
    \[
        {\left|{\left\langle \int_{N} \varphi(g)(v - \pi(g)v) \, \d g, v\right\rangle}\right|} \leq \int_{N}|v - \pi(g)v| \, \d g \cdot |v| \leq |v|^2/2.
    \]
    Thus, the two terms in~\eqref{eqthmpeterweylii} differ in absolute value and cannot cancel.

    Next, using the Peter-Weyl Theorem, we may find a matrix coefficient $f$ such that $|f - \varphi|_\infty < \varepsilon$, where $\varepsilon$ can be chosen arbitrarily.
    We have
    \[
        {\left|\int_{G} (f - \varphi)(g) \pi(g) v \, \d g\right|} \leq \varepsilon |v|,
    \]
    so if $\varepsilon$ is sufficiently small we have $\int_{G} f(g) \pi(g)v \, \d g \neq 0$.

    Since $f$ is a matrix coefficient, so is the function $g \mapsto f(g^{-1})$ by Proposition~\ref{propmatcoeffofpiandpihat}.
    Thus, let $(\rho, W)$ be a finite-dimensional representation with $w \in W$ and $L : W \to \mathbb{C}$ a linear functional such that $f(g^{-1}) = L(\rho(g)w)$.
    Define a map $T : W \to H$ by
    \[
        T(x) = \int_{G} L(\rho(g^{-1})x) \pi(g)v \, \d g.
    \]
    This is an intertwining map by the same argument used to prove~\eqref{eqlemequivariantcxrep}.
    It is nonzero since $T(w) = \int_{G} f(g) \pi(g)v \, \d g \neq 0$.

    We have proven that every nonzero unitary representation of $G$ has a nonzero finite-dimensional invariant subspace, which we may obviously assume to be irreducible.
    From this we deduce the stated result.
    Let $(\pi, H)$ be a unitary representation of $G$.
    Let $\Sigma$ be the set of orthogonal finite-dimensional irreducible invariant subspaces of $H$, ordered by inclusion.
    Thus if $S \in \Sigma$ and $U, V \in S$, then $U$ and $V$ are finite-dimensional irreducible invariant subspaces.
    If $U \neq V$, then $U \perp V$.
    By Zorn's Lemma, $\Sigma$ has a maximal element $S$ and we are done if $S$ spans $H$ as a Hilbert space.
    Otherwise, let $H'$ be the orthogonal complement of the span of $S$.
    By what we have shown, $H'$ contains an invariant irreducible subspace.
    We may append this subspace to $S$, contradicting its maximality.
\end{proof}%}T}E}X

% UnnumberedSection
\section*{EXERCISES}%{T{E{X
% Exercise
\begin{exercise}\label{}
    Let $G$ be totally disconnected and let $\pi : G \to \mathrm{GL}(n, \mathbb{C})$ be a finite-dimensional representation.
    Show that he kernel of $\pi$ is open.
    (\textbf{Hint:} use the fact that $\mathrm{GL}(n, \mathbb{C})$ has no small subgroups.)
    Conclude (in contrast with Theorem~\ref{thm4.2}) that the compact group $\mathrm{GL}(n, \mathbb{Z}_p)$ has no faithful finite-dimensional representation.
\end{exercise}

% Exercise
\begin{exercise}\label{}
    Suppose that $G$ is compact Abelian group and $H \subseteq G$ a closed subgroup.
    Let $\mychi : H \to \mathbb{C}^{\times}$ be a character.
    Show that $\mychi$ can be extended to a character of $G$.
    (\textbf{Hint:} Apply Theorem~\ref{thmpeterweylii} to the space $V = \{f \in L^2(G) : f(hg) = \mychi(h) f(g)\}$.
    To show that $V$ is nonzero, note that if $\varphi \in C(G)$ then $f(g) = \int_{G} \varphi(hg) {\mychi(h)}^{-1} \, \d h$ defines an element of $V$.
    Use Urysohn's Lemma to construct $\varphi$ such that $f \neq 0$.)
\end{exercise}

%}T}E}X

%}T}E}X

%}T}E}X
% Part
\part{Lie Group Fundamentals}\label{}%{T{E{X
% Chapter
\chapter{Lie Subgroups of $\mathrm{GL}(n, \mathbb{C})$}\label{}%{T{E{X
If $U$ is an open subset of $\mathbb{R}^{n}$, we say that a map $\varphi : U \to \mathbb{R}^{m}$ is \emph{smooth} if it has continuous partial derivatives of all orders.
More generally, if $X \subseteq \mathbb{R}^{n}$ is not necessarily open, we say that a map $\varphi : X \to \mathbb{R}^{n}$ is \emph{smooth} if for every $x \in X$ there exists an open set $U$ of $\mathbb{R}^{n}$ containing $x$ such that $\varphi$ can be extended to a smooth map on $U$.
A \emph{diffeomorphism} of $X \subseteq \mathbb{R}^{n}$ with $Y \subseteq \mathbb{R}^{m}$ is a homeomorphism $F : X \to Y$ such that both $F$ and $F^{-1}$ are smooth.
We will assume as known the following useful criterion.

% UnnumberedTheorem
\begin{utheorem}[Inverse Function Theorem]%{T{E{X
    If $U \subseteq \mathbb{R}^{d}$ is open and $u \in U$, if $F : U \to \mathbb{R}^{n}$ is a smooth map, with $d < n$, and if the matrix of partial derivatives $\big(\pdv{F_i}{x_j}\big)$ has rank $d$ at $u$, then $u$ has a neighborhood $N$ such that $F$ induces a diffeomorphism of $N$ onto its image.
\end{utheorem}%}T}E}X

A subset $X$ of a topological space $Y$ is \emph{locally closed} (in $Y$) if for all $x \in X$ there exists an open neighborhood $U$ of $x$ in $Y$ such that $X \cap U$ is closed in $U$.
This is equivalent to saying that $X$ is the intersection of an open set and a closed set.
We say that $X$ is a \emph{submanifold of $\mathbb{R}^{n}$ of dimension $d$} if it is locally closed subset and every point of $X$ has a neighborhood that is diffeomorphic to an open set in $\mathbb{R}^{d}$.

Let us identify $\mathrm{M}(n, \mathbb{C})$ with the Euclidean space $\mathbb{C}^{n^2} \equiv \mathbb{R}^{2n^2}$.
The subset $\mathrm{GL}(n, \mathbb{C})$ is open, and if a closed subgroup $G$ of $\mathrm{GL}(n, \mathbb{C})$ is a submanifold of $\mathbb{R}^{n^2}$ in this identification, we say that $G$ is a \emph{closed Lie subgroup} of $\mathrm{GL}(n, \mathbb{C})$.
It may be shown that any closed subgroup of $\mathrm{GL}(n, \mathbb{C})$ is a closed Lie subgroup.
See Remark~\ref{rem7.1} and Remark~\ref{rem7.2} for some subtleties behind the innocent term ``closed Lie subgroup''.

More generally, a \emph{Lie group} is a topological group $G$ that is a differentiable manifold such that the multiplication and inverse maps $G \times G \to G$ and $G \to G$ are smooth.
We will give a proper definition of a differentiable manifold in the next chapter.
In this chapter, we will restrict ourselves to closed Lie subgroups of $\mathrm{GL}(n, \mathbb{C})$.

% Example
\begin{example}\label{}
    If $F$ is a field, then the \emph{general linear group} $\mathrm{GL}(n,F)$ is the group of invertible $n \times n$ matrices with coefficients in $F$.
    It is a Lie group.
    Assuming $F = \mathbb{R}$ or $\mathbb{C}$, the group $\mathrm{GL}(n,F)$ is an open set in $\mathrm{M}(n,F)$ and hence a manifold of dimension $n^2$ if $F = \mathbb{R}$ or $2n^2$ if $F = \mathbb{C}$.
    The \emph{special linear group} is the subgroup $\mathrm{SL}(n,F)$ of matrices with determinant 1.
    It is a closed Lie subgroup of $\mathrm{GL}(n,F)$ of dimension $n^2 - 1$ or $2(n^2 - 1)$.
\end{example}

% Example
\begin{example}\label{}
    If $F = \mathbb{R}$ or $\mathbb{C}$, let $\mathrm{O}(n,F) = \{g \in \mathrm{GL}(n,F) : g \cdot g^t = I\}$.
    This is the $n \times n$ \emph{orthogonal group}.
    More geometrically, $\mathrm{O}(n,F)$ is the group of linear transformations preserving the quadratic form $Q(x_1, \ldots, x_n) = x_{1}^{2} + x_{2}^{2} + \cdots + x_{n}^{2}$.
    To see this, if $(x) = (x_1, \ldots, x_n)^t$ is represented as a column vector, we have $Q(x) = Q(x_1, \ldots, x_n) = x^t \cdot x$, and it is clear that $Q(gx) = Q(x)$ if $g \cdot g^t = I$.
    The group $\mathrm{O}(n, \mathbb{R})$ is compact and is usually denoted simply $\mathrm{O}(n)$ contains elements of determinants $\pm1$.
    The subgroup of elements of determinant 1 is the \emph{special orthogonal group} $\mathrm{SO}(n)$.
    The dimension of $\mathrm{O}(n)$ and its subgroup $\mathrm{SO}(n)$ of index 2 is $\frac{1}{2}(n^2 - n)$.
    This will be seen in Proposition~\ref{prop5.6} when we compute their Lie algebra (which is the same for both groups).
\end{example}

% Example
\begin{example}\label{}
    More generally, over any field, a vector space $V$ on which there is given a quadratic form $q$ is called a \emph{quadratic space}, and the set $O(V, q)$ of linear transformations of $V$ preserving $q$ is an orthogonal group.
    Over the complex numbers, it is not hard to prove that all orthogonal groups are isomorphic (Exercise 5.4), but over the real numbers, some orthogonal groups are not isomorphic to $\mathrm{O}(n)$.
    If $k + r = n$, let $O(k, r)$ be the subgroup of $\mathrm{GL}(n, \mathbb{R})$ preserving the indefinite quadratic form $x_{1}^{2} + \cdots + x_{k}^{2} - x_{k+1}^{2} - \cdots - x_{n}^{2}$.
    If $r = 0$, this is $\mathrm{O}(n)$, but otherwise this group is noncompact.
    The dimensions of these Lie groups are, like $\mathrm{SO}(n)$, equal to $\frac{1}{2} (n^2 - n)$.
\end{example}

% Example
\begin{example}\label{}
    The \emph{unitary group} $U(n) = \{g \in \mathrm{SL}(n, \mathbb{C}) : g \cdot \overline{g^t} = I\}$.
    If $g \in \mathrm{U}(n)$ then $|\det(g)| = 1$, and every complex number of absolute value 1 is a possible determinant of $g \in \mathrm{U}(n)$.
    The \emph{special unitary group} $\mathrm{SU}(n) = \mathrm{U}(n) \cap \mathrm{SL}(n, \mathbb{C})$.
    The dimensions of $\mathrm{U}(n)$ and $\mathrm{SU}(n)$ are $n^2$ and $n^2 - 1$, just like $\mathrm{GL}(n, \mathbb{R})$ and $\mathrm{SL}(n, \mathbb{R})$.
\end{example}

% Example
\begin{example}\label{}
    If $F = \mathbb{R}$ or $\mathbb{C}$, let $\mathrm{Sp}(2n, F) = \{g \in \mathrm{GL}(2n, F) : g \cdot J \cdot g^t = J\}$, where
    \[
        J = \begin{pmatrix}
            0   & -I_n  \\
            I_n & 0
        \end{pmatrix}
        .
    \]
    This is the \emph{symplectic group}.
    The compact group $\mathrm{Sp}(2n, \mathbb{C}) \cap \mathrm{U}(2n)$ will be denoted as simply $\mathrm{Sp}(2n)$.
\end{example}

A \emph{Lie algebra} over a field $F$ is a vector space $\mathfrak{g}$ over $F$ endowed with a bilinear map, the \emph{Lie bracket}, denoted $(X, Y) \to [X, Y]$ for $X, Y \in \mathfrak{g}$, that satisfies $[X, Y] = - [Y, X]$ and the \emph{Jacobi identity}
% Equation
\begin{equation}\label{eqjacobiidentity}
    [X, [Y, Z]] + [Y, [Z, X]] + [Z, [X, Y]] = 0.
\end{equation}
the identity $[X, Y] = - [Y, X]$ implies that $[X, X] = 0$.

We will show that it is possible to associate a Lie algebra with any Lie group.
We will show this for closed Lie subgroups of $\mathrm{GL}(n, \mathbb{C})$ in this chapter and for arbitrary lie groups in Chapter 7.

First we give two purely algebraic examples of Lie algebras.

% Example
\begin{example}\label{}
    Let $A$ be an associative algebra.
    Define a bilinear operation on $A$ by $[X, Y] = XY - YX$.
    With this definition, $A$ becomes a Lie algebra.

    If $A = \mathrm{M}(n,F)$, where $F$ is a field, we will denote the Lie algebra associated with $A$ by the previous example as $\mathfrak{gl}(n,F)$.
    After Proposition 5.5 it will become clear that this is the Lie algebra of $\mathrm{GL}(n,F)$ when $F = \mathbb{R}$ or $\mathbb{C}$.
    Similarly, if $V$ is a vector space over $F$, then the space $\End(V)$ of $F$-linear transformations $V \to V$ is an associative algebra and hence a Lie algebra, denote $\mathfrak{gl}(V)$.
\end{example}

% Example
\begin{example}\label{}
    Let $F$ be a field and let $A$ be an $F$-algebra.
    By a \emph{derivation} of $A$ we mean a map $D : A \to A$ that if $F$-linear, and satisfies $D(fg) = fD(g) + D(f)g$.
    We have $D(1 \cdot 1) = 2D(1)$, which implies that $D(1) = 0$, and therefore $D(c) = 0$ for any $c \in F \subseteq A$.
    It is easy to check that if $D_1$ and $D_2$ are derivations, then so is $[D_1, D_2] = D_1D_2 - D_2D_1$.
    However, $D_1D_2$ and $D_2D_1$ are themselves not derivations.
    It is easy to check that the derivations of $A$ form a Lie algebra.
\end{example}

The \emph{exponential map} $\exp : \mathrm{M}(n, \mathbb{C}) \to \mathrm{GL}(n, \mathbb{C})$ is defined by
% Equation
\begin{equation}\label{}
    \exp(X) = I + X + \frac{1}{2}X^2 = \frac{1}{6}X^3 + \cdots.
\end{equation}
This series is convergent for all matrices $X$.

% Remark
\begin{remark}\label{}%{T{E{X
    If $X$ and $Y$ commute, then $\exp(X+Y) = \exp(X) \exp(Y)$.
    If they do not commute, this is not true.
\end{remark}%}T}E}X

A \emph{one-parameter subgroup} of Lie group $G$ is a continuous homomorphism $\mathbb{R} \to G$.
We denote this by $t \mapsto g_t$.
Since $tX$ and $uX$ commute, for $X \in \mathrm{M}(n, \mathbb{C})$, the map $t \to \exp(tX)$ is a one-parameter subgroup.
We will also denote $\exp(X) = e^X$.

% Proposition
\begin{proposition}\label{prop5.1}%{T{E{X
    Let $U$ be an open subset of $\mathbb{R}^{n}$, and let $x \in U$.
    Then we may find a smooth function $f$ with compact support contained in $U$ that does not vanish at $x$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    We may assume $x = (x_1, \ldots, x_n)$ is the origin.
    Define
    % DcaseDefinition
    \[
        f(x_1, \ldots, x_n) =
        \begin{dcases}
            e^{-(1 - |x|^2/r^2)^{-1}} & \text{if} \ |x| \leq r \\
            0 & \text{otherwise.} \ 
        \end{dcases}
    \]
    This function is smooth and has support in the ball $\{|x| \leq r\}$.
    Taking $r$ sufficiently, we can make this vanish outside $U$.
\end{proof}%}T}E}X

% Proposition
\begin{proposition}\label{prop5.2}%{T{E{X
    Let $G$ be a closed Lie subgroup of $\mathrm{GL}(n, \mathbb{C})$, and let $X \in \mathrm{M}(n, \mathbb{C})$.
    Then the path $t \to \exp(tX)$ is tangent to the submanifold $G$ of $\mathrm{GL}(n, \mathbb{C})$ at $t = 0$ if and only if it is contained in $G$ for all $t$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    If $\exp(tX)$ is contained in $G$ for all $t$, then clearly it is tangent to $G$ at $t = 0$.
    We must prove the converse.
    Suppose that $\exp(t_0X) \notin G$ for some $t_0 > 0$.
    Using Proposition~\ref{prop5.1}, let $\varphi_0$ be a smooth compactly supported function on $\mathrm{GL}(n, \mathbb{C})$ such that $\varphi_0(g) = 0$ for all $g \in G, \varphi_0 \geq 0$, and $\varphi_0(\exp(t_0X)) \neq 0$.
    Let
    \[
        f(t) = \varphi(\exp(tX)), \qquad \varphi(h) = \int_{G} \varphi_0(hg) \, \d g, \qquad t \in \mathbb{R},
    \]
    in terms of a left Haar measure on $G$.
    Clearly, $\varphi$ is constant on the cosets $hG$ of $G$, vanishes on $G$, but is nonzero at $\exp(t_0X)$.
    For any $t$,
    \[
        f'(t) = \dv{}{u}\varphi(\exp(tX) \exp(uX)) \Big|_{u=0} = 0
    \]
    since the path $u \to \exp(tX)\exp(uX)$ is tangent to the coset $\exp(tX)G$ and $\varphi$ is constant on such cosets.
    Moreover, $f(0) = 0$.
    Therefore, $f(t) = 0$ for all $t$, which is a contradiction since $f(t_0) \neq 0$.
\end{proof}%}T}E}X

% Proposition
\begin{proposition}\label{prop5.3}%{T{E{X
    Let $G$ be a closed Lie subgroup of $\mathrm{GL}(n, \mathbb{C})$.
    The set $\mathrm{Lie}(G)$ of all $X \in \mathrm{M}(n, \mathbb{C})$ such that $\exp(tX) \subseteq G$ is a vector space whose dimension is equal to the dimension of $G$ as a manifold.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    This is clear form the characterization of Proposition~\ref{prop5.2}.
\end{proof}%}T}E}X

% Proposition
\begin{proposition}\label{prop5.4}%{T{E{X
    Let $G$ be a closed Lie subgroup of $\mathrm{GL}(n, \mathbb{C})$.
    The map $X \to \exp(X)$ gives a diffeomorphism of a neighborhood of the identity in $\mathrm{Lie}(G)$ onto a neighborhood of the identity in $G$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Fir we note that since $\exp(X) = I + X + \frac{1}{2} X^2 + \cdots$, the Jacobian of $\exp$ at the identity is 1, s o$\exp$ induces a diffeomorphism of an open neighborhood $U$ of the identity in $\mathrm{M}(n, \mathbb{C})$ onto a neighborhood of the identity in $\mathrm{GL}(n, \mathbb{C}) \subseteq \mathrm{M}(n, \mathbb{C})$.
    Now, since by Proposition~\ref{prop5.3} $\mathrm{Lie}(H)$ is a vector subspace of dimension equal to the dimension of $H$ as a manifold, the Inverse Function Theorem implies that the image of $\mathrm{Lie}(H) \cap U$ must be mapped onto an open neighborhood of the identity in $H$.
\end{proof}%}T}E}X

% Proposition
\begin{proposition}\label{prop5.5}%{T{E{X
    If $G$ is a closed Lie subgroup of $\mathrm{GL}(n, \mathbb{C})$, and if $X, Y \in \mathrm{Lie}(G)$, then $[X, Y] \in \mathrm{Lie}(G)$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    It is evident that $\mathrm{Lie}(G)$ is mapped to itself under conjugation by elements of $G$.
    Thus, $\mathrm{Lie}(G)$ contains
    \[
        \frac{1}{t}(e^{tX}Ye^{-tX} - Y) = XY - YX + \frac{t}{2}(X^2Y - 2XYX + YX^2) + \cdots.
    \]
    Because this is true for all $t$, passing to the limit $t \to 0$ shows that $[X, Y] \in \mathrm{Lie}(G)$.
\end{proof}%}T}E}X

We see that $\mathrm{Lie}(G)$ is a Lie subalgebra of $\mathfrak{gl}(n, \mathbb{C})$.
Thus, we are able to associate a Lie algebra with a Lie group.

% Example
\begin{example}\label{}
    The Lie algebra of $\mathrm{GL}(n,F)$ with $F = \mathbb{R}$ or $\mathbb{C}$ is $\mathfrak{gl}(n,F)$.
\end{example}

% Example
\begin{example}\label{}
    Let $\mathfrak{sl}(n,F)$ be the subspace of $X \in \mathfrak{gl}(n,F)$ such that $\tr(X) = 0$.
    This is a Lie subalgebra, and it is the Lie algebra of $\mathrm{SL}(n,F)$ when $F = \mathbb{R}$ or $\mathbb{C}$.
    This follows immediately from the fact that $\det(e^X) = e^{\tr(X)}$ for any matrix $X$ because if $x_1, \ldots, x_n$ are the eigenvalues of $X$, then $e^{x_1}, \ldots, e^{x_n}$ are the eigenvalues of $e^X$.
\end{example}

% Example
\begin{example}\label{}
    Let $\mathfrak{o}(n,F)$ be the set of $X \in \mathfrak{gl}(n,F)$ that are skew-symmetric, in other words, that satisfy $X + X^t = 0$.
    It is easy to check that $\mathfrak{o}(n,F)$ is closed under the Lie bracket and hence is a Lie subalgebra.
\end{example}

% Proposition
\begin{proposition}\label{prop5.6}%{T{E{X
    If $F = \mathbb{R}$ or $\mathbb{C}$, the Lie algebra of $O(n,F)$ is $\mathfrak{o}(n,F)$.
    The dimension of $\mathrm{O}(n)$ is $\frac{1}{2}(n^2 - n)$, and the dimension of $\mathrm{O}(n, \mathbb{C})$ is $n^2 - n$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Let $G = \mathrm{O}(n,F), \mathfrak{g} = \mathrm{Lie}(G)$.
    Suppose $X \in \mathfrak{o}(n,F)$.
    Exponentiate the identity $-tX = tX^t$ to get
    \[
        \exp(tX)^{-1} = {\exp(tX)}^{t},
    \]
    whence $\exp(tX) \in \mathrm{O}(n,F)$ for all $t \in \mathbb{R}$.
    Thus, for all $t$,
    % UnnumberedAlign
    \begin{align*}
        I &= \exp(tX) \cdot {\exp(tX)}^{t} \\
        &= (I + tX + \frac{1}{2}t^2X^2 + \cdots ) (I + tX^t + \frac{1}{2} t^2(X^t)^2 + \cdots ) \\
        &= I + t(X + X^t) + \frac{1}{2}t^2(X^2 + 2X \cdot X^t  +(X^t)^2) + \cdots.
    \end{align*}
    Since this is true for all $t$, each coefficient in this Taylor series must vanish (except of course the constant one).
    In particular, $X + X^t = 0$.
    This proves that $\mathfrak{g} = \mathfrak{o}(n,F)$.

    The dimensions of $\mathrm{O}(n)$ and $\mathrm{O}(n, \mathbb{C})$ are most easily calculated by computing the dimension of the Lie algebras.
    A skew-symmetric matrix is determined by it supper triangular entries, and there are $\frac{1}{2}(n^2 - n)$ of these.
\end{proof}%}T}E}X

% Example
\begin{example}\label{}
    Let $\mathfrak{u}(n)$ be the set of $X \in \mathrm{GL}(n, \mathbb{C})$ such that $X + \overline{X^t} = 0$.
    One checks easily that this is closed under the $\mathfrak{gl}(n, \mathbb{C})$ Lie bracket $[X, Y] = XY - YX$.
    Despite the fact that these matrices have complex entries, this is a \emph{real} Lie algebra, for it is only a real vector space, not a complex one.
    (It is not closed under multiplication by complex scalars.)
    It may be checked along the lines of Proposition~\ref{prop5.6} that $\mathfrak{u}(n)$ is the Lie algebra of $\mathrm{U}(n)$, and similarly $\mathfrak{su}(n) = \{X \in \mathfrak{u}(n) : \tr(X) = 0\}$ is the Lie algebra of $\mathrm{SU}(n)$.
\end{example}

% Example
\begin{example}\label{}
    Let $\mathfrak{sp}(2n, F)$ be the set of matrices $X \in \mathrm{M}(2n, F)$ that satisfy $XJ + JX^t = 0$, where
    \[
        J = \begin{pmatrix}
            0 & -I_n  \\
            I_n & 0
        \end{pmatrix}
        .
    \]
    This is the Lie algebra of $\mathrm{Sp}(2n, F)$.
\end{example}

% UnnumberedSection
\section*{EXERCISES}%{T{E{X
% Exercise
\begin{exercise}\label{}
    Show that $\mathrm{O}(n, m)$ is the group of $g \in \mathrm{GL}(n + m, \mathbb{R})$ such that $gJ_1 g^t = J_1$, where
    \[
        J_1 = \begin{pmatrix}
            I_n &   \\
            & I_m
        \end{pmatrix}.
    \]
\end{exercise}

% Exercise
\begin{exercise}\label{}
    If $F = \mathbb{R}$ or $\mathbb{C}$, let $\mathrm{O}_J(F)$ be the group of all $g \in \mathrm{GL}(N, F)$ such that $gJ^tg = J$, where $J$ is the $N \times N$ matrix
    % Equation
    \begin{equation}\label{eqexc5.2}
        J = % Pmatrix
        \renewcommand\arraystretch{1}
        \begin{pmatrix}
            &  & 1 \\ % add[2em]toIncreaseSpacing
            & \iddots &  \\
            1 &  &
        \end{pmatrix}.
    \end{equation}
    Show that $\mathrm{O}_J(\mathbb{R})$ is conjugate in $\mathrm{GL}(N, \mathbb{R})$ to $\mathrm{O}(n, n)$ if $N = 2n$ and to $\mathrm{O}(n + 1, n)$ if $N = 2n + 1$.
    (\textbf{Hint:} Find a matrix $\sigma \in \mathrm{GL}(N, \mathbb{R})$ such that $\sigma J^t \sigma = J_1$, where $J$ is as in the previous exercise.)
\end{exercise}

% Exercise
\begin{exercise}\label{}
    Let $J$ be as in the previous exercise, and let
    \[
        % Pmatrix
        % \renewcommand\arraystretch{1}
        \begin{pmatrix}
            \frac{1}{\sqrt{2i}} & & & \cdots & & & -\frac{i}{\sqrt{2i}} \\ % add[2em]toIncreaseSpacing
            & \frac{1}{\sqrt{2i}} & & & & -\frac{i}{2i} & \\
            & & \ddots & & \iddots & & \\
            \vdots & & & & & & \vdots \\
            & & \iddots & & \ddots & & \\
            & \frac{i}{\sqrt{2i}} & & & & -\frac{1}{2i} & \\
            \frac{i}{\sqrt{2i}} & & & \cdots & & & -\frac{1}{\sqrt{2i}}
        \end{pmatrix}
    \]
    with all entries not on one of the two diagonals equal to zero.
    If $N$ is odd, the middle element of this matrix is 1.
    % Enumerate
    % \renewcommand{\labelenumi}{\emph{\Roman{enumi}.}}
    \begin{enumerate}[label= (\roman*),font=\normalfont,before=\normalfont]
        \item Show that $\sigma^t \sigma = J$, with $J$ as in~\eqref{eqexc5.2}.
            With $\mathrm{O}_J(F)$ as in Example 5.2, deduce that $\sigma^{-1} \mathrm{O}_J(\mathbb{C})\sigma = \mathrm{O}(n, \mathbb{C})$.
            Why does the same argument \emph{not} prove that $\sigma^{-1}\mathrm{O}_J(\mathbb{R}) \sigma = \mathrm{O}(n, \mathbb{R})$?
        \item Show that if $g \in \mathrm{O}_J(\mathbb{C})$ and $h = \sigma^{-1}g\sigma$, then $h$ is real if and only if $g$ is unitary.
        \item Show that the group $\mathrm{O}_J(\mathbb{C}) \cap U(N)$ is conjugate in $\mathrm{GL}(N, \mathbb{C})$ to 
    \end{enumerate}
\end{exercise}

% Exercise
\begin{exercise}\label{}
    Let $V_1$ and $V_2$ be vector spaces over a field $F$, let $q_i$ be a quadratic form on $V_i$ for $i = 1, 2$.
    The quadratic spaces are called \emph{equivalent} if there exists an isomorphism $l : V_1 \to V_2$ such that $q_1 = q_2 \circ l$.
    % Enumerate
    % \renewcommand{\labelenumi}{\emph{\Roman{enumi}.}}
    \begin{enumerate}[label=(\roman*),font=\normalfont,before=\normalfont]
        \item Show that over a field of characteristic not equal to 2, any quadratic form is equivalent to $\sum a_i x_i^2$ for some constants $a_i$.
        \item Show that, if $F = \mathbb{C}$, then any quadratic space of dimension $n$ is equivalent to $\mathbb{C}^{n}$ with the quadratic form $x_1^2 + \cdots + x_n^2$.
        \item Show that, if $F = \mathbb{R}$, then any quadratic space of dimension $n$ is equivalent to $\mathbb{R}^{n}$ with the quadratic form $x_1^2 + \cdots + x_r^2 - x_{r+1}^2 - \cdots - x_n^2$ for some $r$.
    \end{enumerate}
\end{exercise}

% Exercise
\begin{exercise}\label{}
    Compute the Lie algebra of $\mathrm{Sp}(2n, \mathbb{R})$ and the dimension of the group.
\end{exercise}

Let $\mathbb{H} = \mathbb{R} \oplus \mathbb{R}i \oplus \mathbb{R}j \oplus \mathbb{R}k$ be the ring of quaternions, where $i^2 = j^2 = k^2 = -1$ and $ij = -ji = k, jk = -kj = i, ki = -ik = j$.
Then $\mathbb{H} = \mathbb{C} \oplus \mathbb{C}j$.
If $x = a + bi + cj + dk \in \mathbb{H}$ with $a, b, c, d$ real, let $\overline{x} = a - bi - cj - dk$.
If $u \in \mathbb{C}$, then $juj^{-1} = \overline{u}$.
The group $\mathrm{GL}(n,\mathbb{H})$ consists of all $n \times n$ invertible quaternion matrices.

% Exercise
\begin{exercise}\label{}
    Show that there is a ring isomorphism $\mathrm{M}(n,\mathbb{H}) \to \mathrm{M}(2n,\mathbb{C})$ with the following description.
    Any $A \in \mathrm{M}(n,\mathbb{H})$ may be written uniquely as $A_1 + A_2j$; the isomorphism in question maps
    \[
        A_1 + A_2j \mapsto \begin{pmatrix}
            A_1 & A_2  \\
            -\overline{A_2} & \overline{A_1}
        \end{pmatrix}.
    \]
\end{exercise}

% Exercise
\begin{exercise}\label{}
    Show that if $A \in \mathrm{M}(n,\mathbb{H})$, then $A \cdot \overline{A}^t = I$  if and only if the complex $2n \times 2n$ matrix
    \[
        \begin{pmatrix}
            A_1 & A_2  \\
            -\overline{A_2} & A_1
        \end{pmatrix}
    \]
    is in both $\mathrm{Sp}(2n, \mathbb{C})$ and $\mathrm{U}(2n)$.
    Recall that the intersection of these groups was the group denoted $\mathrm{Sp}(2n)$.
\end{exercise}

% Exercise
\begin{exercise}\label{}
    Show that the groups $\mathrm{SO}(2)$, $\mathrm{SU}(2)$, and $\mathrm{Sp}(4)$ may be identified with the groups of matrices
    \[
        {\left\{ \begin{pmatrix}
                    a & b  \\
                    -\overline{b} & \overline{a}
                \end{pmatrix} : a, b \in F, |a|^2 + |b|^2 = 1\right\}},
    \]
    where $F = \mathbb{R}, \mathbb{C}$, or $\mathbb{H}$, respectively.
\end{exercise}

%}T}E}X

%}T}E}X
% Chapter
\chapter{Vector Fields}\label{}%{T{E{X
A \emph{smooth premanifold} of dimension $n$ is a Hausdorff topological space $M$ together with a set $\mathcal{U}$ of pairs $(U, \varphi)$, where the set of $U$ such that $(U, \varphi) \in \mathcal{U}$ for some $\varphi$ is an open cover of $M$ and such that, for each $(U, \varphi) \in \mathcal{U}$, the image $\varphi(U)$ of $\varphi$ is a open subset of $\mathbb{R}^{n}$ and $\varphi$ is a homeomorphism of $U$ onto $\varphi(U)$.
We assume that if $U, V \in \mathcal{U}$, then $\varphi_V \circ \varphi_U^{-1}$ is a diffeomorphism from $\varphi_U(U \cap V)$ onto $\varphi_V(U \cap V)$.
The set $\mathcal{U}$ is called a \emph{preatlas}.

If $M$ and $N$ are premanifolds, a continuous map $f : M \to N$ is \emph{smooth} if whenever $(U, \varphi)$ and $(V, \psi)$ are charts of $M$ and $N$, respectively, the map $\psi \circ f \circ \varphi^{-1}$ is a smooth map from $\varphi(U \cap f^{-1}(V)) \to \psi(V)$.
Smooth maps are the morphisms in the category of smooth premanifolds.
The smooth map $f$ is a \emph{diffeomorphism} if it is a bijection and has a smooth inverse.
Open subsets of $\mathbb{R}^{n}$ are naturally premanifolds, and the definitions of smooth maps and diffeomorphisms are consistent with the definitions already given in that special case.

If $M$ is a premanifold with atlas $\mathcal{U}$, and if we replace $\mathcal{U}$ by the largest set $\mathcal{U}'$ of all pairs $(U, \varphi)$, where $U$ is an open subset of $M$ and $\varphi$ is a diffeomorphism of $U$ onto an open subset of $\mathbb{R}^{n}$, then the such that of smooth maps $M \to N$ or $N \to M$, where $N$ is another premanifold, is unchanged.
If $\mathcal{U} = \mathcal{U}'$, then we call $\mathcal{U}'$ and \emph{atlas} and $M$ a \emph{smooth manifold}.

Suppose that $M$ is a smooth manifold and $m \in M$.
If $U$ is a neighborhood of $x$ and $(\varphi, U)$ is a chart such that $\varphi(x)$ is the origin in $\mathbb{R}^{n}$, then we may write $\varphi(u) = (x_1(u), \ldots, x_n(u))$, where $x_1, \ldots, x_n : U \to \mathbb{R}$ are smooth functions.
Composing $\varphi$ with a translation in $\mathbb{R}^{n}$, we may arrange that $x_i(m) = 0$, and it is often advantageous to do so.
We call $x_1, \ldots, x_n$ a set of \emph{local coordinates at} $m$ or \emph{coordinate functions on} $U$.
The set $U$ itself may be called a \emph{coordinate neighborhood}.

Let $m \in M$, and let $F = \mathbb{R}$ or $\mathbb{C}$.
A \emph{germ} of an $F$-valued function is a equivalence class of pairs $(U, f_U)$, where $U$ is an open neighborhood of $x$ and $f : U \to F$ is a function.
The equivalence relation is that $(U, f_U)$ and $(V, f_V)$ are equivalent if $f_U$ and $f_V$ are equal on some open neighborhood $W$ of $x$ contained in $U \cap V$.
Let $\mathcal{O}_m$ be the set of germs of smooth real-valued functions.
It is a ring in an obvious way, and evaluation at $m$ induces a surjective homomorphism $\mathcal{O}_m \to \mathbb{R}$, the \emph{evaluation map}.
We will denote the evaluation map $f \mapsto f(m)$, a slight abuse of notation since $f$ is a germ, not a function.
Let $\mathcal{M}_m$ be the kernel of this homomorphism; that is, the ideal of germs of smooth functions vanishing at $m$.
Then $\mathcal{O}_m$ is a local ring and $\mathcal{M}_m$ is its maximal ideal.

% Lemma
\begin{lemma}\label{lem6.1}%{T{E{X
    Suppose that $f$ is a smooth function on a neighborhood $U$ of the origin in $\mathbb{R}^{n}$, and $f(0, x_2, \ldots, x_n) = 0$ for $(0, x_2, \ldots, x_n) \in U$.
    Then
    % DcaseDefinition
    \[
        g(x_1, x_2, \ldots, x_n) =
        \begin{dcases}
            x_{1}^{-1}f(x_1, \ldots, x_n) & \text{if} \ x_1 \neq 0, \\
            \pdv{f}{x_1}(0, x_2, \ldots, x_n) & \text{if} \ x_1 = 0,
        \end{dcases}
    \]
    defines a smooth function on $U$.
\end{lemma}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    We show first that $g$ is continuous.
    Indeed, with $x_2, \ldots, x_n$ fixed,
    \[
        \lim_{x_1 \to 0} x_1^{-1} f(x_1, \ldots, x_n) = \pdv{f}{x_1}(0, x_2, \ldots, x_n)
    \]
    by the definition of the derivative.
    Convergence is uniform on compact sets in $x_2, \ldots, x_n$ since by the remainder form of Taylor's Theorem
    \[
        {\left|x_1^{-1} f(x_1, \ldots, x_n) - \pdv{f}{x_1}(0, x_2, \ldots, x_n)\right|} \leq \frac{B}{2} x_1,
    \]
    where $B$ is an upper bound for ${\left| \pdv[2]{f}{x_1}(0, x_2, \ldots, x_n)\right|}$ is continuous by the smoothness of $f$, it follows that $g$ is continuous.

    A similar argument based on Taylor's Theorem shows that the higher partial derivatives $ \pdv[n]{g}{x_1}$ are also continuous.

    Finally, the two functions
    \[
        \frac{\partial^{k_2 + \cdots + k_n}f}{\partial x_2^{k_2} \cdots \partial x_n^{k_n}} \ \text{and} \, \frac{\partial^{k_2 + \cdots + k_n}g}{\partial x_2^{k_2} \cdots \partial x_n^{k_n}}
    \]
    bear the same relationship to each other as \(f\) and $g$, so we obtain similarly continuity of the mixed partials \(\dfrac{\partial^{k_2 + \cdots + k_n}g}{\partial x_2^{k_2} \cdots \partial x_n^{k_n}}\).
\end{proof}%}T}E}X

% Proposition
\begin{proposition}\label{prop6.1}%{T{E{X
    Let $m \in M$, where $M$ is a smooth manifold of dimension $n$.
    Let $\mathcal{O} = \mathcal{O}_m$ and $\mathcal{M} = \mathcal{M}_m$.
    Let $x_1, \ldots, x_n$ be the germs of a set of local coordinates at $m$.
    Then $x_1, \ldots, x_n$ generate the ideal $\mathcal{M}$.
    Moreover, $\mathcal{M}/\mathcal{M}^2$ is a vector space of dimension $n$ generated by the images of $x_1, \ldots, x_n$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Although this is really a statement about germs of functions, we will work with representative functions defined in some neighborhood of $m$.

    If $f \in \mathcal{M}$, we write $f = f_1 + f_2$, where $f(x_1, \ldots, x_2) = f(0, x_2, \ldots, x_n)$ and $f_2 = f - f_1$.
    Then $f_2 \in x_1\mathcal{O}$ by Lemma~\ref{lem6.1}, while $f_2$ is the germ of a function in $x_2, \ldots, x_n$ vanishing at $m$ and lies in $x_2\mathcal{O} + \cdots + x_n\mathcal{O}$ by induction on $n$.

    As for the last assertion, if $f \in \mathcal{M}$, let $a_i = \pdv{f}{x_i}(m)$.
    Then $f - \sum_{i}a_ix_i$ vanishes to order 2 at $m$.
    We need to show that it lies in $\mathcal{M}^2$.
    Thus, what we must prove is that if $f$ and $\pdv{f}{x_i}$ vanish at $m$, then $f$ is in $\mathcal{M}^2$.
    To prove this, write $f = f_1 + f_2 + f_3$, where
    % UnnumberedGather
    \begin{gather*}
        f_1(x_1, x_2, \ldots, x_n) = f(x_1, \ldots, x_n) - f(0, x_2, \ldots, x_n) - x_1 \pdv{f}{x_1}(0, x_2, \ldots, x_n), \\
        f_2(x_1, \ldots, x_n) = f(0, x_2, \ldots, x_n), \\
        f_3(x_1, x_2, \ldots, x_n) = x_1 \pdv{f}{x_1}(0, x_2, \ldots, x_n).
    \end{gather*}
    Two applications, of Lemma~\ref{lem6.1} show that $f_1 = x_1^{-2}h$ where $h$ is smooth, so $f_1 \in \mathcal{M}^2$.
    The function $f_2$ also vanishes, with its first-order partial derivatives at $m$, but is a function in one fewer variables, so by induction it is in $\mathcal{M}^2$.
    Lastly, $\pdv{f}{x_1}$ vanishes at $m$ and hence is in $\mathcal{M}$ by the part of this proposition that is already proved, so multiplying by $x_1$ gives an element of $\mathcal{M}^2$.
\end{proof}%}T}E}X

A \emph{local derivation} of $\mathcal{O}_m$ is a map $X : \mathcal{O}_m \to \mathbb{R}$ this is $\mathbb{R}$-linear and such that
% Equation
\begin{equation}\label{eqlocalderivation}
    X(fg) = f(m)X(g) + g(m)X(f).
\end{equation}
Taking $f = g = 1$ gives $X(1 \cdot 1) = 2X(1)$ so $X$ annihilates constant functions.

For example, if $x_1, \ldots, x_n$ are a set of local coordinates and $a_1, \ldots, a_n \in \mathbb{R}$, then
% Equation
\begin{equation}\label{eqexlocalderivation}
    Xf = \sum_{i = 1}^{n} a_i \pdv{f}{x_1}(m)
\end{equation}
is a local derivation.

% Proposition
\begin{proposition}\label{prop6.2}%{T{E{X
    Let $m$ be a point on an $n$-dimensional smooth manifold $M$.
    Every local derivation of $\mathcal{O}_m$ is of the form~\eqref{eqexlocalderivation}.
    The set $T_m(M)$ of such local derivations is an $n$-dimensional real vector space.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    If $f$ and $g$ both vanish at $m$, then~\eqref{eqlocalderivation} implies that a local derivation $X$ vanishes on $\mathcal{M}^2$, and by Proposition~\ref{prop6.1} it is therefore determined by its values on $x_1, \ldots, x_n$.
    If these are $a_1, \ldots, a_n$, then $X$ agrees with the right-hand side of~\eqref{eqexlocalderivation}.
\end{proof}%}T}E}X

We now define \emph{tangent space} $T_m(M)$ to be the space of local derivations of $\mathcal{O}_m$.
We will call elements of $T_m(M)$ \emph{tangent vectors}.
Thus, a tangent vector at $m$ is the same thing as a local derivation of the ring $\mathcal{O}_m$.

This definition of tangent vector and tangent space has the advantage that it is intrinsic.
Proposition~\ref{prop6.2} allows us to relate this definition to the intuitive notion of a tangent vector.
Intuitively, a tangent vector should be an equivalence class of paths through $m$: two paths are equivalent if they are tangent.
By a path we mean a smooth map $u : (-\varepsilon, \varepsilon) \to M$ such that $u(0) = m$ for some $\varepsilon > 0$.
Given a function, or the germ of a function at $m$, we can use the path to define a local derivation
% Equation
\begin{equation}\label{eqlocalderivationofgerm}
    Xf = \dv{t}f(u(t))\Big|_{t = 0}.
\end{equation}
Using the chain rule, this equals~\eqref{eqexlocalderivation} with $a_i = \dv{t}(x_i(u(t)))\Big|_{t = 0}$.

We will denote the element~\eqref{eqexlocalderivation} of $T_m(M)$ by the notation
\[
    X = \sum_{i = 1}^{n} a_i \dv{x_i}.
\]
By a \emph{vector field} $X$ on $M$ we mean a rule that assigns to each point $m \in M$ an element $X_n \in T_m(M)$.
The assignment $m \to X_m$ must be smooth.
This means that if $x_1, \ldots, x_n$ are local coordinates on an open set $U \subseteq M$, then there exist smooth functions $a_1, \ldots, a_n$ on $U$ such that
% Equation
\begin{equation}\label{eqvectorfield}
    X_m = \sum_{i = 1}^{n} a_i(m) \dv{x_i}.
\end{equation}
It follows from the chain rule that this definition is independent of the choice of local coordinates $x_i$.

Now let $A = C^\infty(M, \mathbb{R})$ be the ring of smooth real-valued functions on $M$.
Given a vector field $X$ on $M$, we may obtain a derivation of $A$ as follows.
If $f \in A$, let $X(f)$ be the smooth function that assigns to $m \in M$ the value $X_m(f)$, where we are of course applying $X_m$ to the germ of $f$ at $m$.
For example, $M = U$ is an open set on $\mathbb{R}^{n}$ with coordinate functions $x_1, \ldots, x_n$ on $U$, given smooth functions $a_i : U \to \mathbb{R}$, we may associate a derivation of $A$ with the vector field~\eqref{eqvectorfield} by
% Equation
\begin{equation}\label{eqderivationassociatedwithvectorfield}
    (Xf)(m) = \sum_{i = 1}^{m} a_i(m) \dv{f}{x_i}(m).
\end{equation}
The content of the next theorem is that every derivation of $A$ is associated with a vector field in this way.

% Proposition
\begin{proposition}\label{prop6.3}%{T{E{X
    There is a one-to-one correspondence between vector fields on a smooth manifold $M$ and derivations of $C^\infty(M, \mathbb{R})$.
    Specifically, if $D$ is any derivation of $C^\infty(M, \mathbb{R})$, there is a unique vector field $X$ on $M$ such that $Df = Xf$ for all $f$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    We show first that if $m \in M$, and if $f \in A = C^\infty(M, \mathbb{R})$ has germ zero at $m$, then the function $Df$ vanishes at $m$.
    This implies that $D$ induces a well-defined map $X_m : \mathcal{O}_m \to \mathbb{R}$ that is a local derivation.
    Our assumption means that $f$ vanishes in a neighborhood of $m$, so there is another smooth function $g$ such that $gf = f$, yet $g(m) = 0$.
    Now $D(f)(m) = g(m)D(f) + f(m)D(g)$.
    Since both $f$ and $g$ vanish at $m$, we see that $D(f)(m) = 0$.

    Now let $x_i$ be local coordinates on an open set $U$ of $M$.
    For each $m \in U$ there are real numbers $a_i(m)$ such that~\eqref{eqvectorfield} is true.
    We need to know that the $a_i(m)$ are smooth functions.
    Indeed, we have $a_i(m) = D(x_i)$, so it is smooth.
\end{proof}%}T}E}X

Now let $X$ and $Y$ be vector fields on $M$.
By Proposition~\ref{prop6.3}, we may regard these as derivations of $C^\infty(M, \mathbb{R})$.
As we have noted in Example 5.7, derivations of an arbitrary ring form a Lie algebra.
Thus $[X, Y] = XY - YX$ defines a derivation:
% Equation
\begin{equation}\label{eqderivation}
    [X, Y]f = X(Yf) - Y(Xf).
\end{equation}
By Proposition~\ref{propintofcharanddimoffixedpts} this derivation $[X, Y]$ corresponds to a vector field.
Let us see this again concretely by computing its effect in local coordinates.
If $X = \sum a_i \pdv{x_i}$ and $Y = \sum b_i \pdv{x_i}$, we have $X(Yf) = \sum_{i, j} {\left[a_j \pdv{b_i}{x_j} \pdv{f}{x_i} + a_ib_j \pdv[2]{f}{x_i}{x_j}\right]}$.
This is not a derivation, but if we subtract $Y(Xf)$ to cancel the unwanted mixed partials, we see that
\[
    [X, Y] = \sum_{i, j} {\left[a_j \pdv{b_i}{x_j} - b_j \pdv{a_i}{x_j}\right]}\pdv{x_i}.
\]

% UnnumberedSection
\section*{EXERCISES}%{T{E{X
The following exercise requires some knowledge of topology.

% Exercise
\begin{exercise}\label{}
    Let $X$ be a vector field on the sphere $S^k$.
    If $X_m \neq 0$ for all $m \in S^k$, show that the \emph{antipodal map} $a : S^k \to S^k$ and the identity map $S^k \to S^k$ are homotopic.
    Show that this implies that $k$ is odd.
    (\textbf{Hint:} Normalize the vector field so that $X_m$ is a unit tangent vector for all $m$.
    If $m \in S^k$ consider the great circle $\theta_m : [0, 2\pi] \to S^k$ tangent to $X_m$.
    Then $\theta_m(0) = \theta_m(2\pi) = m$, but $m \mapsto \theta_m(\pi)$ is the antipodal map.)
\end{exercise}

%}T}E}X

%}T}E}X
% Chapter
\chapter{Left-Invariant Vector Fields}\label{}%{T{E{X
To recapitulate, a \emph{Lie group} is a differentiable manifold with a group structure in which the multiplication and inversion maps $G \times G \to G$ and $G \to G$ are smooth.
A homomorphism of Lie groups is a group homomorphism that is also smooth map.

% Remark
\begin{remark}\label{rem7.1}%{T{E{X
    There is a subtlety in the definition of a Lie subgroup.
    A \emph{Lie subgroup} is best defined as a Lie group $H$ with an injective homomorphism $i : H \to G$.
    With this definition, the image of $i$ in $G$ is not closed, however, as the following example shows.
    Let $G$ be $\mathbb{T} \times \mathbb{T}$, where $\mathbb{T}$ is the circle $\mathbb{R}/\mathbb{Z}$.
    Let $H$ be $\mathbb{R}$, and let $i : H \to G$ be the map $i(t) = (\alpha t, \beta t) \mod 1$, where the ratio $\alpha/\beta$ is irrational.
    This is a Lie subgroup, but the image of $H$ is not closed.
    To require a closed image in the definition of a Lie subgroup would invalidate a theorem of Chevalley that subalgebras of the Lie algebra of a Lie group correspond to Lie subgroups.
    If we wish to exclude this type of example, we will explicitly describe a Lie subgroup of $G$ as a \emph{closed} Lie subgroup.
\end{remark}%}T}E}X
% Remark
\begin{remark}\label{rem7.2}%{T{E{X
    On the other hand, in the expression ``closed Lie subgroup'', the term ``Lie'' is redundant.
    It may be shown that a closed subgroup of a Lie group is a submanifold and hence a Lie group.
    See Br\"{o}cker and Tom Dieck~\ref{}, Theorem 3.11 on p.\ 28; Knapp~\ref{} Chapter I Section 4; or Knapp~\ref{}, Theorem 1.5 on p.\ 20.
    We will only prove this for the special case of an Abelian subgroup in Theorem 15.2 below.

    Let $G$ be a Lie group.
    If $g \in G$, then $L_g : G \to G$ defined by $L_g(h) = gh$ is a diffeomorphism and hence induces maps $L_{g, *} : T_h(G) \to T_{gh}(G)$.
    A vector field $X$ on $G$ is \emph{left-invariant} if $L_{g, *}(X_h) = X_{gh}$.
\end{remark}%}T}E}X

% Proposition
\begin{proposition}\label{prop7.1}%{T{E{X
    The vector space of left-invariant vector fields is closed under $[\cdot, \cdot]$ and is a Lie algebra of dimension $\dim(G)$.
    If $X_e \in T_e(G)$, there is a unique left-invariant vector field $X$ on $G$ with the prescribed tangent vector at the identity.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Given a tangent vector $X_e$ at the identity element $e$ of $G$, we may define a left-invariant vector field by $X_g = L_{g, *}(X_e)$, and conversely any left-invariant vector field must satisfy this identity, so the space of left-invariant vector fields in isomorphic to the tangent space of $G$ at the identity.
    Therefore, its vector space dimension equals the dimension of $G$.
\end{proof}%}T}E}X

Let $\operatorname{Lie}(G)$ be the vector space of left-invariant vector fields, which we may identify with the $T_e(G)$.
It is clearly closed under $[\cdot, \cdot ]$.
Suppose now that $G = \mathrm{GL}(n, \mathbb{C})$.
We have defined two different Lie algebras for $G$: first, $\mathrm{M}(n, \mathbb{C})$ with the commutation relation $[X, Y] = XY - YX$ (matrix multiplication); and second, left-invariant vector fields.
The first Lie algebra is an $n^2$-dimensional complex Lie algebra, which we may regard as a $2n^2$-dimensional real Lie algebra that happens to have a complex structure.
The second Lie algebra is a $2n^2$-dimensional vector space by Proposition~\ref{prop7.1} because $G$ is an open set in $\mathrm{M}(n, \mathbb{C}) = \mathbb{C}^{n^2} \cong \mathbb{R}^{2n^2}$ and hence dimension $2n^2$ as a manifold.
We want to see that they are the same.

If $X \in \mathrm{M}(n, \mathbb{C})$, we begin by associating with $X$ a left-invariant vector field.
Since $G$ is an open subset of the real vector space $V = \mathrm{M}(n, \mathbb{C})$, we may identify the tangent space to $G$ at the identity with $V$.
With this identification, an element $X \in V$ is the local derivation at $I$ (see~\eqref{eqlocalderivationofgerm}) defined by
\[
    f \mapsto \dv{t}f(I + tX)\Big|_{t = 0},
\]
where $f$ is the germ of a smooth function at $I$.
The two paths $t \to I + tX$ and $t \to \exp{tX} = I + tX + \cdots $ are tangent when $t = 0$, so this is the same as
\[
    f \to \dv{t}f(\exp(tX))\Big|_{t = 0},
\]
which is a better definition.
Indeed, if $H$ is Lie subgroup of $\mathrm{GL}(n, \mathbb{C})$ and $X$ is in the Lie algebra of $H$, then by Proposition~\ref{prop5.2}, the second path $\exp(tX)$ stays within $H$, so this definition still makes sense.

It is clear how to extrapolate this local derivation to a left-invariant global derivation of $C^\infty(G, \mathbb{R})$.
We must define
% Equation
\begin{equation}\label{eq7.1}
    (\d{X})f(g) = \dv{t}f(g\exp(tX))\Big|_{t = 0}.
\end{equation}
By Proposition~\ref{propintofcharanddimoffixedpts}, the left-invariant derivation $\d{X}$ of $C^\infty(G, \mathbb{R})$ corresponds to a left-invariant vector field.
To distinguish this derivation from the element $X$ of $\mathrm{M}(n, \mathbb{C})$, we will resist the temptation to denote this derivation also as $X$ and denote it by $\d{X}$.

% Lemma
\begin{lemma}\label{lem7.1}%{T{E{X
    let $f$ be a smooth map from a neighborhood of the origin in $\mathbb{R}^{n}$ into a finite-dimensional vector space.
    We may write
    % Equation
    \begin{equation}\label{eq7.2}
        f(x) = c_0 + c_1(x) + B(x, x) + r(x),
    \end{equation}
    where $c_1 : \mathbb{R}^{n} \to V$ is linear, $B : \mathbb{R}^{n} \times \mathbb{R}^{n} \to V$ is symmetric and bilinear, and $r$ vanishes to order 3.
\end{lemma}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    This is just the familiar Taylor expansion.
    Denoting $u = (u_1, \ldots, u_n)$, let $c_0 = f(0)$,
    \[
        c_1(u) = \sum_{i} \pdv{f}{x_i}(0)u_i,
    \]
    and
    \[
        B(u, v) = \frac{1}{2} \sum_{i, j} \pdv[2]{f}{x_i}{x_j}(0)u_iv_j.
    \]
    Both $f(x)$ and $c_0 + c_1(x) + B(x, x)$ have the same partial derivatives of order $\leq 2$, so the difference $r(x)$ vanishes to order 3.
    The fact that $B$ is symmetric follows from the equality of mixed partials:
    \[
        \pdv[2]{f}{x_i}{x_j}(0) = \pdv[2]{f}{x_j}{x_i}(0).
    \]
\end{proof}%}T}E}X

% Proposition
\begin{proposition}\label{prop7.2}%{T{E{X
    If $X, Y \in \mathrm{M}(n, \mathbb{C})$, and if $f$ is a smooth function on $G = \mathrm{GL}(n, \mathbb{C})$, then $\d{[X, Y]}f = \d X(\d{Y}f) - \d Y(\d{X}f)$.
\end{proposition}%}T}E}X

Here $[X, Y]$ means $XY - YX$; that is, the bracket computed as in Chapter 5.
The content of this proposition is that this definition is consistent with the bracket in Chapter 5.
% Proof
\begin{proof}%{T{E{X
    We fix a function $f \in C^\infty(G)$ and an element $g \in G$.
    By Lemma~\ref{lem7.1}, we may write, for $X$ near 0,
    \[
        f(g(I + X)) = c_0 + c_1(X) + B(X, X) + r(X),
    \]
    where $c_1$ is linear in $X$, $B$ is symmetric and bilinear, and $r$ vanishes to order 3 at $X = 0$.
    We will show that
    % Equation
    \begin{equation}\label{eq7.3}
        (\d{X}f)(g) = c_1(X)
    \end{equation}
    and
    % Equation
    \begin{equation}\label{eq7.4}
        (\d{X} \circ \d{Y} f)(g) = c_1(XY) + 2B(X, Y).
    \end{equation}
    Indeed,
    % UnnumberedAlign
    \begin{align*}
        (\d{X}f)(g) &= \dv{t}f(g(I + tX)) \Big|_{t = 0} \\
        &= \dv{t}(c_0 + c_1(tX) + B(tX, tX) + r(tX))\Big|_{t = 0}.
    \end{align*}
    We may ignore the $B$ and the $r$ terms because they vanish to order $\geq 2$, and since $c_1$ is linear, this is just $c_1(X)$ proving~\eqref{eq7.3}.
    Also
    % UnnumberedAlign
    \begin{align*}
        (\d{X} \circ \d{Y}f)(g) &= \dv{t}((\d{Y}f)(g(I + tX)))\Big|_{t = 0} \\
        &= \pdv{t} \pdv{u} f(g(I + tX)(I + uY)) \Big|_{t = u = 0} \\
        &= \pdv{t} \pdv{u}\Big[(c_0 + c_1(tX + uY + tuXY) \\
        &+ B(tX + uY + tuXY, tX + uY + tuXY) + r(tX + uY + tuXY)\Big]\Big|_{t = u = 0}.
    \end{align*}
    We may omit $r$ from this computation since it vanishes to third order.
    Expanding the linear and bilinear maps $c_1$ and $B$, we obtain~\eqref{eq7.4}.

    Similarly,
    \[
        (\d{Y} \circ \d{X}f)(g) = c_1(YX) + 2B(X, Y).
    \]
    Subtracting this from~\eqref{eq7.4} to kill the unwanted $B$ term, we obtain
    \[
        ((\d{X} \circ \d{Y} - \d{Y} \circ \d{X})f)(g) = c_1(XY - YX) = (\d{[X, Y]}f)(g)
    \]
    by~\eqref{eq7.3}.
\end{proof}%}T}E}X

This Proposition shows that if $X \in \mathrm{M}(n, \mathbb{C})$, and if we associate with X a derivation of $C^\infty(G, \mathbb{R})$, where $G = \mathrm{GL}(n, \mathbb{C})$, using the formula~\eqref{eq7.1}, then the two brackets give the same result.

Suppose that $M$ and $N$ are smooth manifolds and $\varphi : M \to N$ is a smooth map.
If $m \in M$ and $n = \varphi(m)$, we get a map $\varphi_* : T_m(M) \to T_n(N)$.
Indeed, if $\mathcal{O}_m$ and $\mathcal{O}_n$ are the local rings, composition with $\varphi$ gives a homomorphism $\mathcal{O}_n \to \mathcal{O}_m$, so if $D$ is a local derivation of $\mathcal{O}_m$, then $\mathcal{O}_n \ni f \mapsto D(f \circ \varphi)$ is a local derivation of $T_n(N)$.

If $\varphi$ is a diffeomorphism of $M$ onto $N$, then we can push a vector field $X$ on $M$ forward this way to obtain a vector field on $N$.
However, if $\varphi$ is \emph{not} a diffeomorphism, this doesn't work because some points in $N$ may not even be in the image of $\varphi$, while others may be in the image of two different points $m_1$ and $m_2$ with no guarantee that $\varphi_*X_{m_1} = \varphi_*X_{m_2}$.

Nevertheless, if $\varphi : G \to H$ is a homomorphism of Lie groups, there is an induced map of Lie algebras, as we will now explain.
Let $X$ be a left-invariant vector field on $G$.
We have induced a map $\varphi_* : T_e(G) \to T_e(H)$, and by Proposition~\ref{prop7.1} applied to $H$ there is a unique left-invariant vector field $Y$ on $H$ such that $\varphi_*X_e = Y_e$.
We regard $Y$ as an element of $\Lie(H)$, and $X \mapsto Y$ is a map $\Lie(G) \to \Lie(H)$, which we denote $\Lie(\varphi)$.

A map $f : \mathfrak{g} \to \mathfrak{h}$ of Lie algebras is naturally called a \emph{homomorphism} if $f([X, Y]) = [f(X), f(Y)]$.

% Proposition
\begin{proposition}\label{prop7.3}%{T{E{X
    If $\varphi : G \to H$ is a Lie group homomorphism, then $\Lie(\varphi) : \Lie(G) \to \Lie(H)$ is a Lie algebra homomorphism.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    If $X, Y \in G$, then $X_e$ and $Y_e$ are local derivations of $\mathcal{O}_e(G)$, and it is clear from the definitions that $\varphi_*([X_e, Y_e]) = [\varphi_*(X_e), \varphi_*(Y_e)]$.
    Consequently, $[\Lie(\varphi)X, \Lie(\varphi)Y]$ and $\Lie(\varphi)([X, Y])$ are left-invariant vector fields on $H$ that agree at the identity, so they are the same by Proposition~\ref{prop7.1}.
\end{proof}%}T}E}X

The Lie algebra homomorphism $\Lie(\varphi)$ is called the \emph{differential} of $\varphi$.

We may ask to what extent the Lie algebra homomorphism $\Lie(\varphi)$ contains complete information about $\varphi$.
For example, given Lie groups $G$ and $H$ with Lie algebras $\mathfrak{g}$ and $\mathfrak{h}$, and a homomorphism $f : \mathfrak{g} \to \mathfrak{h}$, is there a homomorphism $G \to H$ with $\Lie(\varphi) = f$?

In general, the answer is no, as the following example will show.

% Example
\begin{example}\label{ex7.1}
    Let $H = \mathrm{SU}(2)$ and let $G = \mathrm{SO}(3)$.
    $H$ acts on the three dimensional space $V$ of Hermitian matrices $\xi = \begin{pmatrix}
        x & y + iz  \\
        y - iz & -x
    \end{pmatrix}$ of trace zero by $h : \xi \mapsto h\xi h^{-1} = h\xi \overline{h^t}$, and
    \[
        \xi \mapsto -\det(\xi) = x^2 + y^2 + z^2
    \]
    is an invariant positive definite quadratic form on $V$ invariant under this action.
    Thus, the transformation $\xi \mapsto h\xi h^{-1}$ of $V$ is orthogonal, and we have a homomorphism $\psi : \mathrm{SU}(2) \to \mathrm{SO}(3)$.
    Both groups are three-dimensional, and $\psi$ is a local homeomorphism at the identity.
    The differential $\Lie(\psi) : \mathfrak{su}(2) \to \mathfrak{so}(3)$ is therefore an isomorphism and has an inverse, which is a Lie algebra homomorphism $\mathfrak{so}(3) \to \mathfrak{su}(2)$.
    However, $\psi$ itself does not have an inverse since it has a nontrivial element in its kernel, $-I$.
    Therefore, $\Lie(\psi)^{-1} : \mathfrak{so}(3) \to \mathfrak{su}(2)$ is an example of a Lie algebra homomorphism that does not correspond to a Lie group homomorphism $\mathrm{SO}(3) \to \mathrm{SU}(2)$.
\end{example}

Nevertheless, we will see later (Proposition~\ref{prop14.2}) that if $G$ is \emph{simply-connected}, then any Lie algebra homomorphism $\mathfrak{g} \to \mathfrak{h}$ corresponds to a Lie group homomorphism $G \to H$.
Thus, the obstruction to lifting the Lie algebra homomorphism $\mathfrak{so}(3) \to \mathfrak{su}(2)$ to a Lie group homomorphism is topological and corresponds to the fact that $\mathrm{SO}(3)$ is not simply-connected.

% UnnumberedSection
\section*{EXERCISES}%{T{E{X
% Exercise
\begin{exercise}\label{}
    Compute the Lie algebra homomorphism $\Lie(\psi) : \mathfrak{su}(2) \to \mathfrak{so}(3)$ of Example~\ref{ex7.1} explicitly.
\end{exercise}

% Exercise
\begin{exercise}\label{}
    Show that no Lie group can be homeomorphic to the sphere $S^k$ if $k$ is even.
    On the other hand, show that $\mathrm{SU}(2) \cong S^3$.
\end{exercise}

%}T}E}X

%}T}E}X
% Chapter
\chapter{The Exponential Map}\label{}%{T{E{X
The exponential map, introduced for closed Lie subgroups of $\mathrm{GL}(n, \mathbb{C})$ in Chapter 5, can be defined for a general Lie group $G$ as a map $\Lie(G) \to G$.

We may consider a vector field~\eqref{eqderivationassociatedwithvectorfield} that is allowed to vary smoothly.
By this we mean that we introduce a real parameter $\lambda \in (-\varepsilon, \varepsilon)$ for some $\varepsilon > 0$ and smooth functions $a_i : M \times (-\varepsilon, \varepsilon) \to \mathbb{C}$ and consider a vector field, which in local coordinates is given by
% Equation
\begin{equation}\label{eq8.1}
    (Xf)(m) = \sum_{i = 1}^{n} a_i(m, \lambda) \pdv{f}{x_i}(m).
\end{equation}

% Proposition
\begin{proposition}\label{prop8.1}%{T{E{X
    Suppose that $M$ is a smooth manifold, $m \in M$, and $X$ is a vector field on $M$.
    Then, for sufficiently small $\varepsilon > 0$, there exists a path $p : (-\varepsilon, \varepsilon) \to M$ such that $p(0) = m$ and $p_*(\dv{t})(t) = X_{p(t)}$ for $t \in (-\varepsilon, \varepsilon)$.
    Such a curve, on whatever interval it is defined, is uniquely determined.
    If the vector field $X$ is allowed to depend on a parameter $\lambda$ as in~\eqref{eq8.1}, then for small values of $t$, $p(t)$ depends smoothly on $\lambda$.
\end{proposition}%}T}E}X

Here we are regarding the interval $(;-\varepsilon, \varepsilon)$ as a manifold, and $p_*(\dv{t})$ is the image of the tangent vector $\dv{t}$.
We call such a curve an \emph{integral curve} for the vector field.
% Proof
\begin{proof}%{T{E{X
    In terms of local coordinates $x_1, \ldots, x_n$ on $M$, the vector field $X$ is
    \[
        \sum a_i(x_1, \ldots, x_n) \pdv{x_i},
    \]
    where the $a_i$ are smooth functions in the coordinates neighborhood.
    If a path $p(t)$ is specified, let us write $x_i(t)$ for the $x_i$ component of $p(t)$, with the coordinates of $m$ being $x_1 = \cdots = x_n = 0$.
    Applying the tangent vector $p_*(\dv{t})(t)$ to a function $f \in C^\infty(G)$ gives
    \[
        \dv{t}f(x_1(t), \ldots, x_n(t)) = \sum x_i'(t) \pdv{f}{x_i}(x_1(t), \ldots, x_n(t)).
    \]
    On the other hand, applying $X_{p(t)}$ to the same $f$ gives
    \[
        \sum_{i} a_i(x_1(t), \ldots, x_n(t)) \pdv{f}{x_i}(x_1(t), \ldots, x_n(t)),
    \]
    so we need a solution to the first-order system
    \[
        x_i'(t) = a_i(x_1(t), \ldots, x_n(t)), \qquad x_i(0) = 0, \qquad (i = 1, \ldots, n).
    \]
    The existence of such a solution for sufficiently small $|t|$, and its uniqueness on whatever interval it does exist, is guaranteed by a standard result in the theory of ordinary differential equations, which may be found in most texts.
    See, for example, Ince, Chapter 3, particularly Section 3.3, for a rigorous treatment.
    The required Lipschitz condition follows from smoothness of the $a_i$'s.
    For the statement about continuously varying vector fields, one needs to know that corresponding fact about first-order systems, which is discussed in Section 3.31 of Ince.
    Here Ince imposes an assumption of analyticity on the dependence of the differential equation on $\lambda$, which he allows to be a complex parameter, because he wants to conclude analyticity of the solutions; if one weakens this assumption of analyticity to smoothness, one still gets smoothness of the solution.
\end{proof}%}T}E}X

In general, the existence of the integral curve of a vector field is only guaranteed in a small segment $(-\varepsilon, \varepsilon)$, as in Proposition~\ref{prop8.1}.
However, we will now see that, for left-invariant vector fields on a Lie group, the integral curve extends to all $\mathbb{R}$.
This fact underlies the construction of the exponential map.

% Theorem
\begin{theorem}\label{thm8.1}%{T{E{X
    Let $G$ be a Lie group and $\mathfrak{g}$ its Lie algebra.
    There exists a map $\exp : \mathfrak{g} \to G$ this is a local homeomorphism in a neighborhood of the origin in $\mathfrak{g}$ such that, for any $X \in \mathfrak{g}, t \to \exp(tX)$ is an integral curve for the left-invariant vector field $X$
    Moreover, $\exp((t + u)X) = \exp(tX) \exp(uX)$.
\end{theorem}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Let $X \in \mathfrak{g}$.
    We know that for sufficiently small $\varepsilon > 0$ there exists an integral curve $p : (-\varepsilon, \varepsilon) \to G$ for the left-invariant vector field $X$ with $p(0) = 1$.
    We show first that if $p : (a, b) \to G$ is any integral curve for an open interval $(a, b)$ containing 0, then
    % Equation
    \begin{equation}\label{eq8.2}
        p(s)p(t) = p(s + t) \ \text{when} \ s, t, s+t \in (a, b).
    \end{equation}
    Indeed, since $X$ is invariant under left-translation, left-translation by $p(s)$ takes an integral curve for the vector field into another integral curve.
    Thus $t \to p(s)p(t)$ and $t \to p(s + t)$ are both integral curves, with the same initial condition $0 \to p(s)$.
    They are thus the same.

    With this in mind, we show next that if $p : (-a, a) \to G$ is an integral curve for the left-invariant vector field $X$, then we may extend it to all of $\mathbb{R}$.
    Of course, it is sufficient to show that we may extend it to ${\left(-\frac{3}{2}a, \frac{3}{2}a\right)}$.
    We extend it by the rule $p(t) = p(a/2) p(t - a/2)$ when $-a/2 \leq t \leq 3a/2$ and $p(t) = p(-a/2) p(t + a/2)$ when $-3a/2 \leq t \leq a/2$, and it follows from~\eqref{eq8.2} that this definition is consistent on regions of overlap.

    Now define $\exp : \mathfrak{g} \to G$ as follows.
    Let $X \in \mathfrak{g}$, and let $p : \mathbb{R} \to G$ be an $\exp(X) = p(1)$.
    We note that if $u \in \mathbb{R}$, then $t \mapsto p(tu)$ is an integral curve for $uX$, so $\exp(X) = p(u)$.

    The exponential map is a smooth map, at least for $X$ near the origin in $\mathfrak{g}$, by the last statement in Proposition~\ref{prop8.1}.
    Identifying the tangent space at the origin in the vector space $\mathfrak{g}$ with $\mathfrak{g}$ itself, $\exp$ induces a map $T_0(\mathfrak{g}) \to T_e(G)$ (that is $\mathfrak{g} \to \mathfrak{g}$), and this map is the identity map by construction.
    Thus the Jacobian of $\exp$ is nonzero and, by the Inverse Function Theorem, $\exp$ is a local homeomorphism near 0.
\end{proof}%}T}E}X

We also denote $\exp(X)$ as $e^X$ for $X \in \mathfrak{g}$.

% Remark
\begin{remark}\label{}%{T{E{X
    If $G = \mathrm{GL}(n, \mathbb{C})$, then as we explained in Chapter 7, Proposition~\ref{prop7.2} allows us to identify the Lie algebra of $G$ with $\mathrm{M}(n, \mathbb{C})$.
    We observe that the definition of $\exp : \mathrm{M}(n, \mathbb{C}) \to \mathrm{GL}(n, \mathbb{C})$ by a series in~\eqref{eqexc5.2} agrees with the definition in Theorem~\ref{thm8.1}.
    This is because $t \mapsto \exp(tX)$ with either definition is an integral curve for the same left-invariant vector field, and the uniqueness of such an integral curve follows from Proposition~\ref{prop8.1}.
\end{remark}%}T}E}X

A \emph{representation} of a Lie algebra $\mathfrak{g}$ over a field $F$ is a Lie algebra homomorphism $\rho : \mathfrak{g} \to \End(V)$, where $V$ is an $F$-vector space, or more generally a vector space over a field $E$ containing $F$, and $\End(V)$ is given then Lie algebra structure that it inherits from its structure as an associative algebra.
Thus
\[
    \rho([x, y]) = \rho(x)\rho(y) - \rho(y)\rho(x).
\]
We may sometimes find it convenient to denote $\rho(x)v$ as just $xv$ for $x \in \mathfrak{g}$ and $v \in V$.
We may think of $(x, v) \mapsto xv = \pi(x)v$ as a multiplication.
If $V$ is a vector space, given a map $\mathfrak{g} \times V \to V$ denoted $(x, v) \mapsto xv$ such that $v \to xv$, then we call $V$ a $\mathfrak{g}$-\emph{module}.
A \emph{homomorphism} $\varphi : U \to V$ of $\mathfrak{g}$-modules is an $F$-linear map satisfying $\varphi(xv) = x\varphi(v)$.

% Example
\begin{example}\label{}
    If $\varphi : G \to \mathrm{GL}(V)$ is a representation, where $V$ is a real or complex vector space, then the Lie algebra of $\mathrm{GL}(V)$ is $\End(V)$, so the differential $\Lie(\varphi) : \Lie(G) \to \End(V)$, defined by Proposition~\ref{prop7.3}, is a Lie algebra representation.
\end{example}

By the universal property of $U(\mathfrak{g})$ in Theorem 10.1, a Lie algebra representation $\rho : \mathfrak{g} \to \End(V)$ extends to a ring homomorphism $U(\mathfrak{g}) \to \End(V)$ extends to a ring homomorphism $U(\mathfrak{g}) \to \End(V)$, which we continue to denote as $\rho$.

If $\mathfrak{g}$ is a Lie algebra over a field $F$, we get a homomorphism $\ad : \mathfrak{g} \to \End(\mathfrak{g})$, called the \emph{adjoint map}, defined by $\ad(x)(y) = [x, y]$.
We give $\End(\mathfrak{g})$ the Lie algebra structure it inherits as an associative ring.
We have
% Equation
\begin{equation}\label{eq8.3}
    \ad(x)([y, z]) = [\ad(x)(y), z] + [y, \ad(x)(z)]
\end{equation}
since, by the Jacobi identity, both sides equal $[x, [y, z]] = [[x, y], z] + [y, [x, z]]$.
This means that $\ad(x)$ is a derivation of $\mathfrak{g}$.

Also
% Equation
\begin{equation}\label{eq8.4}
    \ad(x)\ad(y) - \ad(x)\ad(x) = \ad([x, y])
\end{equation}
since applying either side to $z \in \mathfrak{g}$ gives $[x, [y, z]] - [y, [x, z]] = [[x, y], z]$ by the Jacobi identity.
So $\ad : \mathfrak{g} \to \End(\mathfrak{g})$ is a Lie algebra representation.

We digress to explain the geometric origin of $\ad$.
To begin with, representations of Lie algebras arise naturally from representations of Lie groups.
Suppose that $G$ is a Lie group and $\mathfrak{g}$ is its Lie algebra.
If $V$ is a vector space over $\mathbb{R}$ or $\mathbb{C}$, any Lie group homomorphism $\pi : G \to \mathrm{GL}(V)$ induces a Lie algebra homomorphism $\mathrm{M}(g) \to \End(V)$ by Proposition~\ref{prop7.3}; that is, a real or complex representation.

In particular, $G$ acts on itself by conjugation, and so it acts on $\mathfrak{g} = T_e(G)$.
This representation is called the \emph{adjoin representation} and is denoted $\Ad : G \to \mathrm{GL}(\mathfrak{g})$.
We show next that the differential of $\Ad$ is $\ad$.
That is:

% Proposition
\begin{proposition}\label{prop8.2}%{T{E{X
    Let $G$ be a Lie group, $\mathfrak{g}$ its Lie algebra, and $\Ad : G \to \mathrm{GL}(\mathfrak{g})$ the adjoint representation.
    Then the Lie group representation $\mathfrak{g} \to \End(\mathfrak{g})$ corresponding to $\Ad$ by Proposition~\ref{prop7.3} is $\ad$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    It will be most convenient for us to think of elements of the Lie algebra as tangent vectors at the identity or as local derivations of the local ring there.
    Let $X, Y \in \mathfrak{g}$.
    If $f \in C^\infty(G)$, define $c(g)f(h) = f(g^{-1}hg)$.
    Then our definitions of the adjoint representation amount to
    \[
        (\Ad(g)Y)f = Y(c(g^{-1})f).
    \]
    To compute the differential of $\Ad$, note that the path $t \to \exp(tX)$ in $G$ is tangent to the identity at $t = 0$ with tangent vector $X$.
    Therefore, under the representation of $\mathfrak{g}$ in Proposition~\ref{prop7.3}, $X$ maps $Y$ to the local derivation at the identity
    \[
        f \mapsto \dv{t}(\Ad(e^{tX})Y)f \Big|_{t = 0} = \dv{t}\dv{u}f(e^{tX}e^{uY}e^{-tX})\Big|_{t = u = 0}.
    \]

    By the chain rule, if $F(t_1, t_2)$ is a function of two real variables,
    % Equation
    \begin{equation}\label{eq8.5}
        \dv{t}F(t, t)\Big|_{t = 0} = \pdv{F}{t_1}(0, 0) + \pdv{F}{t_2}(0, 0).
    \end{equation}
    Applying this, with $u$ fixed to $F(t_1, t_2) = f(e^{t_1 X}e^{uY} e^{-t_2 Y})$, our last expression equals
    \[
        \dv{u}\dv{t}f(e^{tX}e^{uY})\Big|_{t = u = 0} - \dv{u}\dv{t}f(e^{uY}e^{tX})\Big|_{t = u = 0} =  XY f(1) - YX f(1).
    \]
    This is of course the same as the effect of $[X, Y] = \ad(X)Y$.
\end{proof}%}T}E}X

%}T}E}X
% Chapter
\chapter{Tensors and Universal Properties}\label{}%{T{E{X
We will review the basic properties of the tensor product and use them to illustrate the basic notion of a \emph{universal property}, which we will see repeatedly.

If $R$ is a commutative ring and $M$, $N$, and $P$ are $R$-modules, then a \emph{bilinear map} $f : M \times N \to P$ is a map satisfying
% UnnumberedGather
\begin{gather*}
    f(r_1m_1 + r_2m_2, n) = r_1f(m_1, n) + r_2f(m_2, n), \qquad r_i \in R, m_i \in M, n \in N,\\
    f(m, r_1n_1 + r_2n_2) = r_1f(m, n_1) + r_2f(m, n_2), \qquad r_i \in R, n_i \in N, m \in M.
\end{gather*}
More generally, if $M_1, \ldots, M_k$ are $R$-modules, the notion of a $k$-\emph{linear map} $M_1 \times \cdots M_k \to P$ is defined similarly: the map must be linear in each variable.

The \emph{tensor product} $M \otimes_R N$ is an $R$-module together with a bilinear map $\otimes : M \times N \to M \otimes_R N$ satisfying the following property.

\textbf{Universal Property of the Tensor Product.} \textit{If $P$ is any $R$-module and $p : M \times N \to P$ is a bilinear map, there exists a unique $R$-module homomorphism $F : M \otimes N \to P$ such that $p = F \circ \otimes$.}

Why do we call this a universal property?
It says that $\otimes : M \times N \to M \otimes N$ is a ``universal'' bilinear map in the sense that an bilinear map of $M \times N$ factors through it.
The module $M \otimes_R N$ is uniquely determined by the universal property.
This important fact is obvious if one thinks of it correctly.
Before we explain this point, let us make a categorical observation.

If $\mathcal{C}$ is a category, and \emph{initial object} in $\mathcal{C}$ is an object $X_0$ such that, for every object $Y$, the $\Hom$ set $\Hom_\mathcal{C}(X_0, Y)$ consists of a single element.
A \emph{terminal object} is an object $X_\infty$ such that, for every object $Y$, the $\Hom$ set $\Hom_\mathcal{C}(Y, X_\infty)$ consists of a single element.
For example, in the category of sets, the empty set is an initial object and a set consisting of one element is a terminal object.

% Lemma
\begin{lemma}\label{lem9.1}%{T{E{X
    In any category, and two initial objects are isomorphic.
    Any two terminal objects are isomorphic.
\end{lemma}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    If $X_0$ and $X_1$ are initial objects, there exist unique morphisms $f : X_0 \to X_1$ (since $X_0$ is initial) and $g : X_1 \to X_0$ (since $X_1$ is initial).
    Then $g \circ f : X_0 \to X_0$ and $1_{X_0} : X_0 \to X_0$ must coincide since $X_0$ is initial, and similarly $f \circ g = 1_{X_1}$.
    Thus $f$ and $g$ are inverse isomorphisms.
    Similarly, terminal objects are isomorphic.
\end{proof}%}T}E}X

% Theorem
\begin{theorem}\label{thm9.1}%{T{E{X
    The tensor product $M \otimes_R N$, if it exists, is determined up to isomorphism by the universal property.
\end{theorem}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Let $\mathcal{C}$ be the following category.
    An object in $\mathcal{C}$ is an ordered pair $(P, p)$, where $P$ is an $R$-module and $p : M \times N \to P$ is a bilinear map.
    If $X = (P, p)$ and $Y = (Q, q)$ are objects, then a morphism $X \to Y$ consists of an $R$-module homomorphism $f : P \to Q$ such that $q = f \circ p$.
    The universal property of the tensor product means that $\otimes : M \times N \to M \otimes N$ is an initial object in this category and therefore determined up to isomorphism.
\end{proof}%}T}E}X

Of course, we usually denote $\otimes(m, n)$ as $m \otimes n$ in $M \otimes_R N$.
We have not proved that $M \otimes_R N$ exists.
We refer to any text on algebra for this fact, such as Lang, Chapter XVI.

In general by a \emph{universal property} we mean \emph{any characterization of a mathematical object that can be expressed by saying that some associated object is an initial or terminal object in some category}.
The basic paradigm is that \emph{a universal property characterizes an object up to isomorphism}.

A typical application of the universal property of the tensor product is to make $M \otimes_R N$ into a functors.
Specifically, if $\mu : M \to M'$ and $\nu : N \to N'$ are $R$-module homomorphisms, then there is a unique $R$-module homomorphism $\mu \otimes \nu : M \otimes_R N \to M' \otimes_R N'$ such that $(\mu \otimes \nu)(m \otimes n) = \mu(m) \otimes \nu(n)$.
We get this by applying the universal property to the $R$-bilinear map $M \times N \to M' \otimes N'$ defined by $(m, n) \mapsto \mu(m) \otimes \nu(n)$.

As another example of an object that can be defined by a universal property, let $V$ be a vector space over a field $F$.
Let us ask for an $F$-algebra $\bigotimes V$ together with an $F$-linear map $i : V \to \bigotimes V$ satisfying the following condition.

\textbf{Universal Property of the Tensor Algebra.} \textit{If $A$ is any $F$-algebra and $\varphi : V \to A$ is an $F$-linear map then there exists a unique $F$-algebra homomorphism $\Phi : \bigotimes V \to A$ such that $\varphi = \Phi \circ i$.}

It should be clear from the previous discussion that this universal property characterizes the tensor algebra up to isomorphism.
To prove existence, we can construct a ring with this exact property as follows.
Let unadorned $\otimes$ mean $\otimes_F$ in what follows.
By $\otimes^k V$ we mean the $k$-fold tensor product $V \otimes \cdots \otimes V$ ($k$ times); if $k = 0$, then it is natural to take $\otimes^0 V = F$ while $\otimes^1 V = V$.
If $V$ has finite dimension $d$, then $\otimes^k V$ has dimension $d^k$.
Let
\[
    \bigotimes V = \bigoplus_{k = 0}^{\infty} (\otimes^k V).
\]
Then $\bigotimes V$ has the natural structure of a graded $F$-algebra in which the multiplication $\otimes^k V \times \otimes^\ell V \to \otimes^{k + \ell} V$ sends
\[
    (v_1 \otimes \cdots \otimes v_k, u_1 \otimes \cdots \otimes u_\ell) \to v_1 \otimes \cdots \otimes v_k \otimes u_1 \otimes \cdots \otimes u_\ell.
\]
We regard $V$ as a subset of $\bigotimes V$ embedded onto $\otimes^1 V = V$.

% Proposition
\begin{proposition}\label{prop9.1}%{T{E{X
    The universal property of the tensor algebra is satisfied.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    If $\varphi : V \to A$ is any linear map of $V$ into an $F$-algebra, define a map $\Phi : \bigotimes V \to A$ by $\Phi(v_1 \otimes \cdots \otimes v_k) = \varphi(v_1) \cdots \varphi(v_k)$ on $\otimes^kV$.
    It is easy to see that $\Phi$ is a ring homomorphism.
    It is unique since $V$ generates $\bigotimes V$ as an $F$-algebra.
\end{proof}%}T}E}X

We will also encounter the \emph{symmetric} and \emph{exterior powers} of a vector space $V$ over the field $F$.
Let $V^k$ denote $V \times \cdots \times V$ ($k$ times).
A $k$-linear map $f : V^k \to U$ into another vector space is called \emph{symmetric} if for any $\sigma \in S_k$ it satisfies $f(v_{\sigma(1)}, \ldots, v_{\sigma(k)}) = f(v_1, \ldots, v_k)$ and \emph{alternating} if $f(v_{\sigma(1)}, \ldots, v_{\sigma(k)}) = \varepsilon(\sigma) f(v_1, \ldots, v_k)$, where $\varepsilon : S_k \to \{\pm 1\}$ is the alternating (sign) character.
The $k$-th symmetric and exterior powers of $V$, denoted $\vee^k V$ and $\wedge^kV$, are $F$-vector spaces, together with $k$-linear maps $\vee : V^k \to \vee^kV$ and $\wedge : V^k \to \wedge^kV$.
The map $\vee$ is symmetric, and the map $\wedge$ is alternating.
We normally denote $\vee(v_1, \ldots, v_k) = v_1 \vee \cdots \vee v_k$ and similarly for $\wedge$.
The following universal properties are required.

\textbf{Universal Properties of the Symmetric and Exterior Powers:} \textit{Let $f : V^k \to u$ be any symmetric (resp.\ alternating) $k$-linear map.
    Then there exists a unique $F$-linear map $\varphi : \vee^k V \to U$ (resp.\ $\wedge^kV \to U$) such that $f = \varphi \circ \vee$ (resp.\ $f = \varphi \circ \wedge$).}

As usual, the symmetric and exterior algebras are characterized up to isomorphism by the universal property.
We may construct $\vee^kV$ as a quotient of $\otimes^kV$, dividing by the subspace $W$ generated by elements of the form $v_1 \otimes \cdots \otimes v_k - v_{\sigma(1)} \otimes \cdots \otimes v_{\sigma(k)}$, with a similar construction for $\wedge^k$.
The universal property of $\vee^kV$ then follows from the universal property of the tensor product.
Indeed, if $f : V^k \to U$ is any symmetric $k$-linear map, then there is induced a linear map $\psi : \otimes^kV \to U$ such that $f = \psi \circ \otimes$.
Since $f$ is symmetric, $\psi$ vanishes on $W$, so $\psi$ induces a map $\vee^kV = \otimes^kV/W \to U$ and the universal property follows.

If $V$ has dimension $d$, then $\vee^kV$ has dimension ${d + k - 1 \choose k}$, for if $x_1, \ldots, x_d$ is a basis of $V$, then $\{x_{i_1} \vee \cdots \vee x_{i_k} | 1 \leq i_1 \leq \cdots \leq i_k \leq d\}$ is a basis for $\vee^kV$.
On the other hand, the exterior power vanishes unless $k \leq d$, in which case it has dimension $d \choose k$.
A basis consists of $\{x_{i_1} \wedge \cdots \wedge x_{i_k} | 1 \leq i_1 < \cdots < i_k \leq d\}$.
The vector spaces $\vee^kV$ may be collected together to make a commutative graded algebra:
\[
    \bigvee V = \bigoplus_{k = 0}^{\infty} \vee^k V.
\]
This is the \emph{symmetric algebra}.
The exterior algebra $\bigwedge V = \bigoplus_k \wedge^kV$ is constructed similarly.
The spaces $\vee^0V$ and $\wedge^0V$ are one-dimensional and it is natural to take $\vee^0V = \wedge^0V = F$.

% UnnumberedSection
\section*{EXERCISES}%{T{E{X
% Exercise
\begin{exercise}\label{}
    Prove that the tensor algebra $\bigotimes V$ is associative.
\end{exercise}

% Exercise
\begin{exercise}\label{}
    Let $V$ be a finite-dimensional vector space over a field $F$ that may be assumed to be infinite.
    Let $\mathcal{P}(V)$ be the ring of polynomial functions on $V$.
    Note that an element of the dual space $V^*$ is a function on V, so regarding this function as a polynomial given an injection $V^* \to \mathcal{P}(V).$
    Show that this linear map extends to a ring isomorphism $\bigvee V^* \to \mathcal{P}(V)$.
\end{exercise}

% Exercise
\begin{exercise}\label{}
    Prove that if $V$ is a vector space, then $V \otimes V \cong (V \wedge V) \oplus (V \vee V)$.
\end{exercise}

% Exercise
\begin{exercise}\label{}
    Use the universal properties of the symmetric and exterior power to show that if $V$ and $W$ are vector spaces, then there are maps $\vee^kf : \vee^kV \to \vee^kW$ and $\wedge^kf : \wedge^kV \to \wedge^kW$ such that
    \[
        \vee^kf(v_1 \vee \cdots \vee v_k) = f(v_1) \vee \cdots \vee f(v_k), \qquad \wedge^kf(v_1 \wedge \cdots \wedge v_k) = f(v_1) \wedge \cdots \wedge f(v_k).
    \]
\end{exercise}

% Exercise
\begin{exercise}\label{}
    Suppose that $V = F^4$.
    Let $f : V \to V$ be the linear transformation with matrix
    \[
        \begin{pmatrix}
            a & & & \\
            & b & & \\
            & & c & \\
            & & & d
        \end{pmatrix}.
    \]
    Compute the trace of the linear transformations $\vee^2f$ and $\wedge^2f$ on $\vee^2V$ and $\wedge^2V$.
\end{exercise}

%}T}E}X

%}T}E}X
% Chapter
\chapter{The Universal Enveloping Algebra}\label{}%{T{E{X
We have seen that elements of the Lie algebra of a Lie group $G$ are derivations of $C^\infty(G)$; that is, differential operator that are left-invariant.
The universal enveloping algebra if the ring of all left-invariant differential operators, including higher-order ones.
There is a purely algebra construction of this ring.

We recall from  Example 5.6 that if $A$ is an associative algebra, then $A$ may regarded as a Lie algebra by the rule $[a, b] = ab - ba$ for $a, b \in A$.
We will denote this Lie algebra by $\Lie(A)$.

% Theorem
\begin{theorem}\label{thm10.1}%{T{E{X
    Let $\mathfrak{g}$ be a Lie algebra over a field $F$.
    There exists an associative $F$-algebra $U(\mathfrak{g})$ with a Lie algebra homomorphism $i : \mathfrak{g} \to \Lie(U(\mathfrak{g}))$ such that if $A$ is any $F$-algebra, and $\varphi : \mathfrak{g} \to \Lie(A)$ is a Lie algebra homomorphism, then there exists a unique $F$-algebra homomorphism $\Phi : U(\mathfrak{g}) \to A$ such that $\varphi = \Phi \circ i$.
\end{theorem}%}T}E}X

As always, an object (in this case $U(\mathfrak{g})$) defined by a universal property is characterized up to isomorphism by that property.

% Proof
\begin{proof}%{T{E{X
    Let $\mathcal{K}$ be the ideal in $\bigotimes \mathfrak{g}$ generated by elements of the form $[x, y] - x \otimes y - y \otimes x$ for $x, y \in \mathfrak{g}$, and let $U(\mathfrak{g})$ be the quotient $\bigotimes V/\mathcal{K}$.
    Let $\varphi : \mathfrak{g} \to \Lie(A)$ be a \emph{Lie algebra homomorphism}.
    This means that $\varphi$ is an $F$-linear map such that $\varphi([x, y]) = \varphi(x)\varphi(y) - \varphi(y)\varphi(x)$.
    Then $\varphi$ extends to a ring homomorphism $\bigotimes \mathfrak{g} \to A$ by Proposition~\ref{prop9.1}.
    Our assumption implies the $\mathcal{K}$ is in the kernel of this homomorphism, and so there is induced a ring homomorphism $U(\mathfrak{g}) \to A$.
    Clearly, $U(\mathfrak{g})$ is generated by the image of $\mathfrak{g}$, so this homomorphism is uniquely determined.
\end{proof}%}T}E}X

% Proposition
\begin{proposition}\label{prop10.1}%{T{E{X
    If $\mathfrak{g}$ is the Lie algebra of a Lie group $G$, then the natural map $i : \mathfrak{g} \to U(\mathfrak{g})$ is injective.
\end{proposition}%}T}E}X

It is consequence of the Poincar\'{e}-Birkhoff-Witt Theorem, a standard and purely algebra theorem, that $i : \mathfrak{g} \to U(\mathfrak{g})$ is injective for \emph{any} Lie algebra.
Instead of proving the Poincar\'{e}-Birkhoff-Witt Theorem, we give a short proof of this weaker statement.

% Proof
\begin{proof}%{T{E{X
    Let $A$ be the ring of endomorphisms of $C^\infty(G)$.
    Regarding $X \in \mathfrak{g}$ as a derivation of $C^\infty(G)$, we have a Lie algebra homomorphism $\mathfrak{g} \to \Lie(A)$, which by Theorem~\ref{thm10.1} induces a map $U(\mathfrak{g}) \to A$.
    If $X \in \mathfrak{g}$ had zero image in $U(\mathfrak{g})$, it would have zero image in $A$.
    It would therefore be zero.
\end{proof}%}T}E}X

If $V$ is a vector space over $F$ and $\pi : \mathfrak{g} \to \End(V)$ is a representation, then we call a bilinear form $B$ on $V$ \emph{invariant} if
\[
    B(\pi(X)v , w) + B(v, \pi(X)w) = 0
\]
for $X \in \mathfrak{g}, v, w \in V$.
The following proposition shows that this notion of invariance is the Lie algebra analog of the more intuitive corresponding notion for Lie groups.

% Proposition
\begin{proposition}\label{prop10.2}%{T{E{X
    Suppose that $G$ is a Lie group, $\mathfrak{g}$ its Lie algebra, and $\pi : G \to \mathrm{GL}(V)$ a rep admitting an invariant bilinear form $B$.
    Then $B$ is invariant for the differential of $\pi$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Invariance under $\pi$ means that
    \[
        B(\pi(e^{tX})v, \pi(e^{tX})w) = B(v, w).
    \]
    Thus, the derivative of this with respect to $t$ is zero.
    By~\eqref{eq8.5}, this derivative is
    \[
        B(\pi(X)v, w) + b(v, \pi(X)w).
    \]
\end{proof}%}T}E}X

On $\mathfrak{g}$ itself, define $B(x, y) =  \tr(\ad(x)\ad(y))$, the \emph{Killing form}.

% Proposition
\begin{proposition}\label{prop10.3}%{T{E{X
    The Killing form on a Lie algebra is symmetric and invariant with respect to $\ad$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Invariance under $\ad$ means
    % Equation
    \begin{equation}\label{eq10.1}
        B([x, y], z) + B(y, [x, z]) = 0.
    \end{equation}
    Using~\eqref{eq8.4}, $B([x, y], z)$ is the trace of
    \[
        \ad(x)\ad(y)\ad(z) - \ad(y)\ad(x)\ad(z),
    \]
    while $B(y, [x, z])$ is the trace of
    \[
        \ad(y)\ad(x)\ad(z) - \ad(y)\ad(z)\ad(x).
    \]
    Using the property of endomorphisms $A$ and $B$ of a vector space that $\tr(AB) = \tr(BA)$, these sum to zero.
    This same fact implies that $B(x, y) = B(y, x)$.
\end{proof}%}T}E}X

% Theorem
\begin{theorem}\label{thm10.2}%{T{E{X
    Suppose that the Lie algebra $\mathfrak{g}$ admits a nondegenerate symmetric invariant bilinear form $B$.
    Let $x_1, \ldots, x_d$ be a basis of $\mathfrak{g}$, and let $y_1, \ldots, y_d$ be the dual basis, so that $B(x_i, y_j) = \delta_{ij}$ (Kronecker $\delta$).
    Then the element $\Delta = \sum_{i} x_iy_i$ of $U(\mathfrak{g})$ is in the center of $U(\mathfrak{g})$.
    The element $\Delta$ is independent of the choice of basis $x_1, \ldots, x_d$.
\end{theorem}%}T}E}X

This element $\Delta$ is called the \emph{Casimir element} of $U(\mathfrak{g})$ (with respect to $B$).

% Proof
\begin{proof}%{T{E{X
    Let $z \in \mathfrak{g}$.
    There exist constants $\alpha_{ij}$ and $\beta_{ij}$ such that $[z, x_i] = \sum_{j} \alpha_{ij} x_i$ and $[z, y_i] = \sum_{j} \beta_{ij} y_j$.
    Since $B$ is invariant, we have
    \[
        0 = B([z, x_i], y_j) + B(x_i, [z, y_j]) = \alpha_{ij} + \beta_{ji}.
    \]
    Now
    \[
        z \sum_{i} x_iy_i = \sum_{i} ([z, x_i]y_i) = \sum_{i, j} \alpha_{ij} x_j y_i + \sum_{i} x_izy_i,
    \]
    while
    \[
        \sum_{i} x_iy_iz = \sum_{i} (-x_i[z, y_i] + x_izy_i) = - \sum_{i, j} \beta_{ij} x_i y_j + \sum_{i} x_i z y_i,
    \]
    and since $\beta_{ij} = - \alpha_{ji}$, these are equal.
    Thus $\Delta$ commutes with $\mathfrak{g}$, and since $\mathfrak{g}$ generates $U(\mathfrak{g})$ as a ring, it is in the center.

    It remains to be shown that $\Delta$ is independent of the choice of basis $x_1, \ldots, x_d$.
    Suppose that $x_1', \ldots, x_d'$ is another basis.
    Write $x_i' = \sum_{j} \alpha_{ij} x_j$, and if $y_1', \ldots, y_d'$ is the corresponding dual basis, let $y_i' = \sum_{j} \beta_{ij} y_j$.
    The condition that $B(x_i', y_j') = \delta_{ij}$ (Kronecker $\delta$) implies that $\sum_{k} \alpha_{ik} \delta_{jk} = \delta_{ij}$.
    Therefore, the matrices $(\alpha_{ij})$ and $(\beta_{ij})$ are transpose inverses of each other and so we have also $\sum_{k} \alpha_{ki} \beta_{kj} = \delta_{ij}$.
    Now $\sum_{k} x_k' y_k' = \sum_{i, j, k} \alpha_{ki} \beta_{kj} x_i y_j = \sum_{k} x_k y_k = \Delta$.
\end{proof}%}T}E}X

A rep $(\rho, V)$ of a Lie algebra $\mathfrak{g}$ is \emph{irreducible} if there is no proper nonzero subspace $U \subseteq V$ such that $\rho(x)U \subseteq U$ for all $x \in \mathfrak{g}$.

% Proposition
\begin{proposition}\label{10.4}%{T{E{X
    Let $\rho : \mathfrak{g} \to \End(V)$ be an irreducible representation of the Lie algebra $\mathfrak{g}$.
If $c$ is in the center of $U(\mathfrak{g})$, then there exists a scalar $\lambda$ such that $\rho(c) = \lambda I_V$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Let $\lambda$ be any eigenvalue of $\rho(c)$.
    Let $U$ be the $\lambda$-eigenspace of $\rho(c)$.
    Since $\rho(c)$ commutes with $\rho(x)$ for all $x \in \mathfrak{g}$, we see that $\rho(x)U \subseteq U$ for all $x \in \mathfrak{g}$.
    By definition of irreducibility, $U = V$, so $\rho(c)$ acts by the scalar $\lambda$.
\end{proof}%}T}E}X

% UnnumberedSection
\section*{EXERCISES}%{T{E{X
% Example
\begin{example}\label{}
    Let $X_{ij} \in \mathfrak{gl}(n, \mathbb{R}) \ (1 \leq i, j \leq n)$ be the $n \times n$ matrix with a 1 in $i, j$ position and 0's elsewhere.
    Show that $[X_{ij}, X_{kl}] = \delta_{jk}X_{il} - \delta_{il}X_{kj}$, where $\delta_{jk}$ is the Kronecker $\delta$.
    From this, show that
    \[
        \sum_{i_1 = 1}^{n} \cdots \sum_{i_r = 1}^{n} X_{i_1i_2} X_{i_2i_3} \cdots X_{i_ni_1}
    \]
    is in the center of $U(\mathfrak{gl}(n, \mathbb{R}))$.
\end{example}

% Example
\begin{example}\label{}
    Let $G$ be a connected Lie group and $\mathfrak{g}$ its Lie algebra.
    Define an action of $\mathfrak{g}$ on the space $C^\infty(G)$ of smooth functions on $G$ by
    \[
        Xf(g) = \dv{t}f(ge^{tX})\Big|_{t = 0}.
    \]
    % Enumerate
    % \renewcommand{\labelenumi}{\emph{\Roman{enumi}.}}
    \begin{enumerate}[label=(\roman*),font=\normalfont,before=\normalfont]
        \item Show that this a representation of $G$.
            Explain hwy Theorem~\ref{thm10.1} implies that this action of $\mathfrak{g}$ on $C^\infty(G)$ can be extended to a representation of the associative algebra $U(\mathfrak{g})$ on $C^{\infty}(G)$.
        \item If $h \in G$, let $\rho(h)$ and $\lambda(h)$ be the endomorphisms of $G$ given by left and right translation.
            Thus
            \[
                \rho(h)f(g) = f(gh), \qquad \lambda(h)f(g) = f(h^{-1}g).
            \]
            Show that if $h \in G$ and $D \in U(\mathfrak{g})$, then $\lambda(h) \circ D = D \circ \lambda(h)$.
            If $D$ is in the center of $U(\mathfrak{g})$ then prove that $\rho(h) \circ D = D \circ \rho(h)$.
            (\textbf{Hint:} Prove this first if $h$ is of the form $e^X$ for some $X \in G$, and recall that $G$ was assumed to be connected, so it is generated by a neighborhood of the identity.)
    \end{enumerate}
\end{example}

% Example
\begin{example}\label{}
    Let $G = \mathrm{GL}(n, \mathbb{R})$.
    Let $B$ be the ``Borel subgroup'' of upper triangular matrices with positive diagonal entries, and let $K = \mathrm{SO}(n)$.
    % Enumerate
    % \renewcommand{\labelenumi}{\emph{\Roman{enumi}.}}
    \begin{enumerate}[label=(\roman*),font=\normalfont,before=\normalfont]
        \item Show that every element of $g \in G$ has a unique decomposition as $g = bk$ with $b \in B$ and $k \in K$.
        \item Let $s_1, \ldots, s_k$ be complex numbers.
            By (i), we may define an element $\varphi$ of $C^{\infty}(G)$ by
            \[
                \varphi{\left(
                        % Pmatrix
                        \renewcommand\arraystretch{1}
                        \begin{pmatrix}
                            y_1 & * & \cdots & * \\ % add[2em]toIncreaseSpacing
                            0 & y_2 & \cdots & * \\
                            \vdots & \vdots & \ddots & \vdots \\
                            0 & 0 & \cdots & y_n
                        \end{pmatrix}k\right)} = \prod_{i = 1}^{n} y_i^{s_i}, \qquad y_i > 0, k \in K.
            \]
            Show that $\varphi$ is an eigenfunction of the center of $U(\mathfrak{g})$.
            That is, if $D$ is in the center of $U(\mathfrak{g})$, then $D\varphi =  \lambda\varphi$ for some complex number $\lambda$.
            (\textbf{Hint:} Characterize $\varphi$ by properties of left and right translation and use Exercise 10.2 (ii).)
    \end{enumerate}
\end{example}

%}T}E}X

%}T}E}X
% Chapter
\chapter{Extension of Scalars}\label{}%{T{E{X
We will be interested in \emph{complex} representations of both real and complex Lie algebras.
There is an important distinction to be made.
If $\mathfrak{g}$ is a real Lie algebra, then complex representation is an $\mathbb{R}$-linear homomorphism $\mathfrak{g} \to \End(V)$, where $V$ is a complex vector space.
On the other hand, if $\mathfrak{g}$ is a \emph{complex} Lie algebra, we require that the homomorphism be $\mathbb{C}$-linear.
The reader should note that we ask more of a complex representation of a complex Lie algebra that we do of a complex representation of real Lie algebra.

The interplay between real and complex Lie groups and Lie algebras will prove important to us.
We begin this theme right here with some generalities about extension of scalars.

If $R$ is a commutative ring and $S$ is a larger commutative ring containing $R$, we may think of $S$ as an $R$-algebra.
In this case, there are functors between the categories of $R$-modules and $S$-modules.
Namely, if $N$ is an $S$-module, we may regard it as an $R$-module, we may form the $R$-module $M_S = S \otimes_{R} M$.
This has an $S$-module structure such that $t(s \otimes m) = ts \otimes m$ for $t, s \in S$, and $m \in M$.
We call this the $S$-module obtained by \emph{extension of scalars}.
If $\varphi : M \to N$ is an $R$-module homomorphism, $1 \otimes \varphi : M_S \to N_S$ is an $S$-module homomorphism, so extension is a functor.

Of the properties of extension of scalars, we note the following.

% Proposition
\begin{proposition}\label{prop11.1}%{T{E{X
    Let $S \supseteq R$ be commutative rings.
    % Enumerate
    % \renewcommand{\labelenumi}{\emph{\Roman{enumi}.}}
    \begin{enumerate}[label=(\roman*),font=\normalfont,before=\normalfont]
        \item If $M_1$ and $M_2$ are $R$-modules, we have the following natural isomorphisms of $S$-modules:
            % Gather
            \begin{gather}
                S \otimes_{R} R \cong S, \label{eq11.1} \\
                S \otimes_{R} (M_1 \oplus M_2) \cong (S \otimes_{R} M_1) \oplus (S \otimes_{R} M_2), \label{eq11.2} \\
                (S \otimes_{R} M_1) \otimes_{S} (S \otimes_{R} M_2) \cong S \otimes_{R} (M_1 \otimes_{R} M_2). \label{eq11.3}
            \end{gather}
        \item If $M$ is an $R$-module and $N$ is an $S$-module, we have a natural isomorphism
            % Equation
            \begin{equation}\label{eq11.4}
                \Hom_R(M, N) \cong \Hom_S(S \otimes_{R} M, N).
            \end{equation}
    \end{enumerate}
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    To prove~\eqref{eq11.1}, note that the multiplication $S \times R \to S$ is an $R$-bilinear map, hence by universal property of the tensor product induces an $R$-module homomorphism $S \otimes_{R} R \to S$.
    On the other hand, $s \to s \otimes 1$ is an $R$-module homomorphism $S \to S \otimes_{R} R$, and these maps are inverses of each other.
    With our definition of the $S$-module structure on $S \otimes_{R} R$, they are $S$-module isomorphisms.

    To prove~\eqref{eq11.2}, one may characterize the direct sum $M_1 \oplus M_2$ as follows: given an $R$-module $M$ with maps $j_i : M_i \to M, p_i : M \to M_i \ (i = 1, 2)$ such that $p_i \circ j_i = 1_{M_i}$ and $j_1 \circ p_1 + j_2 \circ p_2 = 1_M$, then there are maps
    % UnnumberedGather
    \begin{gather*}
        M \to M_1 \oplus M_2, \qquad m \mapsto (p_1(m), p_2(m)), \\
        M_1 \oplus M_2 \to M, \qquad (m_1, m_2) \mapsto i_1m_1 +  i_2m_2.
    \end{gather*}
    These are easily checked to be inverses of each other, and so $M \cong M_1 \oplus M_2$.
    For example, if $M = M_1 \oplus M_2$, such maps exist -- take the inclusion and projection maps in and out of the direct sum.
    Now applying the functor $M \mapsto S \otimes_{R} M$ gives maps for $S \otimes_{R} (M_1 \otimes_{R} M_2)$ showing that it is isomorphism to the left-hand side of~\eqref{eq11.2}.

    To prove~\eqref{eq11.3}, one has an $S$-bilinear map
    % Equation
    \begin{equation}\label{eq11.5}
        (S \otimes_{R} M_1) \times (S \otimes_{R} M_2) \to S \otimes_{R} (M_1 \otimes_{R} M_2)
    \end{equation}
    such that $((s_1 \otimes m_1), (s_2 \otimes m_2)) \mapsto s_1s_2 \otimes (m_1 \otimes m_2)$.
    To see this, we note that with $s_2$ and $m_2$ fixed, $s_1 \times m_1 \mapsto s_1s_2 \otimes (m_1 \otimes m_2)$ is $R$-bilinear, so by the universal property there is (for fixed $s_2$ and $m_2$) and $R$-module homomorphism that we denote $j_{s_2, m_2} : S \otimes_{R} M_1 \to S \otimes_{R} (M_1 \otimes_{R} M_2)$ such that $j_{s_2, m_2}(s_1 \otimes m_1) = s_1s_2 \otimes (m_1 \otimes m_2)$.
    Now, with $\alpha \in S \otimes M_1$ fixed, the map $(s_2, m_2) \mapsto j_{s_2, m_2}(\alpha)$ is $R$-bilinear, so there exists a map $J_\alpha : S \otimes_{R} M_2 \to S \otimes_{R} (M_1 \otimes_{R} M_2)$ such that $J_\alpha(s_2 \otimes m_2) = j_{s_2, m_2}(\alpha)$.
    The map~\eqref{eq11.5} is $J_{s_1 \otimes m_1}(s_2 \otimes m_2)$.
    This map is $S$-bilinear, so it induces a homomorphism
    % Equation
    \begin{equation}\label{eq11.6}
        (S \otimes_{R} M_1) \otimes_{S} (S \otimes_{R} M_2) \to S \otimes_{R} (M_1 \otimes_{R} M_2).
    \end{equation}
    Similarly, there is an $R$-bilinear map
    \[
        S \times (M_1 \otimes_{R} M_2) \to (S \otimes_{R} M_1) \otimes_{S} (S \otimes_{R} M_2)
    \]
    such that $(s, m_1 \otimes m_2) \mapsto (s \otimes m_1) \otimes (1 \otimes m_2) = (1 \otimes m_1) \otimes (s \otimes m_2)$.
    This induces an $S$-module homomorphism that is the inverse to~\eqref{eq11.6}.

    To prove~\eqref{eq11.4}, we describe the correspondence explicitly.
    If
    \[
        \varphi \in \Hom_R(M, N) \ \text{and} \ \Phi \in \Hom_S(S \otimes M, N),
    \]
    then $\varphi$ and $\Phi$ correspond if $\varphi(m) = \Phi(1 \otimes m)$ and $\Phi(s \otimes m) = s \varphi(m)$.
    It is easily checked that $\varphi \mapsto \Phi$ and $\Phi \mapsto \varphi$ are well-defined inverse isomorphisms.
\end{proof}%}T}E}X

If $V$ is a $d$-dimensional real vector space, then the complex vector space $V_\mathbb{C} = \mathbb{C} \otimes_{\mathbb{R}} V$ is a $d$-dimensional complex vector space.
This follows from Proposition~\ref{prop11.1} because if $V \cong \mathbb{R} \otimes \cdots \otimes \mathbb{R}$ ($d$ copies), then~\eqref{eq11.1} and~\eqref{eq11.2} imply that $V_\mathbb{C} \cong \mathbb{C} os \cdots \otimes \mathbb{C}$ ($d$ copies).
We call $V_\mathbb{C}$ the \emph{complexification} of $V$.
The natural map $V \to V_\mathbb{C}$ given by $v \mapsto 1 \otimes v$ is injective, so we may think of $V$ as a real vector subspace of $V_\mathbb{C}$.

% Proposition
\begin{proposition}\label{prop11.2}%{T{E{X
    % Enumerate
    % \renewcommand{\labelenumi}{\emph{\Roman{enumi}.}}
    \begin{enumerate}[label=(\roman*),font=\normalfont,before=\normalfont]
        \item If $V$ is a real vector space and $W$ is a complex vector space, any $\mathbb{R}$-linear transformation $V \to W$ extends uniquely to a $\mathbb{C}$-linear transformation $V_\mathbb{C} \to W$.
        \item If $V$ and $U$ are real vector spaces, any $\mathbb{R}$-linear transformation $V \to U$ extends uniquely to a $\mathbb{C}$-linear map $V_\mathbb{C} \to U_\mathbb{C}$.
        \item If $V$ and $U$ are real vector spaces, any $\mathbb{R}$-bilinear map $V \times V \to U$ extends uniquely to a $\mathbb{C}$-bilinear map $V_\mathbb{C} \times V_\mathbb{C} \to U_\mathbb{C}$.
    \end{enumerate}
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Part (i) is a special case of Proposition~\ref{prop11.1}.
    Part (ii) follows by taking $W = U_\mathbb{C}$ in part (i) after composing the given linear map $V \to U$ with the inclusion $U_\mathbb{C} \to W$.
    As for (iii), an $\mathbb{R}$-bilinear map $V \times V \to U$ induces an $\mathbb{R}$-linear map $V \otimes_{\mathbb{R}} V \to U$ and hence by (ii) a $\mathbb{C}$-linear map $(V \otimes_{\mathbb{R}} V)_\mathbb{C} \to U_\mathbb{C}$.
    But by~\eqref{eq11.3}, $(V \otimes_{\mathbb{R}} V)_\mathbb{C}$ is $V_\mathbb{C} \otimes_{\mathbb{C}} V_\mathbb{C}$, and a $\mathbb{C}$-linear map $V_\mathbb{C} \otimes_{\mathbb{C}} V_\mathbb{C} \to U_\mathbb{C}$ is the same thing as a $\mathbb{C}$-bilinear map $V_\mathbb{C} \times V_\mathbb{C} \to U_\mathbb{C}$.
\end{proof}%}T}E}X

% Proposition
\begin{proposition}\label{prop11.3}%{T{E{X
    % Enumerate
    % \renewcommand{\labelenumi}{\emph{\Roman{enumi}.}}
    \begin{enumerate}[label=(\roman*),font=\normalfont,before=\normalfont]
        \item The complexification $\mathfrak{g}_\mathbb{C}$ of a real Lie algebra $\mathfrak{g}$ with the bracket extended as in Proposition~\ref{prop11.2} (iii) is a Lie algebra.
        \item If $\mathfrak{g}$ is a real Lie algebra, $\mathfrak{h}$ is a complex Lie algebra, and $\rho : \mathfrak{g} \to \mathfrak{h}$ is a real Lie algebra homomorphism, then $\rho$ extends uniquely to a homomorphism $\rho_\mathbb{C} : \mathfrak{g}_\mathbb{C} \to \mathfrak{h}$ of complex Lie algebras.
            In particular, any complex representation of $\mathfrak{g}$ extends uniquely to a complex representation of $\mathfrak{g}_\mathbb{C}$.
        \item If $\mathfrak{g}$ is a real Lie subalgebra of the complex Lie algebra $\mathfrak{h}$, and if $\mathfrak{h} = \mathfrak{g} \oplus i \mathfrak{g}$ (that is, if $\mathfrak{g}$ and $i\mathfrak{g}$ span $\mathfrak{h}$ but $\mathfrak{g} \cap i \mathfrak{g} = \{0\}$), then $\mathfrak{h} \cong \mathfrak{g}_\mathbb{C}$ as complex Lie algebras.
    \end{enumerate}
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    For (i), then extended bracket satisfies the Jacobi identity since both sides of~\eqref{eqjacobiidentity} are trilinear maps on $\mathfrak{g}_\mathbb{C} \times \mathfrak{g}_\mathbb{C} \times \mathfrak{g}_\mathbb{C} \to \mathfrak{g}_\mathbb{C}$, which by assumption vanish on $\mathfrak{g} \times \mathfrak{g} \times \mathfrak{g}$.
    Since $\mathfrak{g}$ generates $\mathfrak{g}_\mathbb{C}$ over the complex numbers,~\eqref{eqjacobiidentity} is therefore true on $\mathfrak{g}_\mathbb{C}$.

    For (ii), the extension is given by Proposition~\ref{prop11.2} (i), taking $W = \mathfrak{h}$.
    To see that the extension is a Lie algebra homomorphism, note that both $\rho([x, y])$ and $\rho(x)\rho(y) - \rho(y)\rho(x)$ are bilinear maps $\mathfrak{g}_\mathbb{C} \times \mathfrak{g}_\mathbb{C} \to \mathfrak{h}$ that agree on $\mathfrak{g} \times \mathfrak{g}$.
    Since $\mathfrak{g}$ generates $\mathfrak{g}_\mathbb{C}$ over $\mathbb{C}$, they are equal for all $x, y \in \mathfrak{g}_\mathbb{C}$.

    For (iii), by Proposition~\ref{prop11.2} (i), it will be least confusing to distinguish between $\mathfrak{g}$ and its image in $\mathfrak{h}$, so we prove instead the following equivalent statement: if $\mathfrak{g}$ is a real Lie algebra, $\mathfrak{h}$ is a complex Lie algebra, $f : \mathfrak{g} \to \mathfrak{h}$ is an injective homomorphism, and if $\mathfrak{h} = f(\mathfrak{g}) \oplus i f(\mathfrak{g})$, then $f$ extends to an isomorphism $\mathfrak{g}_\mathbb{C} \to \mathfrak{h}$ of complex Lie algebras.
    Now $f$ extends to a Lie algebra homomorphism $f_\mathbb{C} : \mathfrak{g}_\mathbb{C} \to \mathfrak{h}$ by part (ii).
    To see that this is an isomorphism, note that it is surjective since $f(\mathfrak{g})$ spans $\mathfrak{h}$.
    To prove that it is injective, if $f_\mathbb{C}(X + iY) = 0$ with $X, Y \in \mathfrak{g}$, then $f(X) +  i f(Y) = 0$.
    Now because $f(\mathfrak{g}) \cap i f(\mathfrak{g}) = 0$.
    Since $f$ is injective, $X = Y = 0$.
\end{proof}%}T}E}X

Of course, given any complex representation of $\mathfrak{g}_\mathbb{C}$, we may also restrict it to $\mathfrak{g}$, so Proposition~\ref{prop11.3} implies that complex representations of $\mathfrak{g}$ and complex representations of $\mathfrak{g}_\mathbb{C}$ are really the same thing.
(They are equivalent categories.)

As an example, let us consider the complexification of $\mathfrak{u}(n)$.

% Proposition
\begin{proposition}\label{prop11.4}%{T{E{X
    % Enumerate
    % \renewcommand{\labelenumi}{\emph{\Roman{enumi}.}}
    \begin{enumerate}[label=(\roman*),font=\normalfont,before=\normalfont]
        \item Every $n \times n$ complex matrix $X$ can be written uniquely as $X_1 + iX_2$, where $X_1$ and $X_2$ are $n \times n$ complex matrices satisfying $X_1 = - X_1^t$ and $X_2 = X_2^t$.
        \item The complexification of the real Lie algebra $\mathfrak{u}(n)$ is isomorphic to $\mathfrak{gl}(n, \mathbb{C})$.
        \item The complexification of the real Lie algebra $\mathfrak{su}(n)$ is isomorphic to $\mathfrak{sl}(n, \mathbb{C})$.
    \end{enumerate}
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    For (i), we note that we must have
    \[
        X_1 = \frac{1}{2}(X - X^t), \qquad X_2 = \frac{1}{2i} (X + X^t).
    \]

    For (ii), we will use the criterion of Proposition~\ref{prop11.3} (iii).
    We recall that $\mathfrak{u}(n)$ is the \emph{real} Lie algebra consisting of \emph{complex} $n \times n$ matrices satisfying $X = \overline{-X^t}$.
    We want to get the complex conjugation out of the picture before we try to complexify it, so we write $X = X_1 + iX_2$, where $X_1$ and $X_2$ are real $n \times n$ matrices.
    We must have $X_1 = - X_1^t$ and $X_2 = X_2^t$.
    Thus, as a vector space, we may identify $\mathfrak{u}(2)$ with the real vector space of pairs $(X_1, X_2) \in \mathrm{M}(n, \mathbb{R}) \oplus \mathrm{M}(n, \mathbb{R})$, where $X_1$ is skew-symmetric and $X_2$ is symmetric.
    The Lie bracket operation, required by the condition that
    % Equation
    \begin{equation}\label{eq11.7}
        [X, Y] = XY - YX \ \text{when} \ X = X_1 + iX_2 \ \text{and} \ Y = Y_1 + iY_2,
    \end{equation}
    amounts to the rule
    % Equation
    \begin{equation}\label{eq11.8}
        [(X, X_2), (Y_1, Y_2)] = (X_1Y_1 - X_2Y_2 - Y_1X_1 + Y_2X_2, X_1Y_2 + X_2Y_1 - Y_2X_1 - Y_1X_2).
    \end{equation}
    Now (i) shows that the complexification of this vector space (allowing $X_1$ and $X_2$ to be complex) can be identified with $\mathrm{M}(n, \mathbb{C})$.
    Of course,~\eqref{eq11.7} and~\eqref{11.8} are still equivalent if $X_1, X_2, Y_1$, and $Y_2$ are allowed to be complex, so with the Lie bracket in~\eqref{eq11.8}, this Lie algebra is $\mathrm{M}(n, \mathbb{C})$ with the usual bracket.
    We recall from Example 5.6 that this is the Lie algebra $\mathfrak{gl}(n, \mathbb{C})$.

    (iii) is similar to (ii), and we leave it to the reader.
\end{proof}%}T}E}X

% Theorem
\begin{theorem}\label{thm11.1}%{T{E{X
    Every complex representation of the Lie algebra $\mathfrak{u}(n)$ or the Lie algebra $\mathfrak{gl}(n, \mathbb{R})$ extends uniquely to a complex representation of $\mathfrak{gl}(n, \mathbb{C})$.
    Every complex representation of the Lie algebra $\mathfrak{su}(n)$ or the Lie algebra $\mathfrak{sl}(n, \mathbb{R})$ extends uniquely to a complex representation of $\mathfrak{sl}(n, \mathbb{C})$.
\end{theorem}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    This follows from Proposition~\ref{prop11.3} since the complexification of $\mathfrak{u}(n)$ or $\mathfrak{gl}(n, \mathbb{R})$ is $\mathfrak{gl}(n, \mathbb{C})$, while the complexification of $\mathfrak{su}(n)$ or $\mathfrak{sl}(n, \mathbb{R})$ is $\mathfrak{sl}(n, \mathbb{C})$.
    For $\mathfrak{gl}(2, \mathbb{R})$ or $\mathfrak{sl}(2, \mathbb{R})$, this is obvious.
    For $\mathfrak{u}(n)$ and $\mathfrak{su}(n)$, this is Proposition~\ref{prop11.4}.
\end{proof}%}T}E}X

%}T}E}X
% Chapter
\chapter{Representations of $\mathfrak{sl}(2, \mathbb{C})$}\label{}%{T{E{X
Unless otherwise indicated, in this chapter a \emph{representation} of a Lie group or Lie algebra is a complex representation.

Let us exhibit some representations of the group $\mathrm{SL}(2, \mathbb{C})$.
We start with the standard representation on $\mathbb{C}^2$, with $\mathrm{SL}(2, \mathbb{C})$ acting by matrix multiplication on column vectors.
Due to the functionality of $\vee^k$, there is induced a representation of $\mathrm{SL}(2, \mathbb{C})$ on $\vee^k\mathbb{C}^2$.
The dimension of this vector space is $k +  1$.
In short, $\vee^k$ gives us a representation $\mathrm{SL}(2, \mathbb{C}) \to \mathrm{GL}(k + 1, \mathbb{C})$.
There is an induced map of Lie algebras $\mathfrak{sl}(2, \mathbb{C}) \to \mathfrak{gl}(k + 1, \mathbb{C})$ by Proposition~\ref{prop7.3}, and it is not hard to see that this is a complex Lie algebra homomorphism.
We have corresponding representations of $\mathfrak{sl}(2, \mathbb{R})$ and $\mathfrak{su}(2)$, and we will eventually see that these are all the irreducible representations of these groups.

Let us make these symmetric power representations more explicit for the algebra $\mathfrak{g} = \mathfrak{sl}(2, \mathbb{R})$.
A basis of $\mathfrak{g}$ consists of the three matrices
\[
    H = \begin{pmatrix}
        1 & 0  \\
        0 & -1
    \end{pmatrix}, \qquad R = \begin{pmatrix}
        0 & 1  \\
        0 & 0
    \end{pmatrix}, \qquad L = \begin{pmatrix}
        0 & 0  \\
        1 & 0
    \end{pmatrix}.
\]
They satisfy the commutation relations
% Equation
\begin{equation}\label{eq12.1}
    [H, R] = 2R, \qquad [H, L] = -2L, \qquad [R, L] = H.
\end{equation}

Let
\[
    \bm{x} = \begin{pmatrix}
        1 \\
        0
    \end{pmatrix}, \qquad \bm{y} = \begin{pmatrix}
        0 \\
        1
    \end{pmatrix},
\]
be the standard basis of $\mathbb{C}^2$.
We have a corresponding basis of $k + 1$ elements in $\vee^k\mathbb{C}^2$, which we will label by integers $k, k - 2, k - 4, \ldots, -2k$ for reasons that will become clear presently.
Thus we let
\[
    v_{k - 2l} = \bm{x} \vee \cdots \vee \bm{x} \vee \bm{y} \vee \cdots \vee \bm{y} \qquad (k - l \ \text{copies of} \ \bm{x}, l \ \text{copies of} \ \bm{y}).
\]
Since $\vee^k$ is a functor, if $f : \mathbb{C}^2 \to \mathbb{C}^2$ is a linear transformation, there is induced a linear transformation $\vee^kf$ of $\vee^k\mathbb{C}^2$.
(See Exercise 9.4.)
In particular, let us compute the effect of $\vee^kR$ on $v_i$.
In $\mathbb{C}^2$,
\[
    \exp(tR) :
    \begin{dcases}
        \bm{x} \mapsto \bm{x} \\
        \bm{y} \mapsto \bm{y} + t\bm{x}
    \end{dcases}
\]
Therefore, in $\vee^kV$, remembering that the $\vee$ operation is symmetric (commutative), we see that $\exp(tR)$ maps $v_{k - 2l}$ to
\[
    v_{k - 2l} + tlv_{k - 2l + 2} + t^2 {l \choose 2} v_{k - 2l + 4} + \cdots.
\]
Therefore, in the Lie algebra,
% Equation
\begin{equation}\label{eq12.2}
    (\vee^kR)v_{k - 2l} = \dv{t} \exp(tR)v_{k - 2l} \Big|_{t = 0} =
    \begin{dcases}
        lv_{k - 2l + 2} & \text{if} \ l > 0, \\
        0 & \text{if} \ l = 0.
    \end{dcases}
\end{equation}
Similarly, it may be checked that
% Equation
\begin{equation}\label{eq12.3}
    (\vee^kL)v_{k - 2l} =
    \begin{dcases}
        (k - l)v_{k - 2l - 2} & \text{if} \ l < k, \\
        0 & \text{if} \ l < k,
    \end{dcases}
\end{equation}
and
% Equation
\begin{equation}\label{eq12.4}
    (\vee^kH)v_{k - 2l} = (k -  2l)v_{k - 2l}.
\end{equation}
The last identity is the reason for the labeling of the vectors $v_{k - 2l}$: the subscript is the eigenvalue of $H$.

For example, if $k = 3$, then with respect to the basis $v_3, v_1, v_{-1}, v_{-3}$, we find that
% UnnumberedGather
\begin{gather*}
    \vee^3R = \begin{pmatrix}
        0 & 1 & 0 & 0 \\
        0 & 0 & 2 & 0 \\
        0 & 0 & 0 & 3 \\
        0 & 0 & 0 & 0
    \end{pmatrix}, \\
    \vee^3L = \begin{pmatrix}
        0 & 0 & 0 & 0 \\
        3 & 0 & 0 & 0 \\
        0 & 2 & 0 & 0 \\
        0 & 0 & 1 & 0
    \end{pmatrix}, \qquad \vee^3H = \begin{pmatrix}
        3 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 0 & -1 & 0 \\
        0 & 0 & 0 & -3
    \end{pmatrix}.
\end{gather*}
It may be checked directly that these matrices satisfy the commutative relations~\eqref{eq12.1}.

% Proposition
\begin{proposition}\label{prop12.1}%{T{E{X
    The representation $\vee^k\mathbb{C}^2$ of $\mathfrak{sl}(2, \mathbb{R})$ is irreducible.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Suppose that $U$ is a nonzero invariant subspace.
    Choose a nonzero element $\sum a_{k - 2l}v_{k - 2l}$ of $U$.
    Let $k - 2l$ be the smallest integer such that $a_{k - 2l} \neq 0$.
    Applying $R$ to this vector $l$ times shifts each $v_r \to v_{r + 2}$ times a nonzero constant, except for $v_k$, which it kills.
    Consequently, this operation $R^l$ will kill every vector $v_r$ with $r \geq k - 2l$, leaving only a nonzero constant times $v_k$.
    Thus $v_k \in U$.
    Now applying $L$ repeatedly show that $v_{k - 2}, v_{k - 4}, \cdots \in U$, so $U$ contains a basis of $\vee^k\mathbb{C}^2$.
    We see that any nonzero invariant subspace of $\vee^k\mathbb{C}^2$ is the whole space, so the representation is irreducible.
\end{proof}%}T}E}X

If $k = 0$, we reiterate that $\vee^0\mathbb{C}^2 = \mathbb{C}$.
It is a \emph{trivial} $\mathfrak{sl}(2, \mathbb{R})$-module, meaning that $\pi(X)$ acts as zero on it for all $X \in \mathfrak{sl}(2, \mathbb{R})$.

Now we need an element of the center of $U(\mathfrak{sl}(2, \mathbb{R}))$.
An invariant bilinear form on $\mathfrak{g}$ is given by $B(x, y) = \frac{1}{2} \tr(xy)$, where the trace is the usual trace of a matrix, and $xy$ is the product of two matrices, \emph{not} multiplication in $U(\mathfrak{sl}(2, \mathbb{R}))$.
The invariance of this bilinear form follows from the property of the trace that $\tr(xy) = \tr(yx)$ since
\[
    B([x, y], z) + B(y, [x, z]) = \frac{1}{2}(\tr(xyz) - \tr(yxz) + \tr(yxz) - \tr(yzx)) = 0,
\]
proving~\eqref{eq10.1}.
Dual to the basis $H, R, L$ of $\mathfrak{sl}(2, \mathbb{R})$ is the basis $H, 2L, 2R$, and it follows from Theorem~\ref{thm10.2} that the Casimir element
\[
    \Delta = H^2 + 2RL + 2LR
\]
is an element of the center of $U(\mathfrak{sl}(2, \mathbb{R}))$.

% Proposition
\begin{proposition}\label{prop12.2}%{T{E{X
    The element $\Delta$ acts by the scalar $\lambda = k^2 + 2k$ on $\vee^k\mathbb{C}^2$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    To calculate the effect of $\Delta$ on the space, we apply it to a basis vector $v_{k - 2l}$.
    We see that
    % Gather
    \begin{gather}
        H^2v_{k - 2l} = (k - 2l)^2v_{k - 2l}, \label{eq12.5} \\
        2RLv_{k - 2l} = 2R(k - l)v_{k - 2l - 2} = 2(l + 1)(k - l)v_{k - 2l}, \label{eq12.6} \\
        2LRv_{k - 2l} = 2Llv_{k - 2l + 2} = 2l(k - l + 1) v_{k - 2l}. \label{eq12.7}
    \end{gather}
    Adding these,
    % Equation
    \begin{equation}\label{eq12.8}
        \Delta v_{k - 2l} = (k^2 + 2k)v_{k - 2l}.
    \end{equation}
    This completes the proof.
\end{proof}%}T}E}X

% Proposition
\begin{proposition}\label{prop12.3}%{T{E{X
    Let $(\pi, V)$ be a finite-dimensional complex representation of $\mathfrak{sl}(2, \mathbb{R})$.
    Assume that $\Delta$ acts by a scalar $\lambda$ on $V$.
    Let $v_k$ be an eigenvector on $\pi(H)$ on $V$, so that $\pi(H)v_k = kv_k$ for some $k \in \mathbb{C}$ chosen so that the real part of $k$ is as large as possible.
    Then $k$ is a nonnegative integer, $\lambda = k^2 + 2k$, and $v_k$ generates an irreducible subspace of $V$ isomorphic to $\vee^k\mathbb{C}^2$.
\end{proposition}%}T}E}X
% Proof
\begin{proof}%{T{E{X
    Suppose that $v$ is an eigenvector of $H$ with eigenvalue $r$.
    Then we show that $Rv$ (if nonzero) is also an eigenvector, with eigenvalue $r + 2$.
    Indeed, in the enveloping algebra, we have $HR - RH = [H, R] = 2R$, so $HRv = RHv + 2Rv = (r +  2)Rv$.

    Next we show that if $v$ is an eigenvector of $H$ with eigenvalue $k$, and if $Rv = 0$, then $\lambda = k^2 + 2k$.
    Indeed, since $RL - LR = [R, L] = H$, we may write $\Delta = H^2 + 2H + 4LR$, so applying $\Delta$ of $v$ gives $(k^2 + 2k)v$.

    Similarly, if $v$ is an eigenvector of $H$ with eigenvalue $r$, and $Lv \neq 0$, then $Lv$ is an eigenvector with eigenvalue $r - 2$.
    Moreover, if $v$ is an eigenvector of $H$ with eigenvalue $h$, and if $Lv = 0$, then $\lambda = h^2 - 2h$.

    Since $\pi(H)$ is an endomorphism of a complex vector space, it has at least one eigenvalue.
    If $v_k$ is an eigenvector with eigenvalue $k$, chosen so that $k$ has maximum real part, then $Rv_k = 0$, since $k + 2$ is not an eigenvalue.
    Thus $\lambda = k^2 + k$.
    Now, applying $L$ successively, we obtain eigenvectors $Lv_k, L^2v_k, \ldots$ with eigenvalue $k - 2, k - 4, \ldots$.
    Eventually these must vanish.
    Let $h = k - 2l$ be such that $L^lv_k \neq 0$, while $L^{l + 1}v_k = 0$.
    Then $\lambda = h^2 - 2h$.

    Since $h^2 - 2h = k^2 - 2k$, we have $2(h + k) = h^2 - k^2 = (h - k)(h + k)$.
    Either $h - k = 2$ or $h + k = 0$.
    However, $h \leq k$ so $h - k = 2$ is impossible and $h = -k$.
    Since $h = k - 2l$, we see that $l = k$.
    In particular, $k$ is a nonnegative integer.
    Now define $v_{k - 2}, v_{k - 4}, \ldots$ by
    \[
        v_{k - 2l - 2} = \frac{1}{k - l}Lv_{k - 2l}.
    \]
    We claim that~\eqref{eq12.2},~\eqref{eq12.3} and~\eqref{eq12.4} are all satisfied.
    Already~\eqref{eq12.3} and~\eqref{eq12.4} are evident.
    As for~\eqref{eq12.2}, we may argue by induction.
    Suppose that this statement is true for $l$.
    Then, of the equations~\eqref{eq12.5},~\eqref{eq12.6},~\eqref{eq12.7}, and~\eqref{eq12.8}, all but~\eqref{eq12.6} are known;~\eqref{eq12.7} uses the induction hypothesis.
    Now~\eqref{eq12.6} follows by subtracting~\eqref{eq12.5} and~\eqref{eq12.7} from~\eqref{eq12.8}, and~\eqref{eq12.2} follows for $l + 1$.

    It follows that $v_k, v_{k - 2}, \ldots, v_{-k}$ span a submodule of $V$ that is isomorphic to $\vee^k\mathbb{C}^2$.
\end{proof}%}T}E}X

%}T}E}X

%}T}E}X

\vspace{10em}

\end{document}

%-------------------------------------------------------------------
% Comments on the document
%-------------------------------------------------------------------

% foldmarker has been changed, if you open it in VIM be careful.
% set foldmarker={T{E{X,}T}E}X


